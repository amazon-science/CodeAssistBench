[
  {
    "number": 132,
    "title": "Killed when generated video",
    "created_at": "2025-03-02T05:04:44Z",
    "closed_at": "2025-03-04T09:54:01Z",
    "commit_id": "a326079926a4a347ecda8863dc40ba2d7680a294",
    "labels": [],
    "url": "https://github.com/Wan-Video/Wan2.1/issues/132",
    "body": "[2025-03-02 12:01:03,397] INFO: Input image: examples/i2v_input.JPG\n[2025-03-02 12:01:03,542] INFO: Creating WanI2V pipeline.\n[2025-03-02 12:01:54,569] INFO: loading .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P/models_t5_umt5-xxl-enc-bf16.pth\n[2025-03-02 12:02:05,031] INFO: loading .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P/Wan2.1_VAE.pth\n[2025-03-02 12:02:05,867] INFO: loading .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth\n[2025-03-02 12:02:11,709] INFO: Creating WanModel from .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P\n[2025-03-02 12:02:35,384] INFO: Generating video ...\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [45:42<00:00, 68.55s/it]\nKilled",
    "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/132/comments",
    "author": "DorothyDoro",
    "comments": [
      {
        "user": "wxwwt",
        "created_at": "2025-03-02T06:26:00Z",
        "body": "I also have this problem\n\n\n(myenv) dministrator@DESKTOP-C3RIDG2:/opt/project/Wan2.1$ python generate.py  --task t2v-1.3B --size 832*480 --ckpt_dir ./Wan2.1-T2V-1.3B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n[2025-03-02 13:59:35,781] INFO: offload_model is not specified, set to True.\n[2025-03-02 13:59:35,781] INFO: Generation job args: Namespace(task='t2v-1.3B', size='832*480', frame_num=81, ckpt_dir='./Wan2.1-T2V-1.3B', offload_model=True, ulysses_size=1, ring_size=1, t5_fsdp=False, t5_cpu=False, dit_fsdp=False, save_file=None, prompt='Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.', use_prompt_extend=False, prompt_extend_method='local_qwen', prompt_extend_model=None, prompt_extend_target_lang='ch', base_seed=8478258736304712572, image=None, sample_solver='unipc', sample_steps=50, sample_shift=5.0, sample_guide_scale=5.0)\n[2025-03-02 13:59:35,781] INFO: Generation model config: {'__name__': 'Config: Wan T2V 1.3B', 't5_model': 'umt5_xxl', 't5_dtype': torch.bfloat16, 'text_len': 512, 'param_dtype': torch.bfloat16, 'num_train_timesteps': 1000, 'sample_fps': 16, 'sample_neg_prompt': '色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质 量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走', 't5_checkpoint': 'models_t5_umt5-xxl-enc-bf16.pth', 't5_tokenizer': 'google/umt5-xxl', 'vae_checkpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n[2025-03-02 13:59:35,781] INFO: Input prompt: Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\n[2025-03-02 13:59:35,781] INFO: Creating WanT2V pipeline.\nKilled"
      },
      {
        "user": "FurkanGozukara",
        "created_at": "2025-03-02T06:29:22Z",
        "body": "Killed is out of ram\n\nIncrease virtual ram "
      },
      {
        "user": "wxwwt",
        "created_at": "2025-03-02T08:08:50Z",
        "body": "> Killed is out of ram\n> \n> Increase virtual ram\n\nthx  it`s work~"
      }
    ],
    "satisfaction_conditions": [
      "An explanation of why the video generation process is being killed",
      "A solution to prevent the video generation process from being killed",
      "Identification of resource constraints causing the process termination"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-14 01:12:25"
    },
    "dockerfile": "FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PYTHONUNBUFFERED=1\nENV PATH=\"/usr/local/cuda/bin:${PATH}\"\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    curl \\\n    python3-dev \\\n    python3-pip \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository at the specific commit\nRUN git clone https://github.com/Wan-Video/Wan2.1.git /app && \\\n    cd /app && \\\n    git checkout a326079926a4a347ecda8863dc40ba2d7680a294\n\n# Upgrade pip and install PyTorch 2.4.0 with CUDA support first\n# This prevents dependency conflicts and speeds up the build\nRUN pip3 install --no-cache-dir --upgrade pip && \\\n    pip3 install --no-cache-dir torch>=2.4.0 torchvision>=0.17.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install project dependencies in batches to prevent memory issues\nRUN pip3 install --no-cache-dir numpy scipy matplotlib && \\\n    pip3 install --no-cache-dir opencv-python pillow && \\\n    pip3 install --no-cache-dir tqdm transformers einops && \\\n    pip3 install --no-cache-dir huggingface_hub modelscope && \\\n    pip3 install --no-cache-dir -r requirements.txt\n\n# Set up directories for model caching\nRUN mkdir -p /root/.cache/modelscope/hub/models/Wan-AI\nRUN mkdir -p /root/.cache/torch/hub/checkpoints\n\n# Create directories for data and output\nRUN mkdir -p /data /output\n\n# Set the working directory\nWORKDIR /app\n\n# Set environment variables for memory management\nENV OMP_NUM_THREADS=1\nENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n\n# The image is now ready for use\n# Users can mount their model files and run the generation scripts\n# Example: docker run --gpus all --shm-size=16g -v /path/to/models:/data -v /path/to/output:/output wan-video"
  },
  {
    "number": 131,
    "title": "WSL2 Ubuntu: cache_video failed, error: result type Float can't be cast to the desired output type Byte",
    "created_at": "2025-03-01T17:20:49Z",
    "closed_at": "2025-03-01T20:22:42Z",
    "commit_id": "a326079926a4a347ecda8863dc40ba2d7680a294",
    "labels": [],
    "url": "https://github.com/Wan-Video/Wan2.1/issues/131",
    "body": "\n\n\npython generate.py  --task t2v-1.3B --size 480*832 --ckpt_dir ./Wan2.1-T2V-1.3B --prompt \"a metallic skeleton robot on a cooking show, preparing a recipe with a whole chicken\" --save_file ./output.mp4\n\n\n[2025-03-01 14:01:36,940] INFO: offload_model is not specified, set to True.\n[2025-03-01 14:01:36,940] INFO: Generation job args: Namespace(task='t2v-1.3B', size='480*832', frame_num=81, ckpt_dir='./Wan2.1-T2V-1.3B', offload_model=True, ulysses_size=1, ring_size=1, t5_fsdp=False, t5_cpu=False, dit_fsdp=False, save_file='./output.mp4', prompt='a metallic skeleton robot on a cooking show, preparing a recipe with a whole chicken', use_prompt_extend=False, prompt_extend_method='local_qwen', prompt_extend_model=None, prompt_extend_target_lang='ch', base_seed=4277550218863685172, image=None, sample_solver='unipc', sample_steps=50, sample_shift=5.0, sample_guide_scale=5.0)\n[2025-03-01 14:01:36,940] INFO: Generation model config: {'__name__': 'Config: Wan T2V 1.3B', 't5_model': 'umt5_xxl', 't5_dtype': torch.bfloat16, 'text_len': 512, 'param_dtype': torch.bfloat16, 'num_train_timesteps': 1000, 'sample_fps': 16, 'sample_neg_prompt': '色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质 量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走', 't5_checkpoint': 'models_t5_umt5-xxl-enc-bf16.pth', 't5_tokenizer': 'google/umt5-xxl', 'vae_checkpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n[2025-03-01 14:01:36,940] INFO: Input prompt: a metallic skeleton robot on a cooking show, preparing a recipe with a whole chicken\n[2025-03-01 14:01:36,940] INFO: Creating WanT2V pipeline.\n[2025-03-01 14:02:33,960] INFO: loading ./Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\n[2025-03-01 14:03:39,030] INFO: loading ./Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\n[2025-03-01 14:03:41,640] INFO: Creating WanModel from ./Wan2.1-T2V-1.3B\n[2025-03-01 14:07:17,091] INFO: Generating video ...\n100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [08:27<00:00, 10.16s/it]\n[2025-03-01 14:16:14,586] INFO: Saving generated video to ./output.mp4\ncache_video failed, error: result type Float can't be cast to the desired output type Byte\n[2025-03-01 14:16:15,400] INFO: Finished.",
    "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/131/comments",
    "author": "egaralmeida",
    "comments": [
      {
        "user": "egaralmeida",
        "created_at": "2025-03-01T20:22:42Z",
        "body": "Fixed by installing imageio-ffmpeg, which is in the requirements. Not sure why it didn't install for me along many other requirements."
      },
      {
        "user": "garysdevil",
        "created_at": "2025-03-02T03:21:53Z",
        "body": "```log\nheckpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n[2025-03-02 10:33:59,629] INFO: Input prompt: Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\n[2025-03-02 10:33:59,629] INFO: Creating WanT2V pipeline.\n[2025-03-02 10:34:36,622] INFO: loading ./Wan2.1-T2V-1.3B\\models_t5_umt5-xxl-enc-bf16.pth\n[2025-03-02 10:34:41,096] INFO: loading ./Wan2.1-T2V-1.3B\\Wan2.1_VAE.pth\n[2025-03-02 10:34:41,508] INFO: Creating WanModel from ./Wan2.1-T2V-1.3B\n[2025-03-02 10:34:43,656] INFO: Generating video ...\n100%|████████████████████████████████████████████████████████████████████████████████████████| 50/50 [15:22<00:00, 18.45s/it]\n[2025-03-02 10:52:29,068] INFO: Saving generated video to 1.pm4\ncache_video failed, error: result type Float can't be cast to the desired output type Byte\n[2025-03-02 10:52:29,291] INFO: Finished.\n(wan2.1) PS D:\\Dev\\Wan2.1> pip install imageio-ffmpeg                                                                         \nRequirement already satisfied: imageio-ffmpeg in c:\\users\\gary\\.conda\\envs\\wan2.1\\lib\\site-packages (0.6.0)                   \n(wan2.1) PS D:\\Dev\\Wan2.1> \n```"
      },
      {
        "user": "dieptran2500",
        "created_at": "2025-03-02T17:38:19Z",
        "body": "i have same problem , any one know how to fix?"
      },
      {
        "user": "lxm065",
        "created_at": "2025-03-04T02:15:27Z",
        "body": "i have the same problem , and i install imageio-ffmpeg\n\nError opening output files: Invalid argument\n\n\n[2025-03-04 10:03:31,847] INFO: Saving generated video to t2v-1.3B_832*480_1_1_Two_anthropomorphic_cats_in_comfy_boxing_gear_and__20250304_100331.mp4\n[out#0/mp4 @ 00000128ae1f02c0] Error opening output D:\\ai\\Wan2.1\\t2v-1.3B_832*480_1_1_Two_anthropomorphic_cats_in_comfy_boxing_gear_and__20250304_100331.mp4: Invalid argument\nError opening output file D:\\ai\\Wan2.1\\t2v-1.3B_832*480_1_1_Two_anthropomorphic_cats_in_comfy_boxing_gear_and__20250304_100331.mp4.\nError opening output files: Invalid argument\ncache_video failed, error: result type Float can't be cast to the desired output type Byte\n[2025-03-04 10:03:32,273] INFO: Finished."
      },
      {
        "user": "garysdevil",
        "created_at": "2025-03-13T14:03:56Z",
        "body": "> ```\n> heckpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n> [2025-03-02 10:33:59,629] INFO: Input prompt: Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\n> [2025-03-02 10:33:59,629] INFO: Creating WanT2V pipeline.\n> [2025-03-02 10:34:36,622] INFO: loading ./Wan2.1-T2V-1.3B\\models_t5_umt5-xxl-enc-bf16.pth\n> [2025-03-02 10:34:41,096] INFO: loading ./Wan2.1-T2V-1.3B\\Wan2.1_VAE.pth\n> [2025-03-02 10:34:41,508] INFO: Creating WanModel from ./Wan2.1-T2V-1.3B\n> [2025-03-02 10:34:43,656] INFO: Generating video ...\n> 100%|████████████████████████████████████████████████████████████████████████████████████████| 50/50 [15:22<00:00, 18.45s/it]\n> [2025-03-02 10:52:29,068] INFO: Saving generated video to 1.pm4\n> cache_video failed, error: result type Float can't be cast to the desired output type Byte\n> [2025-03-02 10:52:29,291] INFO: Finished.\n> (wan2.1) PS D:\\Dev\\Wan2.1> pip install imageio-ffmpeg                                                                         \n> Requirement already satisfied: imageio-ffmpeg in c:\\users\\gary\\.conda\\envs\\wan2.1\\lib\\site-packages (0.6.0)                   \n> (wan2.1) PS D:\\Dev\\Wan2.1> \n> ```\n\nI resolve this question by setting an absolute path `--save_file \"D:\\Dev\\Wan2.1\\2.1.mp4\" `"
      }
    ],
    "satisfaction_conditions": [
      "A solution that resolves the 'cache_video failed, error: result type Float can't be cast to the desired output type Byte' error",
      "A way to successfully save the generated video output to a file",
      "A solution that addresses dependency or configuration issues in the video generation pipeline",
      "A workaround for file path handling issues"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-14 01:12:35"
    },
    "dockerfile": "FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PYTHONUNBUFFERED=1\nENV PATH=\"/usr/local/cuda/bin:${PATH}\"\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    curl \\\n    python3-dev \\\n    python3-pip \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository at the specific commit\nRUN git clone https://github.com/Wan-Video/Wan2.1.git /app && \\\n    cd /app && \\\n    git checkout a326079926a4a347ecda8863dc40ba2d7680a294\n\n# Upgrade pip and install PyTorch 2.4.0 with CUDA support first\nRUN pip3 install --no-cache-dir --upgrade pip && \\\n    pip3 install --no-cache-dir torch>=2.4.0 torchvision>=0.17.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install project dependencies in batches to improve build reliability\nRUN pip3 install --no-cache-dir numpy scipy matplotlib && \\\n    pip3 install --no-cache-dir opencv-python pillow && \\\n    pip3 install --no-cache-dir tqdm transformers einops && \\\n    pip3 install --no-cache-dir huggingface_hub modelscope && \\\n    pip3 install --no-cache-dir -r requirements.txt\n\n# Create model and output directories\nRUN mkdir -p /models /output\n\n# Fix for the Float to Byte casting error in cache_video\n# Modify the code to handle the type conversion properly\nRUN sed -i 's/np.array(frames)/np.array(frames, dtype=np.uint8)/g' wan/utils/utils.py\n\n# Set environment variables for better performance\nENV OMP_NUM_THREADS=1\nENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n\n# Set up a volume for models and output\nVOLUME [\"/models\", \"/output\"]\n\n# Set the working directory\nWORKDIR /app\n\n# Example usage:\n# docker run --gpus all -v /path/to/models:/models -v /path/to/output:/output wan2-1-image \\\n#   python generate.py --task t2v-1.3B --size 480*832 --ckpt_dir /models/Wan2.1-T2V-1.3B \\\n#   --prompt \"your prompt here\" --save_file /output/output.mp4"
  },
  {
    "number": 50,
    "title": "运行1.3B的gradio会自动下载14B的模型",
    "created_at": "2025-02-26T09:25:40Z",
    "closed_at": "2025-03-04T09:47:36Z",
    "commit_id": "73648654c5242bd8e11bd05ea36ffa87a6424ff6",
    "labels": [],
    "url": "https://github.com/Wan-Video/Wan2.1/issues/50",
    "body": "运行时会下载一个models--Qwen--Qwen2.5-14B-Instruct文件夹，28G大小",
    "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/50/comments",
    "author": "jasonlbx13",
    "comments": [
      {
        "user": "Memoriaaa",
        "created_at": "2025-02-26T09:40:03Z",
        "body": "gradio的demo默认开了提示词增强，会调用Qwen2.5，你可以改下代码关了\n\n参考：\n```python\n# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\nimport argparse\nimport os.path as osp\nimport sys\nimport warnings\n\nimport gradio as gr\n\nwarnings.filterwarnings('ignore')\n\n# Model\nsys.path.insert(0, '/'.join(osp.realpath(__file__).split('/')[:-2]))\nimport wan\nfrom wan.configs import WAN_CONFIGS\nfrom wan.utils.prompt_extend import DashScopePromptExpander, QwenPromptExpander\nfrom wan.utils.utils import cache_video\n\n# Global Var\nprompt_expander = None\nwan_t2v = None\n\n\n# Button Func\ndef prompt_enc(prompt, tar_lang):\n    return prompt\n    # global prompt_expander\n    # prompt_output = prompt_expander(prompt, tar_lang=tar_lang.lower())\n    # if prompt_output.status == False:\n    #     return prompt\n    # else:\n    #     return prompt_output.prompt\n\n\ndef t2v_generation(txt2vid_prompt, resolution, sd_steps, guide_scale,\n                   shift_scale, seed, n_prompt):\n    global wan_t2v\n    # print(f\"{txt2vid_prompt},{resolution},{sd_steps},{guide_scale},{shift_scale},{seed},{n_prompt}\")\n\n    W = int(resolution.split(\"*\")[0])\n    H = int(resolution.split(\"*\")[1])\n    video = wan_t2v.generate(\n        txt2vid_prompt,\n        size=(W, H),\n        shift=shift_scale,\n        sampling_steps=sd_steps,\n        guide_scale=guide_scale,\n        n_prompt=n_prompt,\n        seed=seed,\n        offload_model=False)\n\n    cache_video(\n        tensor=video[None],\n        save_file=\"example.mp4\",\n        fps=16,\n        nrow=1,\n        normalize=True,\n        value_range=(-1, 1))\n\n    return \"example.mp4\"\n\n\n# Interface\ndef gradio_interface():\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\n                    <div style=\"text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;\">\n                        Wan2.1 (T2V-1.3B)\n                    </div>\n                    <div style=\"text-align: center; font-size: 16px; font-weight: normal; margin-bottom: 20px;\">\n                        Wan: Open and Advanced Large-Scale Video Generative Models.\n                    </div>\n                    \"\"\")\n\n        with gr.Row():\n            with gr.Column():\n                txt2vid_prompt = gr.Textbox(\n                    label=\"Prompt\",\n                    placeholder=\"Describe the video you want to generate\",\n                )\n                tar_lang = gr.Radio(\n                    choices=[\"CH\", \"EN\"],\n                    label=\"Target language of prompt enhance\",\n                    value=\"CH\")\n                run_p_button = gr.Button(value=\"Prompt Enhance\")\n\n                with gr.Accordion(\"Advanced Options\", open=True):\n                    resolution = gr.Dropdown(\n                        label='Resolution(Width*Height)',\n                        choices=[\n                            '480*832',\n                            '832*480',\n                            '624*624',\n                            '704*544',\n                            '544*704',\n                        ],\n                        value='480*832')\n\n                    with gr.Row():\n                        sd_steps = gr.Slider(\n                            label=\"Diffusion steps\",\n                            minimum=1,\n                            maximum=1000,\n                            value=50,\n                            step=1)\n                        guide_scale = gr.Slider(\n                            label=\"Guide scale\",\n                            minimum=0,\n                            maximum=20,\n                            value=6.0,\n                            step=1)\n                    with gr.Row():\n                        shift_scale = gr.Slider(\n                            label=\"Shift scale\",\n                            minimum=0,\n                            maximum=20,\n                            value=8.0,\n                            step=1)\n                        seed = gr.Slider(\n                            label=\"Seed\",\n                            minimum=-1,\n                            maximum=2147483647,\n                            step=1,\n                            value=-1)\n                    n_prompt = gr.Textbox(\n                        label=\"Negative Prompt\",\n                        placeholder=\"Describe the negative prompt you want to add\"\n                    )\n\n                run_t2v_button = gr.Button(\"Generate Video\")\n\n            with gr.Column():\n                result_gallery = gr.Video(\n                    label='Generated Video', interactive=False, height=600)\n\n        run_p_button.click(\n            fn=prompt_enc,\n            inputs=[txt2vid_prompt, tar_lang],\n            outputs=[txt2vid_prompt])\n\n        run_t2v_button.click(\n            fn=t2v_generation,\n            inputs=[\n                txt2vid_prompt, resolution, sd_steps, guide_scale, shift_scale,\n                seed, n_prompt\n            ],\n            outputs=[result_gallery],\n        )\n\n    return demo\n\n\n# Main\ndef _parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"Generate a video from a text prompt or image using Gradio\")\n    parser.add_argument(\n        \"--ckpt_dir\",\n        type=str,\n        default=\"cache\",\n        help=\"The path to the checkpoint directory.\")\n    parser.add_argument(\n        \"--prompt_extend_method\",\n        type=str,\n        default=\"local_qwen\",\n        choices=[\"dashscope\", \"local_qwen\", \"None\"],\n        help=\"The prompt extend method to use.\")\n    parser.add_argument(\n        \"--prompt_extend_model\",\n        type=str,\n        default=None,\n        help=\"The prompt extend model to use.\")\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == '__main__':\n    args = _parse_args()\n\n    # print(\"Step1: Init prompt_expander...\", end='', flush=True)\n    # if args.prompt_extend_method == \"dashscope\":\n    #     prompt_expander = DashScopePromptExpander(\n    #         model_name=args.prompt_extend_model, is_vl=False)\n    # elif args.prompt_extend_method == \"local_qwen\":\n    #     prompt_expander = QwenPromptExpander(\n    #         model_name=args.prompt_extend_model, is_vl=False, device=0)\n    # else:\n    #     raise NotImplementedError(\n    #         f\"Unsupport prompt_extend_method: {args.prompt_extend_method}\")\n    # print(\"done\", flush=True)\n\n    print(\"Step2: Init 1.3B t2v model...\", end='', flush=True)\n    cfg = WAN_CONFIGS['t2v-1.3B']\n    wan_t2v = wan.WanT2V(\n        config=cfg,\n        checkpoint_dir=args.ckpt_dir,\n        device_id=0,\n        rank=0,\n        t5_fsdp=False,\n        dit_fsdp=False,\n        use_usp=False,\n    )\n    print(\"done\", flush=True)\n\n    demo = gradio_interface()\n    demo.launch(server_name=\"0.0.0.0\", share=False, server_port=8904)\n```"
      },
      {
        "user": "jasonlbx13",
        "created_at": "2025-02-26T09:42:36Z",
        "body": "感谢您的解答！\n\n"
      },
      {
        "user": "fallbernana123456",
        "created_at": "2025-02-26T09:48:38Z",
        "body": "> 感谢您的解答！\n\n你在阿里云上申请一个 api-key，再使用--prompt_extend_method 'dashscope'参数就可以使用了\n"
      }
    ],
    "satisfaction_conditions": [
      "A solution that prevents the automatic download of the large 14B Qwen model",
      "Code modification guidance to disable the prompt enhancement feature",
      "Understanding of why the large model was being downloaded"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-14 01:13:08"
    },
    "dockerfile": "FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PYTHONUNBUFFERED=1\nENV PATH=\"/usr/local/cuda/bin:${PATH}\"\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    curl \\\n    python3-dev \\\n    python3-pip \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository at the specific commit\nRUN git clone https://github.com/Wan-Video/Wan2.1.git /app && \\\n    cd /app && \\\n    git checkout 73648654c5242bd8e11bd05ea36ffa87a6424ff6\n\n# Upgrade pip and install PyTorch 2.4.0 with CUDA support first\nRUN pip3 install --no-cache-dir --upgrade pip && \\\n    pip3 install --no-cache-dir torch>=2.4.0 torchvision>=0.17.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Fix for Issue #50: Prevent automatic download of Qwen2.5-14B-Instruct model\n# Create a modified requirements.txt file without the Qwen model dependency\nRUN grep -v \"qwen\" requirements.txt > requirements_modified.txt || true\n\n# Install project dependencies in batches to improve build reliability\nRUN pip3 install --no-cache-dir numpy scipy matplotlib && \\\n    pip3 install --no-cache-dir opencv-python pillow && \\\n    pip3 install --no-cache-dir tqdm transformers einops && \\\n    pip3 install --no-cache-dir huggingface_hub modelscope && \\\n    pip3 install --no-cache-dir -r requirements_modified.txt\n\n# Create model directories for user to mount models\nRUN mkdir -p /models/Wan2.1-T2V-1.3B /models/Wan2.1-T2V-14B /models/Wan2.1-I2V-14B-480P /models/Wan2.1-I2V-14B-720P\n\n# Set environment variables for better performance\nENV OMP_NUM_THREADS=1\nENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n\n# Set up a volume for models and output\nVOLUME [\"/models\", \"/output\"]\n\n# Set the working directory\nWORKDIR /app"
  }
]