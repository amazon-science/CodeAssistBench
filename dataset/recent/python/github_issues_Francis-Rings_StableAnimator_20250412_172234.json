[
  {
    "number": 92,
    "title": "微调后的模型推理结果与微调训练时的验证结果差异问题",
    "created_at": "2025-02-26T03:15:56Z",
    "closed_at": "2025-02-27T02:26:47Z",
    "commit_id": "0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b",
    "labels": [],
    "url": "https://github.com/Francis-Rings/StableAnimator/issues/92",
    "body": "你好，我想知道保存下来的其他模型比如model.safetensors、model_1、model_2等怎么用呢？\n还是说这些都是不需要的，重点只是checkpoint-{global_step}这三个模型？\n我在用微调后的三个模型进行推理时发现生成的动作与微调训练时同步数生成的验证gif文件的动作不一致。\n下述视频是运行推理代码得到的，大概由于gif文件太大，上传失败，我想告诉你的是微调训练时同步数生成的验证gif得到的手部是正常的，与下述视频不同。\n",
    "comments_url": "https://api.github.com/repos/Francis-Rings/StableAnimator/issues/92/comments",
    "author": "YxY-HK",
    "comments": [
      {
        "user": "Francis-Rings",
        "created_at": "2025-02-27T01:50:15Z",
        "body": "Hi, you only need to load `checkpoint-{global_step}/pose_net-{global_step}.pth`, `checkpoint-{global_step}/face_encoder-{global_step}.pth`, `checkpoint-{global_step}/unet-{global_step}.pth` into the StableAnimator for conducting inference. For more details, please refer to the README file."
      },
      {
        "user": "YxY-HK",
        "created_at": "2025-02-27T02:26:47Z",
        "body": "Ok，Thx"
      }
    ],
    "satisfaction_conditions": [
      "Clear guidance on which model files are necessary for inference",
      "Instructions on how to properly load the fine-tuned models for inference",
      "Reference to documentation for additional details"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-14 01:09:57"
    },
    "dockerfile": "FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04\n\n# Set environment variables to avoid interactive installation prompts\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=Asia/Shanghai\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    python3 \\\n    python3-pip \\\n    wget \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/Francis-Rings/StableAnimator.git . && \\\n    git checkout 0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b\n\n# Upgrade pip to avoid dependency conflicts\nRUN pip3 install --upgrade pip\n\n# Install filelock with compatible version first\nRUN pip3 install filelock==3.12.2\n\n# Install PyTorch with CUDA support using a compatible version\nRUN pip3 install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118 --no-deps && \\\n    pip3 install --no-deps typing_extensions sympy networkx jinja2 && \\\n    pip3 install numpy\n\n# Install project dependencies\nRUN pip3 install -r requirements.txt || true\n\n# Install additional dependencies that might be needed for model inference\nRUN pip3 install accelerate transformers safetensors diffusers\n\n# Set environment variables for CUDA\nENV CUDA_VISIBLE_DEVICES=0\n\n# Create directories for model weights and outputs\nRUN mkdir -p models/weights outputs\n\n# Set the working directory to the project root\nWORKDIR /app\n\n# Note: This Dockerfile sets up the environment for the StableAnimator project\n# To use the fine-tuned models, place your model files in the models/weights directory\n# The issue mentions model.safetensors and checkpoint-* models for inference"
  },
  {
    "number": 85,
    "title": "Error Loading Model State Dict: Missing Keys in UNetSpatioTemporalConditionModel",
    "created_at": "2025-02-03T02:34:48Z",
    "closed_at": "2025-02-05T08:59:50Z",
    "commit_id": "0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b",
    "labels": [],
    "url": "https://github.com/Francis-Rings/StableAnimator/issues/85",
    "body": "**Description:**  \nAfter training the model using the provided training script, I encountered an error when trying to load the model for inference. The error indicates that several keys are missing from the state dict of the `UNetSpatioTemporalConditionModel`. It appears that there might be a mismatch between the trained model and the expected state dict keys during loading.\n\n**Error Message:**  \n```python\nunet_state_dict = torch.load(args.unet_model_name_or_path, map_location=\"cpu\")\nTraceback (most recent call last):\n  File \"/workspace/StableAnimator/inference_basic.py\", line 319, in <module>\n    unet.load_state_dict(unet_state_dict, strict=True)\n  File \"/workspace/StableAnimator/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\n    raise RuntimeError(\nRuntimeError: Error(s) in loading state_dict for UNetSpatioTemporalConditionModel:\n        Missing key(s) in state_dict: \"down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"mid_block.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"mid_block.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\".\n```\n\n**Reproduction Steps:**  \n1. **Training:**  \n   The model was trained using the following bash command:\n   ```bash\n   CUDA_VISIBLE_DEVICES=3,7,6,5,4,2,1,0 accelerate launch train_single.py \\\n    --pretrained_model_name_or_path=\"path/checkpoints/stable-video-diffusion-img2vid-xt\" \\\n    --finetune_mode=True \\\n    --posenet_model_finetune_path=\"path/checkpoints/Animation/pose_net.pth\" \\\n    --face_encoder_finetune_path=\"path/checkpoints/Animation/face_encoder.pth\" \\\n    --unet_model_finetune_path=\"path/checkpoints/Animation/unet.pth\" \\\n    --output_dir=\"path/checkpoints/Animation2\" \\\n    --data_root_path=\"path/preprocess/\" \\\n    --data_path=\"path/preprocess/video_path.txt\" \\\n    --dataset_width=576 \\\n    --dataset_height=1024 \\\n    --validation_image_folder=\"path/validation/images\" \\\n    --validation_control_folder=\"path/validation/poses\" \\\n    --validation_image=\"path/validation/reference.png\" \\\n    --num_workers=8 \\\n    --lr_warmup_steps=500 \\\n    --sample_n_frames=8 \\\n    --learning_rate=5e-6 \\\n    --per_gpu_batch_size=1 \\\n    --num_train_epochs=600 \\\n    --mixed_precision=\"fp16\" \\\n    --gradient_accumulation_steps=1 \\\n    --checkpointing_steps=3000 \\\n    --validation_steps=9999999 \\\n    --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --enable_xformers_memory_efficient_attention \\\n    --checkpoints_total_limit=90000 \\\n    --resume_from_checkpoint=\"latest\"\n   ```\n\n2. **Loading:**  \n   After training, I attempted to load the model with the following code:\n   ```python\n   unet_state_dict = torch.load(args.unet_model_name_or_path, map_location=\"cpu\")\n   unet.load_state_dict(unet_state_dict, strict=True)\n   ```\n   This resulted in the error shown above.\n\n**Environment:**  \n- **Python:** 3.12.3\n- **PyTorch:** 2.5.1+cu124 \n- **Diffusers:** 0.32.1\n\n**Additional Context:**  \n- The error lists several missing keys in the state dict (e.g., `\"down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\"`, etc.).\n- This issue may indicate a mismatch between the model architecture used during training and the one expected during inference.  \n- Has there been any recent change in the model structure or naming conventions that could lead to this issue?\n\nAny help or guidance in resolving this issue would be greatly appreciated.",
    "comments_url": "https://api.github.com/repos/Francis-Rings/StableAnimator/issues/85/comments",
    "author": "cvecve147",
    "comments": [
      {
        "user": "Francis-Rings",
        "created_at": "2025-02-04T13:12:13Z",
        "body": "Hi, please check whether AnimationIDAttnNormalizedProcessor is activated. It seems that the weights of AnimationIDAttnNormalizedProcessor were not saved during training."
      },
      {
        "user": "cvecve147",
        "created_at": "2025-02-05T08:59:51Z",
        "body": "Thank you for your prompt response and valuable guidance. Upon further investigation, I discovered that the root cause of the issue was the use of the --enable_xformers_memory_efficient_attention parameter during training, which resulted in the AnimationIDAttnNormalizedProcessor weights not being saved correctly. After removing this parameter, the model weights are now saved and loaded properly. I greatly appreciate your support and insights in resolving this matter!"
      }
    ],
    "satisfaction_conditions": [
      "Identification of the root cause of the missing keys in the model state dict",
      "A specific parameter or configuration causing the model loading issue",
      "A practical solution to resolve the model loading error"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-14 01:10:10"
    },
    "dockerfile": "FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nENV DEBIAN_FRONTEND=noninteractive\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wget \\\n    curl \\\n    python3 \\\n    python3-pip \\\n    python3-dev \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1 \\\n    libglib2.0-0 \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/Francis-Rings/StableAnimator.git . && \\\n    git checkout 0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b\n\n# Install PyTorch first\nRUN pip3 install --no-cache-dir torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install diffusers separately\nRUN pip3 install --no-cache-dir diffusers==0.32.1\n\n# Install other dependencies in batches\nRUN pip3 install --no-cache-dir numpy opencv-python pillow matplotlib tqdm scikit-image && \\\n    pip3 install --no-cache-dir transformers accelerate einops omegaconf && \\\n    pip3 install --no-cache-dir onnxruntime onnx insightface && \\\n    pip3 install --no-cache-dir ninja gradio==4.19.2 && \\\n    pip3 install --no-cache-dir bitsandbytes==0.41.3 xformers==0.0.23.post1\n\n# Create necessary directories for model checkpoints and data\nRUN mkdir -p checkpoints/DWPose \\\n    checkpoints/Animation \\\n    checkpoints/SVD/feature_extractor \\\n    checkpoints/SVD/image_encoder \\\n    checkpoints/SVD/scheduler \\\n    checkpoints/SVD/unet \\\n    checkpoints/SVD/vae \\\n    models/antelopev2 \\\n    animation_data/rec \\\n    animation_data/vec \\\n    validation/ground_truth \\\n    validation/poses\n\n# Create a file with guidance for the UNetSpatioTemporalConditionModel issue\nRUN echo \"To fix the UNetSpatioTemporalConditionModel state dict loading issue, try loading the model with strict=False or update the model architecture to match the trained weights. The missing keys are related to the transformer attention processors.\" > model_loading_fix.txt\n\nENV PYTHONPATH=\"${PYTHONPATH}:/app\"\n\nCMD [\"echo\", \"StableAnimator environment is ready. To address the model state dict loading issue, check model_loading_fix.txt for guidance.\"]"
  }
]