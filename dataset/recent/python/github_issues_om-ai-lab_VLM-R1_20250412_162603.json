[
  {
    "number": 133,
    "title": "step\u4e0e\u4f7f\u7528\u7684\u6570\u636e\u91cf\u4e0d\u7b26",
    "created_at": "2025-03-07T12:41:55Z",
    "closed_at": "2025-03-12T08:13:32Z",
    "labels": [],
    "url": "https://github.com/om-ai-lab/VLM-R1/issues/133",
    "body": "\u6211\u4f7f\u75287k\u6570\u636e\u5e76\u8c03\u7528\u5982\u4e0b\u811a\u672c\u8fdb\u884c\u8bad\u7ec3\uff0cstep\u6570\u5374\u67093500\uff0c\u8bf7\u95ee\u8981\u600e\u4e48\u89e3\u51b3\n\n#!/bin/bash\n\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff08\u5982\u679c\u9700\u8981\uff09\nexport MASTER_ADDR=\"127.0.0.1\"\nexport MASTER_PORT=\"12346\"\nexport RUN_NAME=\"original_reward_4k_4k\"  # \u4f60\u53ef\u4ee5\u66ff\u6362\u8fd9\u4e2a\u4e3a\u5b9e\u9645\u7684RUN_NAME\n\n# \u8fd0\u884c\u547d\u4ee4\ntorchrun --nproc_per_node=\"8\" \\\n    --nnodes=\"1\" \\\n    --node_rank=\"0\" \\\n    --master_addr=\"$MASTER_ADDR\" \\\n    --master_port=\"$MASTER_PORT\" \\\n    src/open_r1/grpo_text.py \\\n    --deepspeed local_scripts/zero3.json \\\n    --output_dir output/$RUN_NAME \\\n    --model_name_or_path /workspace/denglinger/Qwen2.5-VL-3B-Instruct \\\n    --dataset_name /workspace/denglinger/VLM-R1-main/src/open-r1-multimodal/data_config/chartqa.yaml \\\n    --image_root /workspace/denglinger/Dataset/ChartQA_Dataset \\\n    --max_prompt_length 1024 \\\n    --num_generations 8 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 2 \\\n    --logging_steps 1 \\\n    --bf16 \\\n    --torch_dtype bfloat16 \\\n    --data_seed 42 \\\n    --report_to wandb \\\n    --gradient_checkpointing false \\\n    --attn_implementation flash_attention_2 \\\n    --num_train_epochs 1 \\\n    --run_name $RUN_NAME \\\n    --save_steps 100 \\\n    --save_only_model true\n",
    "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/133/comments",
    "author": "dle666",
    "comments": [
      {
        "user": "SZhanZ",
        "created_at": "2025-03-07T13:02:42Z",
        "body": "\u4f60\u597d\uff0c\u6211\u4eec\u7684train sampler\u66f4\u65b0\u8fc7\u4e86\uff0c\u73b0\u5728\u8981\u628a`--per_device_train_batch_size`\u8bbe\u7f6e\u4e3a`8`\uff0c\u5e94\u8be5\u5c31\u6b63\u5e38\u4e86\u3002"
      },
      {
        "user": "dle666",
        "created_at": "2025-03-07T13:07:42Z",
        "body": "\u5df2\u89e3\u51b3\uff0c\u975e\u5e38\u611f\u8c22\n"
      }
    ],
    "satisfaction_conditions": [
      "An explanation of why the number of training steps doesn't match the expected amount based on the dataset size",
      "A configuration adjustment that aligns the training steps with the actual dataset size",
      "Clear instructions on which parameter needs to be modified to fix the step count issue"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-14 01:06:23"
    }
  },
  {
    "number": 83,
    "title": "\u6709\u5173\u56fe\u50cf\u5206\u8fa8\u7387\u65b9\u9762\u7684\u7591\u95ee",
    "created_at": "2025-02-27T09:38:53Z",
    "closed_at": "2025-02-28T01:15:34Z",
    "labels": [],
    "url": "https://github.com/om-ai-lab/VLM-R1/issues/83",
    "body": "\u975e\u5e38\u6f02\u4eae\u7684\u5de5\u4f5c\uff01\u6709\u4e2a\u7591\u95ee\uff0c\u5173\u4e8e\u51c6\u5907\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u3002\u8bf7\u95eeR1\u8bad\u7ec3\u8fc7\u7a0b\u662f\u52a8\u6001\u5206\u8fa8\u7387\u8fd8\u662f\u56fa\u5b9a\u5206\u8fa8\u7387\uff1fLLaMA-Factory\u7684SFT\u8bad\u7ec3\u5462(\u5047\u8bbe\u7528\u7684\u662fQWen2.5-VL-3B)\uff1f",
    "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/83/comments",
    "author": "CaptainEven",
    "comments": [
      {
        "user": "SZhanZ",
        "created_at": "2025-02-27T10:32:29Z",
        "body": "Hello\uff0c\u4f60\u597d\nR1\u548cSFT\u90fd\u662f\u52a8\u6001\u5206\u8fa8\u7387"
      },
      {
        "user": "CaptainEven",
        "created_at": "2025-02-28T01:15:34Z",
        "body": "\u597d\u7684\uff0c\u611f\u8c22\uff01"
      }
    ],
    "satisfaction_conditions": [
      "Clear information about the resolution handling approach in both R1 training and LLaMA-Factory SFT training",
      "Concise and direct answer to technical questions about image resolution in model training"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-14 01:06:29"
    }
  },
  {
    "number": 81,
    "title": "About OOM",
    "created_at": "2025-02-27T05:33:56Z",
    "closed_at": "2025-03-31T06:47:14Z",
    "labels": [],
    "url": "https://github.com/om-ai-lab/VLM-R1/issues/81",
    "body": "My parameters are as follows: \n    --nproc_per_node=\"6\"\n    --num_generations 4 \n    --per_device_train_batch_size 4 \n    --gradient_accumulation_steps 1 \n\nI used 6 x H20\uff086 x 96G\uff09, Out of memory error after 4 steps of training.\nThat doesn't seem to fit with the minimum configuration, and what should I do to fix it?",
    "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/81/comments",
    "author": "Lane315",
    "comments": [
      {
        "user": "qizheng-1-1z",
        "created_at": "2025-02-27T05:57:42Z",
        "body": "i got the same problem,sad.."
      },
      {
        "user": "LaFeuilleMorte",
        "created_at": "2025-02-27T06:22:04Z",
        "body": "> My parameters are as follows: --nproc_per_node=\"6\" --num_generations 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 1\n> \n> I used 6 x H20\uff086 x 96G\uff09, Out of memory error after 4 steps of training. That doesn't seem to fit with the minimum configuration, and what should I do to fix it?\n\nMaybe you can reduce per_device_train_batch_size to 1?"
      },
      {
        "user": "Lane315",
        "created_at": "2025-02-27T06:48:33Z",
        "body": "> > My parameters are as follows: --nproc_per_node=\"6\" --num_generations 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 1\n> > I used 6 x H20\uff086 x 96G\uff09, Out of memory error after 4 steps of training. That doesn't seem to fit with the minimum configuration, and what should I do to fix it?\n> \n> Maybe you can reduce per_device_train_batch_size to 1?\n\nYou are right. I know it's possible to reduce the batchsize, but this increases the training time. sad.."
      }
    ],
    "satisfaction_conditions": [
      "A solution that reduces memory usage while maintaining reasonable training time",
      "An explanation of why their current configuration is causing OOM errors despite seemingly meeting minimum requirements",
      "Alternative approaches to resolve OOM errors beyond just reducing batch size"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-14 01:06:37"
    }
  },
  {
    "number": 5,
    "title": "Still missing gta_subsample.json",
    "created_at": "2025-02-18T13:56:51Z",
    "closed_at": "2025-02-18T17:21:27Z",
    "labels": [],
    "url": "https://github.com/om-ai-lab/VLM-R1/issues/5",
    "body": "Hi, authors. Thanks for your great work. As the title said, the gta_subsamples.json is still missing in you r provided link in antohr issue.\n\n#2 ",
    "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/5/comments",
    "author": "yueyang130",
    "comments": [
      {
        "user": "SZhanZ",
        "created_at": "2025-02-18T15:14:59Z",
        "body": "Sorry, I uploaded an incorrect zip file. Let me upload it again."
      },
      {
        "user": "SZhanZ",
        "created_at": "2025-02-18T16:28:53Z",
        "body": "The current one should be ok.\n"
      },
      {
        "user": "yueyang130",
        "created_at": "2025-02-18T17:21:27Z",
        "body": "thanks\uff01"
      }
    ],
    "satisfaction_conditions": [
      "Access to the missing gta_subsample.json file",
      "A working download link for the required file",
      "Timely response to the file access request"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-14 01:06:51"
    }
  }
]