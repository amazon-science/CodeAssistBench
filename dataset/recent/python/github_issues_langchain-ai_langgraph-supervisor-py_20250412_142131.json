[
  {
    "number": 25,
    "title": "Incorrect Tool Calls in Multi Agents with Supervisor",
    "created_at": "2025-02-21T15:02:18Z",
    "closed_at": "2025-02-26T02:15:13Z",
    "commit_id": "bc0030b7f8761eef253da57e20340f20254018ab",
    "labels": [],
    "url": "https://github.com/langchain-ai/langgraph-supervisor-py/issues/25",
    "body": "Hello everyone, I'm having trouble getting the MultiAgents to work together correctly with the Langgraph-Supervisor.\n\nWhen I have more than one agent, each with their own tools, it seems that an agent keeps trying to call the tools of other agents several times, until he receives an error saying that that tool is not available and he must try one of the available ones, listing the tools that exist. Only at this moment does the agent make the correct call to the right tool and finally respond correctly.\n\nI don't understand why the agent keeps calling another agent's tools if it was binded with only its own tools. \n\nBelow I put the Debug excerpt that shows this moment of the problem and also my complete source code for the three agents and the supervisor so that you can understand what is happening and can help me solve this problem.\n\nThank you in advance for everyone's help.\n\n```python\nfrom typing import Literal\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# --- Temperature tool ---\n@tool\ndef get_temperature(location: str):\n    \"\"\"Call to get the current temperature on location.\"\"\"\n    if location.lower() in [\"munich\"]:\n        return \"It's 15 degrees Celsius.\"\n    else:\n        return \"It's 32 degrees Celsius.\"\n\n# We'll create a model and bind the tool so the LLM knows it can call `get_temperature`.\ntools = [get_temperature]\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n\n# --- Existing agent workflow definition ---\ndef call_model(state: MessagesState):\n    \"\"\"Call the LLM with the conversation so far.\"\"\"\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    \"\"\"If there's a tool call requested, go to 'tools', else end.\"\"\"\n    messages = state[\"messages\"]\n    print(\"Temperature Agent\")\n    print(messages)\n    last_message = messages[-1]\n    print(last_message.tool_calls)\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ntemperature_workflow = StateGraph(MessagesState)\n\ntool_node = ToolNode(tools)\n\ntemperature_workflow.add_node(\"agent\", call_model)\ntemperature_workflow.add_node(\"tools\", tool_node)\n\ntemperature_workflow.add_edge(START, \"agent\")\ntemperature_workflow.add_conditional_edges(\"agent\", should_continue)\ntemperature_workflow.add_edge(\"tools\", \"agent\")\n\ntemperature_agent_graph =  temperature_workflow.compile(name=\"temperature_agent\")\n\nfrom typing import Literal\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# --- Wind tool ---\n@tool\ndef get_wind(location: str):\n    \"\"\"Call to get the current wind on location.\"\"\"\n    if location.lower() in [\"munich\"]:\n        return \"It's High Wind.\"\n    else:\n        return \"It's Low Wind.\"\n\n# We'll create a model and bind the tool so the LLM knows it can call `get_wind`.\ntools = [get_wind]\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n#model = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\").bind_tools(tools)\n# --- Existing agent workflow definition ---\ndef call_model(state: MessagesState):\n    \"\"\"Call the LLM with the conversation so far.\"\"\"\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    \"\"\"If there's a tool call requested, go to 'tools', else end.\"\"\"\n    messages = state[\"messages\"]\n    print(\"Wind Agent\")\n    print(messages)\n    last_message = messages[-1]\n    print(last_message.tool_calls)\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\nwind_workflow = StateGraph(MessagesState)\n\ntool_node = ToolNode(tools)\n\nwind_workflow.add_node(\"agent\", call_model)\nwind_workflow.add_node(\"tools\", tool_node)\n\nwind_workflow.add_edge(START, \"agent\")\nwind_workflow.add_conditional_edges(\"agent\", should_continue)\nwind_workflow.add_edge(\"tools\", \"agent\")\n\nwind_agent_graph =  wind_workflow.compile(name=\"wind_agent\")\n\nfrom typing import Literal\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# --- Rain tool ---\n@tool\ndef get_rain(location: str):\n    \"\"\"Call to get the current rain on location.\"\"\"\n    if location.lower() in [\"munich\"]:\n        return \"It's High rain.\"\n    else:\n        return \"It's Low rain.\"\n\n# We'll create a model and bind the tool so the LLM knows it can call `get_rain`.\ntools = [get_rain]\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n#model = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\").bind_tools(tools)\n\n# --- Existing agent workflow definition ---\ndef call_model(state: MessagesState):\n    \"\"\"Call the LLM with the conversation so far.\"\"\"\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    \"\"\"If there's a tool call requested, go to 'tools', else end.\"\"\"\n    messages = state[\"messages\"]\n    print(\"Rain Agent\")\n    print(messages)\n    last_message = messages[-1]\n    print(last_message.tool_calls)\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\nrain_workflow = StateGraph(MessagesState)\n\ntool_node = ToolNode(tools)\n\nrain_workflow.add_node(\"agent\", call_model)\nrain_workflow.add_node(\"tools\", tool_node)\n\nrain_workflow.add_edge(START, \"agent\")\nrain_workflow.add_conditional_edges(\"agent\", should_continue)\nrain_workflow.add_edge(\"tools\", \"agent\")\n\nrain_agent_graph =  rain_workflow.compile(name=\"rain_agent\")\n\nfrom langgraph_supervisor import create_supervisor\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\n\nclass MyCustomState(AgentState):\n    foo: str\n    bar: int\n\nmodel_supervisor = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools)\n#model_supervisor = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\").bind_tools(tools)\n\nsupervisor_workflow = create_supervisor(\n    agents=[temperature_agent_graph, wind_agent_graph, rain_agent_graph],\n    model=model_supervisor,\n    prompt=(\n        \"You are a supervisor managing the following agents: a temperature agent, a wind agent and a rain agent.\"\n        \"You know what each agent does and how they can help you. You can ask them to perform tasks and provide you with the results.\"\n        \"You also understand which agent to call for which task.\"\n        \"When one or more agents were called, You have to use the results from the agents to formulate your answer.\"\n    ),\n    output_mode=\"last_message\",\n    #output_mode=\"full_history\",\n    supervisor_name=\"supervisor_agent\",\n    state_schema=MyCustomState\n    \n)\n\nsupervisor_app = supervisor_workflow.compile()\n\nsupervisor_app.invoke(\n    {\"messages\": [HumanMessage(content=\"How is the wind in France and Munich?\")], \"foo\":\"foo\", \"bar\":1},\n    debug=True\n)\n\n```\n\n```\nWind Agent\n[HumanMessage(content='How is the wind in France and Munich?', additional_kwargs={}, response_metadata={}, id='f2ee6ddd-aaf3-44ee-b489-b2e22e60d8a5'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_NiPkIW0HXh6sNtmKXD1etU0t', 'function': {'arguments': '{}', 'name': 'transfer_to_wind_agent'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 160, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor_agent', id='run-587735ea-afaa-4a61-8377-29bfb5186b19-0', tool_calls=[{'name': 'transfer_to_wind_agent', 'args': {}, 'id': 'call_NiPkIW0HXh6sNtmKXD1etU0t', 'type': 'tool_call'}], usage_metadata={'input_tokens': 160, 'output_tokens': 14, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to wind_agent', name='transfer_to_wind_agent', id='cd987d67-ce6c-403e-9075-600b47d4a9a7', tool_call_id='call_NiPkIW0HXh6sNtmKXD1etU0t'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_MjKI7LAjJdXpgTjOZH5Mh6Gw', 'function': {'arguments': '{\"location\": \"France\"}', 'name': 'get_temperature'}, 'type': 'function'}, {'id': 'call_gGTMEca1TrrC1OVc8rR11BxE', 'function': {'arguments': '{\"location\": \"Munich\"}', 'name': 'get_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 88, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_00428b782a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e74fe640-0041-41cc-946d-d5f2eea3dcd7-0', tool_calls=[{'name': 'get_temperature', 'args': {'location': 'France'}, 'id': 'call_MjKI7LAjJdXpgTjOZH5Mh6Gw', 'type': 'tool_call'}, {'name': 'get_temperature', 'args': {'location': 'Munich'}, 'id': 'call_gGTMEca1TrrC1OVc8rR11BxE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 88, 'output_tokens': 46, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Error: get_temperature is not a valid tool, try one of [get_wind].', name='get_temperature', id='04ce3a9b-9e04-4663-bff8-65718e5771e5', tool_call_id='call_MjKI7LAjJdXpgTjOZH5Mh6Gw', status='error'), ToolMessage(content='Error: get_temperature is not a valid tool, try one of [get_wind].', name='get_temperature', id='d984472b-9b17-4ad8-bf7e-123552140066', tool_call_id='call_gGTMEca1TrrC1OVc8rR11BxE', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Dj5BqOlE1Ro968TbFHajNPEH', 'function': {'arguments': '{\"location\": \"France\"}', 'name': 'get_temperature'}, 'type': 'function'}, {'id': 'call_1yYapvolYW6fFrBWbRBoBOhE', 'function': {'arguments': '{\"location\": \"Munich\"}', 'name': 'get_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 183, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b13305b4-f4d9-48ae-b781-5a12142fb911-0', tool_calls=[{'name': 'get_temperature', 'args': {'location': 'France'}, 'id': 'call_Dj5BqOlE1Ro968TbFHajNPEH', 'type': 'tool_call'}, {'name': 'get_temperature', 'args': {'location': 'Munich'}, 'id': 'call_1yYapvolYW6fFrBWbRBoBOhE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 183, 'output_tokens': 46, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n[{'name': 'get_temperature', 'args': {'location': 'France'}, 'id': 'call_Dj5BqOlE1Ro968TbFHajNPEH', 'type': 'tool_call'}, {'name': 'get_temperature', 'args': {'location': 'Munich'}, 'id': 'call_1yYapvolYW6fFrBWbRBoBOhE', 'type': 'tool_call'}]\n\n```\n\nThis part of debug Messages shows the problem: \n\n*ToolMessage(content='Error: get_temperature is not a valid tool, try one of [get_wind]*\n",
    "comments_url": "https://api.github.com/repos/langchain-ai/langgraph-supervisor-py/issues/25/comments",
    "author": "giu-ferreira-cientista",
    "comments": [
      {
        "user": "XinyueZ",
        "created_at": "2025-02-25T14:35:02Z",
        "body": "I just found that this framework never calls my tools, does anyone else have similar problems?"
      },
      {
        "user": "vbarda",
        "created_at": "2025-02-26T02:15:13Z",
        "body": "@giu-ferreira-cientista it's because you're re-defining the model with different bound tools - you have this line for two different agents`model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)`"
      },
      {
        "user": "vbarda",
        "created_at": "2025-02-26T02:15:49Z",
        "body": "@XinyueZ feel free to open an issue with a concrete reproducible example -- likely it's a problem of the LLM you're using"
      },
      {
        "user": "giu-ferreira-cientista",
        "created_at": "2025-02-27T17:05:37Z",
        "body": "Thank you @vbarda ! My bad. Now it's working!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why tools are being incorrectly called across different agents",
      "Identification of the model binding configuration issue",
      "A solution that prevents agents from attempting to call tools that belong to other agents"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-14 01:15:36"
    },
    "dockerfile": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install git and other dependencies\nRUN apt-get update && \\\n    apt-get install -y git && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/langchain-ai/langgraph-supervisor-py.git /app && \\\n    cd /app && \\\n    git checkout bc0030b7f8761eef253da57e20340f20254018ab\n\n# Install pip dependencies\n# First install the package in development mode\nRUN pip install --no-cache-dir -e .\n\n# Install additional dependencies that might be needed for the issue\nRUN pip install --no-cache-dir \\\n    langchain-openai \\\n    langchain-anthropic \\\n    langchain-core\n\n# Set environment variables for OpenAI and Anthropic (user will need to provide their own keys)\nENV OPENAI_API_KEY=\"your-openai-api-key\"\nENV ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# Set Python path to include the current directory\nENV PYTHONPATH=/app:$PYTHONPATH\n\n# Working directory is ready for user to run tests or develop solutions\nCMD [\"bash\"]"
  }
]