{"task_id": "cab_lenient_1", "number": 11, "title": "docs(README.md): fix spelling", "created_at": "2025-03-29T18:24:35Z", "closed_at": "2025-03-29T18:28:51Z", "commit_id": "cba07f2ef9603d2e27386df4e339c685f5767136", "labels": [], "url": "https://github.com/EmberEmu/Hexi/pull/11", "body": "- accomodate => accommodate\r\n- determing => determining\r\n- read => reads", "comments_url": "https://api.github.com/repos/EmberEmu/Hexi/issues/11/comments", "author": "vladdoster", "comments": [{"user": "vladdoster", "created_at": "2025-03-29T18:25:39Z", "body": "Congratulations on reaching the front-page of HN!"}, {"user": "Chaosvex", "created_at": "2025-03-29T18:26:31Z", "body": "Well spotted, thanks! :)"}], "satisfaction_conditions": ["Acknowledgment of the spelling corrections identified in the README.md file", "Recognition of the contribution, however small"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:53"}, "dockerfile": null, "language": "c++"}
{"task_id": "cab_lenient_2", "number": 39, "title": "Custom termial program selection", "created_at": "2025-02-25T15:02:59Z", "closed_at": "2025-02-26T01:38:02Z", "commit_id": "9b424b5ad6cfc133de24c630b56f8e12ad36a22f", "labels": [], "url": "https://github.com/markeel/gdterm/issues/39", "body": "Hi! It's an great project! And I tried on my windows PC. It is working as I expected!\nThe only thing that I was using gitbash as my default terminal. So if there has an option to change default terminal in editor setting that could be the last part I am looking forward to.\nThanks!\nBy the way. I tried to change \"cmd\" to \"bash\" in **PtyProxyWin**, But it is not working!", "comments_url": "https://api.github.com/repos/markeel/gdterm/issues/39/comments", "author": "hakuhan", "comments": [{"user": "markeel", "created_at": "2025-02-25T19:52:32Z", "body": "You can put the git bash command in your initial command list in the editor settings for gdterm\n\nIt will then execute that command immediately when the terminal starts"}, {"user": "hakuhan", "created_at": "2025-02-26T01:38:02Z", "body": "Thanks! I use `bash --login -i` as start command line to achieve it."}], "satisfaction_conditions": ["A way to use Git Bash as the default terminal in the editor", "A configuration method that works within the existing editor settings", "A solution that works on Windows"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:49"}, "dockerfile": null, "language": "c++"}
{"task_id": "cab_lenient_3", "number": 6, "title": "How does TOS caching and GC roots interact?", "created_at": "2025-03-05T15:27:48Z", "closed_at": "2025-03-06T03:52:35Z", "commit_id": "b1fd84b0ef125cd82595d9bcb4b0589f9b8f4e0a", "labels": [], "url": "https://github.com/lynx-family/primjs/issues/6", "body": "Hi PrimJS developers. First of all, great work on the runtime!\n\nGC docs say that one of the roots is the `Interpreter Execution Stack`. However, the interpreter docs also say primjs does TOS caching. Once values are TOS cached into register x0/x1, don't they become invisible to the GC? In that case, how are the objects kept alive?\n\nThis is just a curious question, as I work on CPython, and we're planning to do TOS caching too.\n", "comments_url": "https://api.github.com/repos/lynx-family/primjs/issues/6/comments", "author": "Fidget-Spinner", "comments": [{"user": "sunzhipengbd", "created_at": "2025-03-06T03:48:18Z", "body": "Before the interpreter jumps to runtime, x0 and x1 are pushed onto the stack.\nThen the gc mark stage will scan the stack"}, {"user": "Fidget-Spinner", "created_at": "2025-03-06T03:52:25Z", "body": "Makes sense. Thanks!"}], "satisfaction_conditions": ["An explanation of how TOS cached values remain visible to the garbage collector", "A clear description of the mechanism that prevents memory leaks when using TOS caching", "Information that is transferable to other runtime implementations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:04"}, "dockerfile": null, "language": "c++"}
{"task_id": "cab_lenient_4", "number": 61, "title": "blocking_send_recv_example.py seems incorrect", "created_at": "2025-03-19T05:27:33Z", "closed_at": "2025-03-20T00:20:25Z", "commit_id": "4b073797578685afa65755e0893952eecb41a067", "labels": [], "url": "https://github.com/ai-dynamo/nixl/issues/61", "body": "Hello! Firstly, this library looks extremely promising, would solve a very big issue I was dealing with!\n\nI had a look at the `blocking_send_recv_example.py` to see how I could potentially send over a tensor.\nThis test doesn't seem to work, it misuses `zmq`, as both procs are doing `connect`, while the correct usage is for the target proc to use `.bind` and the initiator proc to use `.connect`. The string literal for the addr in `.connect` is also wrong as there are a few extra spaces.\n\nAfter fixing a few of these issues myself, I am getting the following error from the initiator:\n```\nTraceback (most recent call last):\n  File \"/mnt/large_shared/libs/nixl/test/python/blocking_send_recv_example.py\", line 93, in <module>\n    xfer_handle = agent.initialize_xfer(\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/large_shared/users/federico/env_nightly/lib/python3.11/site-packages/nixl/_api.py\", line 299, in initialize_xfer\n    handle = self.agent.createXferReq(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: createXferReq(): incompatible function arguments. The following argument types are supported:\n    1. (self: nixl._bindings.nixlAgent, operation: nixl._bindings.nixl_xfer_op_t, local_descs: nixl._bindings.nixlXferDList, remote_descs: nixl._bindings.nixlXferDList, remote_agent: str, notif_msg: str = '', backend: int = 0) -> int\n\nInvoked with: <nixl._bindings.nixlAgent object at 0x7f17915f8370>, <nixl_xfer_op_t.NIXL_READ: 0>, <nixl._bindings.nixlRegDList object at 0x7f1793d44fb0>, <nixl._bindings.nixlRegDList object at 0x7f179173abf0>, b'b', 'UUID'\n[1742361716.638792] [g001:2525325:0]          rcache.c:643  UCX  WARN  mlx5_0: destroying inuse region 0x5566884a1010 [0x5566875a20c0..0x5566875a20f0] g- rw ref 1 lkey 0x1f2aea rkey 0x1f2aea atomic_rkey 0x21d268\n[g001:2525325:0:2525325]      rcache.c:383  Assertion `region->refcount == 0' failed: region 0x5566884a1010 0x5566875a20c0..0x5566875a20f0 of mlx5_0\n```\n\nWould be great if there was a functional example on how to send over a tensor 🙏 ", "comments_url": "https://api.github.com/repos/ai-dynamo/nixl/issues/61/comments", "author": "cassanof", "comments": [{"user": "mkhazraee", "created_at": "2025-03-19T22:01:30Z", "body": "Hello and thanks for pointing this out. We have fixed it in PR #65 and it's already merged. We further added data checks and some clean ups to the code.\n \nOne point to consider is that two sided and blocking is not our targeted mode of operation, this was an example to give an idea of how it's possible to replicate 2-sided with 1-sided. (Since it was an example to demonstrate the idea, it was supposed to be in our examples directory, and we plan to add CIs for examples directory very soon to avoid these issues). You can still pass tensors to 1-sided operations too, it's the same API."}, {"user": "cassanof", "created_at": "2025-03-20T00:20:25Z", "body": "thank you! \n\ni got it to work last night. been a big unblocker for me. thanks for releasing this library!"}], "satisfaction_conditions": ["A fix for the issues in the blocking_send_recv_example.py example code", "A working example of how to transfer tensors using the library", "Clarification on the intended usage patterns of the library"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 00:59:52"}, "dockerfile": "FROM nvcr.io/nvidia/pytorch:25.02-py3\n\n# Set timezone\nENV TZ=America\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n\n# Install required dependencies\nRUN apt-get update -y && apt-get install -y \\\n    git \\\n    build-essential \\\n    cmake \\\n    pkg-config \\\n    libnuma-dev \\\n    numactl \\\n    wget \\\n    autotools-dev \\\n    automake \\\n    libtool \\\n    libz-dev \\\n    libiberty-dev \\\n    flex \\\n    libibverbs-dev \\\n    libgoogle-glog-dev \\\n    libgtest-dev \\\n    libjsoncpp-dev \\\n    libpython3-dev \\\n    libboost-all-dev \\\n    libssl-dev \\\n    libgrpc-dev \\\n    libgrpc++-dev \\\n    libprotobuf-dev \\\n    protobuf-compiler-grpc \\\n    pybind11-dev \\\n    python3-full \\\n    python3-pip \\\n    python3-numpy \\\n    meson \\\n    ninja-build \\\n    uuid-dev \\\n    pciutils \\\n    libpci-dev \\\n    ibverbs-utils \\\n    libibmad-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nRUN pip3 install --no-cache-dir meson ninja pybind11\n\n# Install UCX 1.18.0 with optimized build flags\nWORKDIR /tmp\nRUN wget https://github.com/openucx/ucx/releases/download/v1.18.0/ucx-1.18.0.tar.gz && \\\n    tar xzf ucx-1.18.0.tar.gz && \\\n    cd ucx-1.18.0 && \\\n    ./contrib/configure-release \\\n    --prefix=/usr/local \\\n    --enable-optimizations \\\n    --enable-cma \\\n    --enable-mt \\\n    --with-cuda=/usr/local/cuda && \\\n    make -j$(nproc) && \\\n    make install && \\\n    ldconfig && \\\n    cd .. && \\\n    rm -rf ucx-1.18.0 ucx-1.18.0.tar.gz\n\n# Clone the repository and checkout the specific commit\nWORKDIR /app\nRUN git clone https://github.com/ai-dynamo/nixl.git && \\\n    cd nixl && \\\n    git checkout 4b073797578685afa65755e0893952eecb41a067\n\n# Set environment variables\nENV LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\nENV PATH=/usr/local/bin:$PATH\n\n# Build the project\nWORKDIR /app/nixl\nRUN meson setup build && \\\n    cd build && \\\n    ninja\n\n# Install the Python package\nWORKDIR /app/nixl\nRUN pip install --no-cache-dir .\n\n# Set working directory to the repository root\nWORKDIR /app/nixl", "language": "c++"}
{"task_id": "cab_lenient_5", "number": 103, "title": "blocking_send_recv_example seems not working", "created_at": "2025-03-31T22:14:43Z", "closed_at": "2025-04-02T05:51:02Z", "commit_id": "c6b871cd912921cd431fe6f87b17cc37c2440c66", "labels": [], "url": "https://github.com/ai-dynamo/nixl/issues/103", "body": "r```\noot@ad-h100-80gb-sxm-ib-8x-research-01:/workspace/nixl/examples/python# python3 blocking_send_recv_example.py --name wei_test --zmq_ip 172.16.121.7 \nLoaded plugin UCX_MO\nLoaded plugin UCX\nInitialized NIXL agent: wei_test\ninitiator Tensors: [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\nTraceback (most recent call last):\n  File \"/workspace/nixl/examples/python/blocking_send_recv_example.py\", line 79, in <module>\n    peer_name = agent.add_remote_agent(remote_meta)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/nixl/_api.py\", line 335, in add_remote_agent\n    agent_name = self.agent.loadRemoteMD(metadata)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnixl._bindings.nixlInvalidParamError: NIXL_ERR_INVALID_PARAM\n```", "comments_url": "https://api.github.com/repos/ai-dynamo/nixl/issues/103/comments", "author": "gongwei-130", "comments": [{"user": "tstamler", "created_at": "2025-04-01T13:36:00Z", "body": "Hi @gongwei-130 , can you share how you are running the target application?"}, {"user": "gongwei-130", "created_at": "2025-04-01T18:04:35Z", "body": "@tstamler \n`python3 blocking_send_recv_example.py --name test --zmq_ip NCCL_SOCKET_IFNAME_IP_ADDRESS --zmq_port 8080 --mode target`"}, {"user": "tstamler", "created_at": "2025-04-01T20:45:29Z", "body": "Just using these commands I'm not able to reproduce because the sockets aren't able to connect. The default port used in the original command is 5555, but in the target command you are specifying port 8080. Can you double check that these are the exact matching command line arguments to reproduce this issue? \n\nI suspect that you may have specified the same name for both ends of the test, which would give this exact error."}, {"user": "donglinz", "created_at": "2025-04-02T03:49:54Z", "body": "@tstamler I have exactly the same issue. I build the nixl container with ```./contrib/build-container.sh``` and launch the target & the initiator.\n\nThe initiator hang and the target failed.\n\n```\npython blocking_send_recv_example.py  --name test --zmq_ip localhost --mode target\nLoaded plugin UCX\nLoaded plugin UCX_MO\nInitialized NIXL agent: test\ntarget Tensors: [tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n```\n\n```\npython blocking_send_recv_example.py  --name test --zmq_ip localhost\nLoaded plugin UCX\nLoaded plugin UCX_MO\nInitialized NIXL agent: test\ninitiator Tensors: [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\nTraceback (most recent call last):\n  File \"/data/donglin/nixl/examples/python/blocking_send_recv_example.py\", line 79, in <module>\n    peer_name = agent.add_remote_agent(remote_meta)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/nixl/_api.py\", line 335, in add_remote_agent\n    agent_name = self.agent.loadRemoteMD(metadata)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnixl._bindings.nixlInvalidParamError: NIXL_ERR_INVALID_PARAM\n```\n\nI am testing with a h100 dgx node.\n\nLet me know if I made any mistakes in running the example or you need more information from me to reproduce."}, {"user": "mkhazraee", "created_at": "2025-04-02T03:56:56Z", "body": "Clarifying what Tim mentioned, --name values should not be the same, they're agent names. So you can do something like this:\npython blocking_send_recv_example.py  --name target_007 --zmq_ip localhost --mode target\npython blocking_send_recv_example.py  --name james_bond --zmq_ip localhost\n\nAlso sometimes using localhost causes issues in some systems, better to do 127.0.0.1.\n\nLet us know if that fixes the problem."}, {"user": "gongwei-130", "created_at": "2025-04-02T04:44:33Z", "body": "yes, I think it is the name issue. The document should clarify that to avoid confusion."}, {"user": "mkhazraee", "created_at": "2025-04-02T05:51:02Z", "body": "Agreed, we're doing some improvements to the test environment, including more documentation, for sure will include this."}], "satisfaction_conditions": ["Clarification that different agent names must be used for initiator and target in the blocking_send_recv_example", "Documentation improvements that prevent confusion about parameter requirements", "Working configuration guidance for the blocking_send_recv_example", "Explanation of error messages related to agent configuration"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:30"}, "dockerfile": null, "language": "c++"}
{"task_id": "cab_lenient_6", "number": 34, "title": "Change QMeshPolygonNode polygon radius in code", "created_at": "2025-02-02T18:27:52Z", "closed_at": "2025-02-19T14:23:40Z", "commit_id": "7afa037049f660257df8e9e9399ad74740596663", "labels": [], "url": "https://github.com/erayzesen/godot-quarkphysics/issues/34", "body": "I am trying to change the polygon radius in code for a softbody. Is seems that the function: set_polygon_radius(value) does not work. I tested it like this:\n```\nextends QSoftBodyNode\n\nvar expanded_size = 100\nvar normal_size= 40\n\nfunc _physics_process(delta: float) -> void:            \n\tif Input.is_action_pressed(\"expand\"):\n\t\t$Mesh.set_polygon_radius(expanded_size)  #$Mesh is a QMeshPolygonNode\n\telse:\n\t\t$Mesh.set_polygon_radius(normal_size)\n\n```\nThis does not work too\n```\nextends QSoftBodyNode\n\nvar expanded_size = 100\nvar normal_size = 40\n\nfunc _physics_process(delta: float) -> void:            \n\tif Input.is_action_pressed(\"expand\"):\n\t\t$Mesh.polygon_radius = expanded_size\n\telse:\n\t\t$Mesh.polygon_radius = normal_size\n\n```\n\nI know the code isnt efficent, its just for demonstration", "comments_url": "https://api.github.com/repos/erayzesen/godot-quarkphysics/issues/34/comments", "author": "WilleIshere", "comments": [{"user": "erayzesen", "created_at": "2025-02-02T22:44:05Z", "body": "Hi @WilleIshere. Objects like QMeshCircleNode, QMeshPolygonNode, and QMeshRectNode are nodes that generate the target mesh when added to the scene based on your settings. We do something similar with QMeshAdvancedNode using an editor plugin.\n\nIf a QMeshNode object is under a QSoftBodyNode, its particles move individually.  Therefore, if you want to control the particles during the simulation, you need to use methods related to the particles directly. In the example you provided, you would need to modify both the local,global positions of the particles and the spring properties that enforce distance constraints between them."}, {"user": "WilleIshere", "created_at": "2025-02-03T09:38:33Z", "body": "Thanks, Can you give an example how this can be done?"}, {"user": "erayzesen", "created_at": "2025-02-03T16:58:38Z", "body": "Of course. \n\n```\nfunc _process(delta: float) -> void:\n\tvar mesh:QMeshNode=$QMeshPolygonNode\n\tif(Input.is_action_pressed(\"ui_up\")) :\n\t\tfor i in range(mesh.get_particle_count()) :\n\t\t\tvar p:QParticleObject=mesh.get_particle_at(i)\n\t\t\tp.set_position(p.get_position()+ p.get_position().normalized() )\n\t\tfor i in range(mesh.get_spring_count()) :\n\t\t\tvar s:QSpringObject=mesh.get_spring_at(i)\n\t\t\tvar current_local_distance=s.get_particle_b().get_position()-s.get_particle_a().get_position()\n\t\t\tvar new_length=current_local_distance.length()\n\t\t\ts.set_length(new_length)\n```\n\nThis code should position the particles 1 unit outward from the center each time the up arrow key is pressed and update the springs accordingly."}, {"user": "WilleIshere", "created_at": "2025-02-03T18:33:53Z", "body": "Thank you so much!"}], "satisfaction_conditions": ["A working code example showing how to dynamically resize a QMeshPolygonNode in a QSoftBodyNode", "An explanation of why the original approach didn't work", "Guidance on the correct approach to manipulate soft body meshes during runtime"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:26"}, "dockerfile": null, "language": "c++"}
{"task_id": "cab_lenient_7", "number": 43, "title": "Add Custom Edge Sorting with Predicate Function added edgeRetrieval.cpp", "created_at": "2025-01-15T14:37:41Z", "closed_at": "2025-01-18T14:58:18Z", "commit_id": "b60fe8c2f7a4dcd67abdbde03827dcd37ff4e999", "labels": ["enhancement", "ADVANCED", "SWOC"], "url": "https://github.com/SharonIV0x86/Appledore/pull/43", "body": "closes: #34 \r\n\r\nTested and works", "comments_url": "https://api.github.com/repos/SharonIV0x86/Appledore/issues/43/comments", "author": "ash01825", "comments": [{"user": "SharonIV0x86", "created_at": "2025-01-16T10:00:26Z", "body": "@ash01825 I am not able to edit these files in the PR, maybe you have disabled the option that allows maintainers to edit the code. \r\n\r\nThere are a few changes need to be done, the first one is to include ``<functional>`` header in ``GraphMatrix.h`` without which ``std::function`` wont work.\r\nSecond one is to add a check in your ``getAllEdges`` function, if the graph is unweighted you cannot really return anything, so throw an exception at that point.\r\n\r\nIf you can enable the option that allows me to edit the code in the PR then it will be good, as i have to do some changes in example and the function also."}, {"user": "ash01825", "created_at": "2025-01-16T10:12:18Z", "body": "shouldn't getAllEdges return all existing Edges for Unweighted Graphs too?"}, {"user": "SharonIV0x86", "created_at": "2025-01-16T10:26:41Z", "body": "> shouldn't getAllEdges return all existing Edges for Unweighted Graphs too?\r\n\r\nInteresting, well yes it can, but in the returned tuple \r\n```cpp\r\nstd::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n```\r\nthe ``EdgeType`` will be ``UnweightedG`` and user cannot actually use ``UnweightedG`` anywhere, maybe there is a way we can return \r\nthis for weighted graphs\r\n```cpp\r\nstd::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n```\r\nand this for unweighted graphs?\r\n```cpp\r\nstd::vector<std::tuple<VertexType, VertexType>\r\n```"}, {"user": "SharonIV0x86", "created_at": "2025-01-16T10:46:18Z", "body": "@ash01825 Possibly we can utilize ``std::variant`` but will require more code. although this is not that important as of now.\r\n\r\n> > shouldn't getAllEdges return all existing Edges for Unweighted Graphs too?\r\n> \r\n> Interesting, well yes it can, but in the returned tuple\r\n> \r\n> ```c++\r\n> std::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n> ```\r\n> \r\n> the `EdgeType` will be `UnweightedG` and user cannot actually use `UnweightedG` anywhere, maybe there is a way we can return this for weighted graphs\r\n> \r\n> ```c++\r\n> std::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n> ```\r\n> \r\n> and this for unweighted graphs?\r\n> \r\n> ```c++\r\n> std::vector<std::tuple<VertexType, VertexType>\r\n> ```\r\n\r\n"}, {"user": "SharonIV0x86", "created_at": "2025-01-17T05:22:48Z", "body": "@ash01825 any update?"}, {"user": "ash01825", "created_at": "2025-01-17T08:12:16Z", "body": "yeah sorry was out yesterday yeah I've made the changes"}, {"user": "SharonIV0x86", "created_at": "2025-01-17T08:23:22Z", "body": "> yeah sorry was out yesterday yeah I've made the changes\r\n\r\nIts fine no issues, the thing i am concerned about is that i want to make some changes to your current example file in this PR, but i dont have the permission to do so as you must have unchecked the ``allow maintainers to edit files`` while making this PR due to which i am not able to edit the files. \r\n\r\nSo either you give me permission to edit the code or i'll have to make those changes after merging the PR, your call."}, {"user": "ash01825", "created_at": "2025-01-17T11:22:39Z", "body": "Yeah I've turned on the allow edit my maintainers👍"}, {"user": "SharonIV0x86", "created_at": "2025-01-17T12:39:04Z", "body": "@ash01825 I have approved the changes and PR will be merged in sometime. Till then you are free to work on some other issue."}, {"user": "SharonIV0x86", "created_at": "2025-01-18T14:58:53Z", "body": "@ash01825 The PR is merged, and points are assigned to you. Thank you for contributing, kindly star ⭐ the repository as it shows appreciation to repository maintainers for their work."}], "satisfaction_conditions": ["Inclusion of necessary header files for the implementation", "Proper handling of edge retrieval for both weighted and unweighted graphs", "Enabling maintainer edit permissions on the PR", "Functional implementation that passes testing", "Addressing all feedback from code review"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:00:21"}, "dockerfile": "FROM ubuntu:22.04\n\n# Set noninteractive installation to avoid prompts\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    cmake \\\n    g++ \\\n    make \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/SharonIV0X86/Appledore.git && \\\n    cd Appledore && \\\n    git checkout b60fe8c2f7a4dcd67abdbde03827dcd37ff4e999\n\n# Set up a build directory\nWORKDIR /app/Appledore/build\n\n# Generate build system with CMake if there's a CMakeLists.txt, otherwise prepare for manual build\nRUN if [ -f ../CMakeLists.txt ]; then \\\n        cmake ..; \\\n    else \\\n        echo \"No CMakeLists.txt found. The project may require manual build.\"; \\\n        mkdir -p include examples; \\\n    fi\n\n# Build the project if it has a CMakeLists.txt\nRUN if [ -f ../CMakeLists.txt ]; then \\\n        make; \\\n    else \\\n        echo \"Project ready for manual compilation.\"; \\\n    fi\n\n# Set the working directory back to the project root\nWORKDIR /app/Appledore\n\n# The container is now ready with the project built or prepared for building\n# Users can compile examples or work with the library headers as needed\nCMD [\"/bin/bash\"]", "language": "c++"}
{"task_id": "cab_lenient_8", "number": 7, "title": "llm_llm suddenly causes error in handling multi-byte utf8 string", "created_at": "2025-01-27T13:34:38Z", "closed_at": "2025-02-08T01:19:19Z", "commit_id": "fe01d735cae761a7b3db1a12d52a8dbd35d5aaa4", "labels": [], "url": "https://github.com/m5stack/StackFlow/issues/7", "body": "Environment: StackFlow v1.4.0 and M5Module-LLM dev branch\n\nThe output string of llm_llm is sent separetedly in json format, but the separation point can be at wrong point inside of multi-byte character. When this wrong separation happens, maybe the json output is corrupted to make some error.\n\nIf llm_llm gets \"ガンダムについて語ってください\" (in ja language) as input for inference, it will stop by the below error.\n[W][inference][ 199]: lLaMa_->Run have error!\n\nThe result for the input is always \"(snip) 作品は、1960年に発売された(snip)\", and separated at \"発\"character\n\"作品は、\", \"196\", \"0年にXX\", \"Y売された\"\n(発 is 3 bytes char 0xe799ba: XX=e799 Y=ba )\n\nIf json output is stopped, no error seems to happen.\nExtended log is the following. Ignore 6066d1, it is my logging mistake.\n\n[I][task_output][ 249]: send:作品は、\n[I][task_output][ 251]: datalen:12\n[I][task_output][ 253]: data:e4,bd,9c,e5,93,81,e3,81\n[I][task_output][ 255]: data:af,6066d1\n[I][task_output][ 273]: send stream\n[I][task_output][ 249]: send:196\n[I][task_output][ 251]: datalen:3\n[I][task_output][ 273]: send stream\n[I][task_output][ 249]: send:0年に��\n[I][task_output][ 251]: datalen:9\n[I][task_output][ 253]: data:30,e5,b9,b4,e3,81,ab,e7\n[I][task_output][ 255]: data:99,6066d1\n// if json is output, the error is here.\n[I][task_output][ 249]: send:�売された\n[I][task_output][ 251]: datalen:13\n[I][task_output][ 253]: data:ba,e5,a3,b2,e3,81,95,e3\n[I][task_output][ 255]: data:82,6066d1\n[I][task_output][ 273]: send stream\n\nThe logging code is like this in llm_llm::task_output()\n```\n        SLOGI(\"send:%s\", data.c_str());   // this is the original logging \n        const char* cstr = data.c_str();\n        SLOGI(\"datalen:%d\",data.length());\n        if(data.length() > 8)\n            SLOGI(\"data:%x,%x,%x,%x,%x,%x,%x,%x\",cstr[0],cstr[1],cstr[2],cstr[3],cstr[4],cstr[5],cstr[6],cstr[7]);\n        if(data.length() > 8)  SLOGI(\"data:%x, _%x_ \",cstr[8]);  // mistake\n```", "comments_url": "https://api.github.com/repos/m5stack/StackFlow/issues/7/comments", "author": "nyasu3w", "comments": [{"user": "Abandon-ht", "created_at": "2025-02-06T08:43:16Z", "body": "Thanks for your feedback. The cached token content is incorrectly truncated when output. I will fix it.\n\n```cpp\nif (cached_token.size() >= 3)\n{\n\tfloat t_cost_ms = t_cost.cost();\n\tfloat token_per_sec = token_ids.size() / (t_cost_ms / 1000);\n\tauto tmp_out = tokenizer->Decode(cached_token);\n\tprintf(\"tmp_out: %s\\n\", tmp_out.c_str());\n\t_attr.runing_callback(cached_token.data(), cached_token.size(), tmp_out.c_str(), token_per_sec, _attr.reserve);\n\tcached_token.clear();\n}\n```\n\nThis problem can be avoided by changing \"if (cached_token.size() >= 3)\" to \"if (cached_token.size() >= 5)\"."}, {"user": "nyasu3w", "created_at": "2025-02-06T14:43:55Z", "body": "Thanks for the information. I can enjoy LLM(s) in Japanese with the code even before it is released."}], "satisfaction_conditions": ["A fix for the UTF-8 character truncation issue in the LLM output", "Support for properly displaying Japanese language content", "A solution that works with their existing setup (StackFlow v1.4.0 and M5Module-LLM)", "A timely solution they could implement before an official release"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:02:33"}, "dockerfile": "FROM ubuntu:20.04\n\n# Avoid interactive prompts during installation\nENV DEBIAN_FRONTEND=noninteractive\n\n# Set up timezone information\nRUN apt-get update && apt-get install -y tzdata && \\\n    ln -fs /usr/share/zoneinfo/UTC /etc/localtime && \\\n    dpkg-reconfigure -f noninteractive tzdata\n\n# Install build dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    cmake \\\n    python3 \\\n    python3-pip \\\n    python3-dev \\\n    scons \\\n    wget \\\n    unzip \\\n    pkg-config \\\n    libssl-dev \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nRUN pip3 install --no-cache-dir numpy protobuf\n\n# Create working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/m5stack/StackFlow.git && \\\n    cd StackFlow && \\\n    git checkout fe01d735cae761a7b3db1a12d52a8dbd35d5aaa4\n\n# Set working directory to the repository\nWORKDIR /app/StackFlow\n\n# Set up environment for building the LLM framework\nRUN cd projects/llm_framework && \\\n    if [ -f ./setup.sh ]; then chmod +x ./setup.sh && ./setup.sh; fi\n\n# Build the project\nRUN cd projects/llm_framework && \\\n    if [ -f ./build.sh ]; then chmod +x ./build.sh && ./build.sh; fi\n\n# Set the default command\nCMD [\"echo\", \"Environment is ready to work with StackFlow and fix the UTF-8 multi-byte string issue in llm_llm. Navigate to /app/StackFlow to work with the project.\"]", "language": "c++"}
{"task_id": "cab_lenient_9", "number": 42, "title": "Fix compatability with IMGUI_DISABLE_OBSOLETE_FUNCTIONS", "created_at": "2025-01-02T12:29:24Z", "closed_at": "2025-01-03T03:25:03Z", "commit_id": "09c9458293adc8a63001f68e541e79f97fbe49dc", "labels": ["type:chore", "prio:medium", "status:done"], "url": "https://github.com/brenocq/implot3d/pull/42", "body": "Hello!\r\nI noticed that ImPlot3D currently does not compile when configured with ```IMGUI_DISABLE_OBSOLETE_FUNCTIONS```. In particular, the ```IM_FLOOR``` and ```IM_OFFSETOF``` macros are no longer available in this case. This pull request changes those calls to ```ImFloor``` and C++11's ```offsetof``` respectively.\r\n", "comments_url": "https://api.github.com/repos/brenocq/implot3d/issues/42/comments", "author": "bratpilz", "comments": [{"user": "bratpilz", "created_at": "2025-01-03T10:42:34Z", "body": "No problem, thanks for merging it so quickly! Where do you see these macros used in ImPlot exactly though? I can't seem to find any usage of IM_FLOOR or IM_OFFSETOF in the master branch. Are you talking about something else?"}, {"user": "brenocq", "created_at": "2025-01-03T19:30:42Z", "body": "> Where do you see these macros used in ImPlot exactly though? I can't seem to find any usage of IM_FLOOR or IM_OFFSETOF in the master branch. Are you talking about something else?\r\n\r\nOoooh I was testing with the latest release (`v0.16`), but I just checked the `master` branch and it is indeed already fixed there. I'll talk with @epezent about creating a new release!\r\n\r\nThank you again @bratpilz!"}], "satisfaction_conditions": ["A solution that allows ImPlot3D to compile when IMGUI_DISABLE_OBSOLETE_FUNCTIONS is defined", "Replacement of deprecated ImGui macros with their modern equivalents", "Clarification on where these macros are used in the codebase", "Information about version differences between release and master branches"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:00"}, "dockerfile": null, "language": "c++"}
{"task_id": "cab_lenient_10", "number": 40, "title": "Enabled META and ALT keys to work in Emacs by making them send Escape", "created_at": "2025-03-09T01:03:09Z", "closed_at": "2025-03-13T00:28:38Z", "commit_id": "9b424b5ad6cfc133de24c630b56f8e12ad36a22f", "labels": [], "url": "https://github.com/markeel/gdterm/pull/40", "body": "I tried your addon and it is fantastic. The only issue I had is I wanted to run Emacs within the terminal but the M-<key> combinations were not working, so I have made a small update to make the META and ALT keys send an Escape to resolve that issue.", "comments_url": "https://api.github.com/repos/markeel/gdterm/issues/40/comments", "author": "MichaelBScott", "comments": [{"user": "markeel", "created_at": "2025-03-10T16:02:55Z", "body": "I took a look at the change and I think this is on the right path, but what other terminals do is set a config option for this kind of behavior or react to an escape code being sent.  The libtmt library only did things that matches what an ANSI term type would do, but I have since extended that because it was inadequate for MS windows.  If emacs sends an escape code to have an escape sent for meta, ten I'd prefer to react to that.  Btw were you using emacs on Linux or Windows?"}, {"user": "MichaelBScott", "created_at": "2025-03-10T18:59:05Z", "body": "Thank you for the info, it sounds like it needs a more complete solution. The change I made was based on how to resolve the same issue when using XTerm by setting:\r\n\r\n XTerm.vt100.metaSendsEscape: true\r\n\r\nAdditionally, since pushing that change I have also determined CTRL doesn't quite fully work either. C-/ ended up writing the / character instead of performing the action for the keybinding.\r\n\r\nOh, and I am using Linux."}, {"user": "markeel", "created_at": "2025-03-11T21:01:53Z", "body": "I'm not sure there is really any downside to how you made the change since I'm not sure when or why you might accidentally send an escape by just pressing the alt or meta keys, but since xterm made it configurable I'm leaning to adding that to the GDTerm settings.\r\n\r\nBTW the Ctrl-/ is broken and I'll add an issue.  The range in the Godot key didn't match for the characters '/' and '~' so that's why that's not working.\r\n\r\nI'm not really an emacs user, but I loaded it onto my system and will do a few tests as well."}, {"user": "markeel", "created_at": "2025-03-11T21:41:47Z", "body": "BTW when I tried to use 'emacs -nw' on my Ubuntu system it did not behave well until I changed the TERM environment variable to \"xterm-256color\", so I tested it in my standard Gnome Terminal and it didn't behave well with that TERM either.\r\n\r\nThe library I used (libtmt) doesn't really attempt to do a full TERM=xterm-256color terminal emulation but it is apparently close enough that when using emacs it seemed to behave much better.  Not sure why.  It may be that emacs did some things that weren't quite compatible with a terminal as primitive as TERM=ansi.\r\n\r\n"}, {"user": "markeel", "created_at": "2025-03-11T21:49:04Z", "body": "I merged a change for Ctrl-/ but you will need to merge your updates and compile from source to check them out."}, {"user": "MichaelBScott", "created_at": "2025-03-13T00:28:14Z", "body": "Hi @markeel,\r\nI have built the version using your latest changes from the main branch and as far as I can tell emacs is now fully working with the Send ALT Meta as ESC option selected.\r\n\r\nC-/ is now performing the correct action for the keybinding.\r\nAlso, setting TERM=xterm-256-color seems to work correctly for me.\r\n\r\nThis pull request is no longer required so I am closing it.\r\n\r\nThank you so much for your help.\r\n"}, {"user": "markeel", "created_at": "2025-03-13T04:41:06Z", "body": "> Hi @markeel, I have built the version using your latest changes from the main branch and as far as I can tell emacs is now fully working with the Send ALT Meta as ESC option selected.\r\n> \r\n> C-/ is now performing the correct action for the keybinding. Also, setting TERM=xterm-256-color seems to work correctly for me.\r\n> \r\n> This pull request is no longer required so I am closing it.\r\n> \r\n> Thank you so much for your help.\r\n\r\nThanks for using the plugin and providing feedback!  And I'm glad the changes are working.  \r\n\r\nI'll release a new version when I fix the issue with background color, but that might be toward the end of the month."}], "satisfaction_conditions": ["Support for META and ALT key combinations in Emacs within the terminal", "Proper handling of CTRL key combinations in the terminal", "Configurable option for META/ALT key behavior", "Compatibility with proper terminal environment settings"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:40"}, "dockerfile": null, "language": "c++"}
{"task_id": "cab_lenient_11", "number": 17, "title": "[Platform] add dispatch key", "created_at": "2025-02-07T02:51:08Z", "closed_at": "2025-02-21T09:10:31Z", "commit_id": "7d9ae22ecb6dc3ea4e720e5109cf46e1ae7da730", "labels": [], "url": "https://github.com/vllm-project/vllm-ascend/pull/17", "body": "### What this PR does / why we need it?\r\nAdd dispatch key for NPU, so that the log could be print correctly.\r\n\r\nNow\r\n```\r\nexecutor_base.py:110] # CPU blocks: 220478, # CPU blocks: 21845\r\n```\r\n\r\nAfter this pr\r\n```\r\nexecutor_base.py:110] # NPU blocks: 220478, # CPU blocks: 21845\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nN/A\r\n\r\n### How was this patch tested?\r\nCI passed and log printed as above\r\n\r\n", "comments_url": "https://api.github.com/repos/vllm-project/vllm-ascend/issues/17/comments", "author": "MengqingCao", "comments": [{"user": "wangxiyuan", "created_at": "2025-02-17T01:43:00Z", "body": "from torch expert suggestion: change the value to `PrivateUse1`. "}, {"user": "MengqingCao", "created_at": "2025-02-17T02:53:10Z", "body": "> from torch expert suggestion: change the value to `PrivateUse1`.\r\n\r\nThanks a lot! This could fix the issue that torch does not recognize key `npu`.\r\nBut the log printed will become `executor_base.py:110] # PrivateUse1 blocks: 220478, # CPU blocks: 21845`, I'll fix this in vLLM."}, {"user": "wangxiyuan", "created_at": "2025-02-17T02:55:08Z", "body": "> > from torch expert suggestion: change the value to `PrivateUse1`.\r\n> \r\n> Thanks a lot! This could fix the issue that torch does not recognize key `npu`. But the log printed will become `executor_base.py:110] # PrivateUse1 blocks: 220478, # CPU blocks: 21845`, I'll fix this in vLLM.\r\n\r\nYes, the log in vllm should use device_name instead."}, {"user": "MengqingCao", "created_at": "2025-02-17T02:57:46Z", "body": "> Yes, the log in vllm should use device_name instead.\r\n\r\nAgree"}], "satisfaction_conditions": ["A solution that allows torch to recognize the NPU device type", "A way to display the correct device name in logs", "Guidance from torch experts on the proper approach for adding custom device types"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:12:00"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_12", "number": 413, "title": "ImportError: cannot import name 'MCPServerSse' from 'agents.mcp'", "created_at": "2025-04-01T12:45:09Z", "closed_at": "2025-04-07T04:40:23Z", "commit_id": "9c53abe8c15ab2cf1c5591c1db1f61b52a1b24dc", "labels": ["bug", "needs-more-info"], "url": "https://github.com/openai/openai-agents-python/issues/413", "body": "Traceback (most recent call last):\nFile \"C:\\Users\\Lenovo\\Desktop\\Strats AI\\open ai sdk\\main.py\", line 4, in\nfrom agents.mcp import MCPServerSse, MCPServerStdio\nImportError: cannot import name 'MCPServerSse' from 'agents.mcp' (C:\\Users\\Lenovo\\Desktop\\Strats AI\\open ai sdk\\venv\\Lib\\site-packages\\agents\\mcp_init_.py)\n\nThis is the error i am facing despite creating the venv and installing the latest version of the open ai sdk", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/413/comments", "author": "oms0401", "comments": [{"user": "rm-openai", "created_at": "2025-04-01T15:45:25Z", "body": "Are you on Python 3.9? Can you post the full error/stack trace?"}, {"user": "smortezah", "created_at": "2025-04-02T08:50:01Z", "body": "Same for Python 3.12.\n\n```\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[2], line 1\n----> 1 from agents.mcp import MCPServerStdio\n      2 samples_dir='.'\n      4 async with MCPServerStdio(\n      5     params={\n      6         \"command\": \"npx\",\n      7         \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n      8     }\n      9 ) as server:\n\nImportError: cannot import name 'MCPServerStdio' from 'agents.mcp' (.venv/lib/python3.12/site-packages/agents/mcp/__init__.py)\n```"}, {"user": "limingyang325", "created_at": "2025-04-02T11:29:14Z", "body": "\n> Are you on Python 3.9? Can you post the full error/stack trace?\nI am using Python 3.9, and I encountered the same issue.\n"}, {"user": "rm-openai", "created_at": "2025-04-02T15:13:06Z", "body": "Can you try `from agents.mcp.server import MCPServerSse` and tell me what error you see?\n\nAlso this wont work on Python 3.9, as MCP support requires 3.10+"}, {"user": "smortezah", "created_at": "2025-04-02T15:34:07Z", "body": "> Can you try `from agents.mcp.server import MCPServerSse` and tell me what error you see?\n\nNot working.\n\n```\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 1\n----> 1 from agents.mcp.server import MCPServerSse\n\nFile ~/.venv/lib/python3.12/site-packages/agents/mcp/server.py:10\n      7 from typing import Any, Literal\n      9 from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n---> 10 from mcp import ClientSession, StdioServerParameters, Tool as MCPTool, stdio_client\n     11 from mcp.client.sse import sse_client\n     12 from mcp.types import CallToolResult, JSONRPCMessage\n\nFile ~/mcp.py:6\n      3 import shutil\n      5 from agents import Agent, Runner\n----> 6 from agents.mcp import MCPServer, MCPServerStdio\n      9 async def run(mcp_server: MCPServer):\n     10     agent = Agent(\n     11         name=\"Assistant\",\n     12         instructions=\"Use the tools to read the filesystem and answer questions based on those files.\",\n     13         mcp_servers=[mcp_server],\n     14     )\n\nImportError: cannot import name 'MCPServer' from 'agents.mcp' (.venv/lib/python3.12/site-packages/agents/mcp/__init__.py)\n```"}, {"user": "rm-openai", "created_at": "2025-04-02T15:36:54Z", "body": "@smortezah thanks for bearing with me - can you try running this and telling me what you see?\n```\nimport importlib.metadata\nimport os\nimport sys\n\nprint(sys.version)\ntry:\n    print(importlib.metadata.version(\"agents\"))\nexcept Exception:\n    print(\"-1\")\n\ntry:\n    import mcp\n\n    print(dir(mcp))\nexcept Exception:\n    print(\"mcp not found\")\n\nagents_dir = importlib.import_module(\"agents\").__path__[0]\nprint(str(agents_dir))\n\n\nmcp_file = os.path.join(str(agents_dir), \"mcp\", \"__init__.py\")\nwith open(mcp_file) as f:\n    print(f.read())\n```"}, {"user": "smortezah", "created_at": "2025-04-02T15:45:18Z", "body": "```\n3.12.9 (main, Feb  5 2025, 18:58:23) [Clang 19.1.6 ]\n-1\nmcp not found\n~/.venv/lib/python3.12/site-packages/agents\ntry:\n    from .server import (\n        MCPServer,\n        MCPServerSse,\n        MCPServerSseParams,\n        MCPServerStdio,\n        MCPServerStdioParams,\n    )\nexcept ImportError:\n    pass\n\nfrom .util import MCPUtil\n\n__all__ = [\n    \"MCPServer\",\n    \"MCPServerSse\",\n    \"MCPServerSseParams\",\n    \"MCPServerStdio\",\n    \"MCPServerStdioParams\",\n    \"MCPUtil\",\n]\n```"}, {"user": "rm-openai", "created_at": "2025-04-02T15:53:20Z", "body": "@smortezah How did you install the `openai-agents` package? Seems like somehow the MCP dep didnt get pulled in.\n\nCan you also try\n```\nimport importlib.metadata\nprint(importlib.metadata.version(\"openai-agents\")\n```\n\nand reinstalling the package via\n```\npip uninstall openai-agents\npip install openai-agents\n```"}, {"user": "smortezah", "created_at": "2025-04-02T16:01:50Z", "body": "@rm-openai I installed it with `uv add \"openai-agents[viz]\"`.\n\n```\nimport importlib.metadata\nprint(importlib.metadata.version(\"openai-agents\")\n```\n0.0.7\n\nAlso, none of the followings worked:\n```\nuv remove \"openai-agents[viz]\"\nuv add \"openai-agents[viz]\"\n```\nand\n```\nuv remove openai-agents\nuv add openai-agents\n```\n\nHOWEVER, it works when I use `pip` instead of `uv`:\n```\nbrew install python@3.12\npython3.12 -m venv venv3.12\nsource venv3.12/bin/activate\npip install openai-agents\n\npython -c \"from agents.mcp.server import MCPServerSse\"\n```"}, {"user": "rm-openai", "created_at": "2025-04-02T16:10:40Z", "body": "@smortezah it sounds like you might not be using `uv run` when you install via uv. This worked fine for me:\n\n```\nmkdir test_mcp && cd test_mcp && uv init .\n\nuv add \"openai-agents[viz]\" && uv run python -c \"from agents.mcp.server import MCPServerSse\"\n```"}, {"user": "smortezah", "created_at": "2025-04-02T16:19:05Z", "body": "@rm-openai I guess I found the source of issue. If I put a python file that only includes `from agents.mcp.server import MCPServerSse` in the root directory of my project, it works. However, if I put this file in a subdirectory, it stops working regardless of where I call this file from; that is, whether I run `python a.py` or `uv run a.py` from the root directory or from within the subdirectory, it throws the error."}, {"user": "smortezah", "created_at": "2025-04-04T15:52:27Z", "body": "@rm-openai @oms0401 Solved.\n\nI encountered an interesting situation where I had a file named `mcp.py` in my subdirectory. Attempting to import `from mcp` resulted in a circular import. Interestingly, I wasn’t importing from `mcp` in my Jupyter notebook or the Python file I was trying to execute. However, the presence of `mcp.py` in the directory led to the following error:\n`ImportError: cannot import name ‘MCPServer’ from ‘agents.mcp’ (.venv/lib/python3.12/site-packages/agents/mcp/__init__.py)`\n\nTo resolve this issue, I simply renamed `mcp.py`."}, {"user": "rm-openai", "created_at": "2025-04-04T19:17:51Z", "body": "Wow that is kinda crazy. Makes sense though."}, {"user": "oms0401", "created_at": "2025-04-07T04:40:23Z", "body": "Yes the issue is solved right now but the sdk is not stable in the current python 3.12 versio"}, {"user": "ycjcl868", "created_at": "2025-04-11T09:18:16Z", "body": "Same for Python 3.12.\n\n"}], "satisfaction_conditions": ["Identification of the root cause of the import error", "A practical solution to resolve the import error", "Clarification on Python version compatibility", "Understanding of package installation methods that work correctly"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:41"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_13", "number": 263, "title": "Fix potential infinite tool call loop by resetting tool_choice after …", "created_at": "2025-03-20T13:29:21Z", "closed_at": "2025-03-25T15:30:53Z", "commit_id": "9384a0fb3fd13151c010d3f45c89bfcb05172784", "labels": [], "url": "https://github.com/openai/openai-agents-python/pull/263", "body": "# Fix potential infinite tool call loop by resetting tool_choice after tool execution\r\n\r\n## Summary\r\n\r\nThis PR fixes an issue where setting `tool_choice` to \"required\" or a specific function name could cause models to get stuck in an infinite tool call loop.\r\n\r\nWhen `tool_choice` is set to force tool usage, this setting persists across model invocations. This PR automatically resets `tool_choice` to \"auto\" after tool execution, allowing the model to decide whether to make additional tool calls in subsequent turns.\r\n\r\nUnlike using `tool_use_behavior=\"stop_on_first_tool\"`, this approach lets the model continue processing tool results while preventing forced repeated tool calls.\r\n\r\n## Test plan\r\n\r\n- Added tests to verify tool_choice reset behavior for both agent and run_config settings\r\n- Added integration test to verify the solution prevents infinite loops\r\n- All tests pass\r\n\r\n## Checks\r\n\r\n- [x] I've added new tests for the fix\r\n- [x] I've updated the relevant documentation (added comment in code)\r\n- [x] I've run `make lint` and `make format`\r\n- [x] I've made sure tests pass\r\n", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/263/comments", "author": "mini-peanut", "comments": [{"user": "rm-openai", "created_at": "2025-03-20T14:43:59Z", "body": "This is a good idea! What do you think about making it a configurable param, default to `reset=True`?"}, {"user": "mini-peanut", "created_at": "2025-03-20T14:51:40Z", "body": "> This is a good idea! What do you think about making it a configurable param, default to `reset=True`?\r\n\r\n@rm-openai Thanks for the feedback! I considered adding a config parameter, but wonder if it might add complexity without clear use cases. Most users would want to prevent infinite loops by default, and those with specific needs could already implement custom behaviors through the existing API.\r\n\r\nUnless you have specific scenarios in mind where maintaining forced tool calls is beneficial, perhaps the simpler approach is better?"}, {"user": "rm-openai", "created_at": "2025-03-20T15:12:45Z", "body": "@mini-peanut, yeah one use case I had in mind was this:\r\n\r\nSetup:\r\n```\r\nagent = Agent(\r\n  instructions=\"Use the find_company tool to find the company info. Then use the search_directory tool to get the CEO's email.\",\r\n  tools=[find_company, search_directory],\r\n  tool_choice=\"required\",\r\n  tool_use_behavior={\"stop_at_tool_names\": \"search_directory\"},\r\n```\r\n\r\nIf we reset `tool_choice`, then we can't trust the Agent to reliably call the second tool.\r\n\r\nThoughts?"}, {"user": "mini-peanut", "created_at": "2025-03-20T16:16:50Z", "body": "> @mini-peanut, yeah one use case I had in mind was this:\r\n> \r\n> Setup:\r\n> \r\n> ```\r\n> agent = Agent(\r\n>   instructions=\"Use the find_company tool to find the company info. Then use the search_directory tool to get the CEO's email.\",\r\n>   tools=[find_company, search_directory],\r\n>   tool_choice=\"required\",\r\n>   tool_use_behavior={\"stop_at_tool_names\": \"search_directory\"},\r\n> ```\r\n> \r\n> If we reset `tool_choice`, then we can't trust the Agent to reliably call the second tool.\r\n> \r\n> Thoughts?\r\n\r\n@rm-openai Thanks for sharing that use case. I'd like to refine my approach to focus on the specific problem we're solving.\r\n\r\n**The Problem:** Setting `tool_choice` to \"required\" or a specific function name can inadvertently cause infinite loops.\r\n\r\n**Core Hypothesis:** When a user forces a single specific function call, they rarely intend for that same function to be repeatedly called in an infinite loop. This differs from intentional sequential calling of different functions.\r\n\r\n**Problem Scenario:** This issue typically manifests in two specific cases:\r\n1. When `tool_choice` is set to a specific function name, causing the same function to be called repeatedly\r\n2. When `tool_choice=\"required\"` with only one available tool, which functionally behaves the same way\r\n\r\n**Concerns with Adding a Configuration Parameter:**\r\nUsers with legitimate sequential tool usage would need to explicitly set `reset_tool_choice_after_use` to `False`.\r\n\r\n**Targeted Solution:** We can address these specific scenarios without disrupting legitimate use cases:\r\n```python\r\n# Only reset in the problematic scenarios where loops are likely unintentional\r\nif (isinstance(tool_choice, str) and tool_choice not in [\"auto\", \"required\", \"none\"]) or \r\n   (tool_choice == \"required\" and len(tools) == 1):\r\n    # Reset to \"auto\"\r\n```\r\n\r\nThis approach precisely targets the infinite loop problem without affecting the multi-tool sequential calling pattern you described, and without requiring additional configuration.\r\n"}, {"user": "rm-openai", "created_at": "2025-03-21T14:29:41Z", "body": "lgtm - but would you mind fixing lint/typechecking please? can't merge without that"}, {"user": "mini-peanut", "created_at": "2025-03-22T06:19:43Z", "body": "@rm-openai Fixed, and the code should pass the checks. Thanks for your patience\r\n"}, {"user": "rm-openai", "created_at": "2025-03-22T16:06:59Z", "body": "Unfortunately looks like typechecking is still not passing"}, {"user": "rm-openai", "created_at": "2025-03-25T15:30:54Z", "body": "I'm merging this because it's mostly great. I think it will need a couple of followups:\r\n1. Instead of copying the agent, we should do internal bookkeping of the resets\r\n2. I still think this should be configurable\r\n3. I'm not sure it makes sense to reset the RunConfig ModelSettings. \r\n\r\nI'll follow up with all of those!"}], "satisfaction_conditions": ["A solution that prevents infinite tool call loops when tool_choice is set to 'required' or a specific function name", "A targeted approach that addresses problematic scenarios without disrupting legitimate sequential tool usage", "A solution that doesn't require additional configuration parameters unless absolutely necessary", "Code that passes all required checks (lint, typechecking, tests)", "Proper test coverage to verify the solution works as intended"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:11"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_14", "number": 298, "title": "[Bug]: GroupChatManager.a_run_chat does not handle NoEligibleSpeaker Exception", "created_at": "2024-12-27T06:39:08Z", "closed_at": "2025-01-03T18:27:17Z", "commit_id": "687af856fe7a22d4ab4e5ec05c941ded83de1fe1", "labels": ["bug"], "url": "https://github.com/ag2ai/ag2/issues/298", "body": "### Describe the bug\r\n\r\nAccording to the code in GroupChatManager.run_chat when GroupChat.select_speaker raise NoEligibleSpeaker , the groupchat will be terminated. This feature enables coders to define termination conditions in customized speaker_selection_method. \r\n\r\n```python\r\ndef run_chat(\r\n        self,\r\n        messages: Optional[list[dict]] = None,\r\n        sender: Optional[Agent] = None,\r\n        config: Optional[GroupChat] = None,\r\n    ) -> tuple[bool, Optional[str]]:\r\n        \"\"\"Run a group chat.\"\"\"\r\n        \r\n         # other codes before ...\r\n       \r\n        for i in range(groupchat.max_round):\r\n            self._last_speaker = speaker\r\n            groupchat.append(message, speaker)\r\n            # broadcast the message to all agents except the speaker\r\n            for agent in groupchat.agents:\r\n                if agent != speaker:\r\n                    self.send(message, agent, request_reply=False, silent=True)\r\n            if self._is_termination_msg(message) or i == groupchat.max_round - 1:\r\n                # The conversation is over or it's the last round\r\n                break\r\n            try:\r\n                # select the next speaker\r\n                speaker = groupchat.select_speaker(speaker, self)\r\n                if not silent:\r\n                    iostream = IOStream.get_default()\r\n                    iostream.print(colored(f\"\\nNext speaker: {speaker.name}\\n\", \"green\"), flush=True)\r\n                # let the speaker speak\r\n                reply = speaker.generate_reply(sender=self)\r\n            except KeyboardInterrupt:\r\n                # let the admin agent speak if interrupted\r\n                if groupchat.admin_name in groupchat.agent_names:\r\n                    # admin agent is one of the participants\r\n                    speaker = groupchat.agent_by_name(groupchat.admin_name)\r\n                    reply = speaker.generate_reply(sender=self)\r\n                else:\r\n                    # admin agent is not found in the participants\r\n                    raise\r\n            except NoEligibleSpeaker:\r\n                # No eligible speaker, terminate the conversation\r\n                break\r\n\r\n        # other codes after ...\r\n        return True, None\r\n\r\n```\r\n\r\nHowever, it seems that GroupChatManager.a_run_chat do not have this feature.  \r\nI am not sure whether it is a feature or bug.\r\n```python\r\n\r\nasync def a_run_chat(\r\n        self,\r\n        messages: Optional[list[dict]] = None,\r\n        sender: Optional[Agent] = None,\r\n        config: Optional[GroupChat] = None,\r\n    ):\r\n        # other codes before ...\r\n        for i in range(groupchat.max_round):\r\n            groupchat.append(message, speaker)\r\n\r\n            if self._is_termination_msg(message):\r\n                # The conversation is over\r\n                break\r\n\r\n            # broadcast the message to all agents except the speaker\r\n            for agent in groupchat.agents:\r\n                if agent != speaker:\r\n                    await self.a_send(message, agent, request_reply=False, silent=True)\r\n            if i == groupchat.max_round - 1:\r\n                # the last round\r\n                break\r\n            try:\r\n                # select the next speaker\r\n                speaker = await groupchat.a_select_speaker(speaker, self)\r\n                # let the speaker speak\r\n                reply = await speaker.a_generate_reply(sender=self)\r\n            except KeyboardInterrupt:\r\n                # let the admin agent speak if interrupted\r\n                if groupchat.admin_name in groupchat.agent_names:\r\n                    # admin agent is one of the participants\r\n                    speaker = groupchat.agent_by_name(groupchat.admin_name)\r\n                    reply = await speaker.a_generate_reply(sender=self)\r\n                else:\r\n                    # admin agent is not found in the participants\r\n                    raise\r\n           # It does not have the following exception handler\r\n           #  except NoEligibleSpeaker:  \r\n           #     break\r\n\r\n            if reply is None:\r\n                break\r\n\r\n       # other codes after ...\r\n      \r\n```\r\n### Steps to reproduce\r\n\r\nDefine a speaker_selection_method returning None under some conditions. ( That should be a proper case when we try to define the termination condition\r\n\r\n### Model Used\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Screenshots and logs\r\n\r\n_No response_\r\n\r\n### Additional Information\r\n\r\n_No response_", "comments_url": "https://api.github.com/repos/ag2ai/ag2/issues/298/comments", "author": "linmou", "comments": [{"user": "marklysze", "created_at": "2024-12-27T17:45:30Z", "body": "Thanks @linmou, I have addressed this in my Telemetry Phase 1 code, if you need it more urgently then I'll create a new PR. Telemetry Phase 1 #296 "}, {"user": "linmou", "created_at": "2024-12-27T20:56:20Z", "body": "> Thanks @linmou, I have addressed this in my Telemetry Phase 1 code, if you need it more urgently then I'll create a new PR. Telemetry Phase 1 #296\r\n\r\nNot so urgent , I can change my code locally. "}, {"user": "marklysze", "created_at": "2024-12-30T03:02:43Z", "body": "I'm also addressing this in #315, as I think that will merge earlier than #296."}, {"user": "marklysze", "created_at": "2025-01-03T18:27:17Z", "body": "#315 has merged, so we're good to go :)"}], "satisfaction_conditions": ["Confirmation that the NoEligibleSpeaker exception handling will be added to the a_run_chat method", "A timeline for when the fix will be available in the codebase", "Acknowledgment that this is a legitimate issue rather than intended behavior"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:09"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_15", "number": 144, "title": "Prevent Timeout", "created_at": "2025-03-20T20:04:09Z", "closed_at": "2025-03-22T18:23:31Z", "commit_id": "c4ad14be09186e3286fe68dc891bb8ad1845d9dd", "labels": [], "url": "https://github.com/ezyang/codemcp/issues/144", "body": "Hello, quick question, how do you manage the Timeout errors that you encounter when dealing with long answers ?", "comments_url": "https://api.github.com/repos/ezyang/codemcp/issues/144/comments", "author": "Pekno", "comments": [{"user": "ezyang", "created_at": "2025-03-20T22:30:59Z", "body": "Each of the builtin tool actions is quick so you never get close to the 60s timeout. This is more of a problem for custom commands which could take a long time to run. I think probably the right way to handle this when we come to it is to run the command asynchronously, block the tool call 55sec or so, and if the async command is not done yet we return and ask the LLM to do another tool call to wait some more."}, {"user": "Pekno", "created_at": "2025-03-20T22:37:06Z", "body": "Maybe my actions are too broad, but I encounter a lot of timeout when juste asking things like \"Implement X feature\", wich it seems to understand and try to implement, but then after a while just timeout and the conversation closes."}, {"user": "borrelan", "created_at": "2025-03-21T05:21:04Z", "body": "I've experienced random cannot connect to Claude and occasionally cannot connect to codemcp.  I either reload the mcp or restart Claude Desktop, which resolves the issue for a while."}, {"user": "ezyang", "created_at": "2025-03-21T06:45:43Z", "body": "Oh so there is an infinite loop bug on main I need to push a fix for lol. If the logs say \"advanced patch apply\" before it hangs it's that"}, {"user": "notschema", "created_at": "2025-03-21T07:43:02Z", "body": "I'm also having a similar issue where when codemcp makes a file change, it hangs at doing the write task, even if it completes it."}, {"user": "ezyang", "created_at": "2025-03-21T23:32:48Z", "body": "I just cut a new release with the infinite loop fix. Please give it a try. I'll close this issue in a week or so if no one reports that it's still happening on the newest version."}, {"user": "Pekno", "created_at": "2025-03-21T23:41:34Z", "body": "Everything seems back in order, didn't encounter any timeout for the time being. Will try with more tests tomorow and will close if no issues. Thanks for the quick fix ! And great work for this MCP !"}, {"user": "notschema", "created_at": "2025-03-22T02:30:57Z", "body": "I'm not sure if should open a different issue or not; just because it's sort of related to timeout issues; but even when creating a file for example a simple txt document. the file is created, then it will hang for another ~30 seconds until saying **\"I've created a test file named test_file.txt in the %name% directory. The file contains a simple test function that adds two numbers together\"**\n\nIs this normal behavior? "}, {"user": "ezyang", "created_at": "2025-03-22T02:52:49Z", "body": "If the MCP finished running then further delay is an Anthropic problem. I have noticed this happens sometimes.\n\nAnother cause for hang is if something bad happens to codemcp server. Then Claude Desktop is just wedged and you need to restart it.\n\nIf you have evidence (eg logs) that it is specifically a codemcp problem I will look more. But a lot of slowness I have noticed in practice is Anthropic"}, {"user": "Pekno", "created_at": "2025-03-22T18:23:32Z", "body": "Everything is working as intended, no more Timeout ! Thanks again for the fix !"}], "satisfaction_conditions": ["A fix for timeout errors when dealing with long answers", "Ability to complete complex tasks without the conversation closing prematurely", "Stable performance without requiring manual restarts or reloads"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:07:48"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_16", "number": 12, "title": "Best approach to change ports?", "created_at": "2024-12-19T20:12:20Z", "closed_at": "2024-12-20T02:24:32Z", "commit_id": "06f654f4bef1ad7f20e044c21dc4049b7cf74365", "labels": ["bug"], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/12", "body": "Wondering best approach to remap ports for both the main app and the cloudflare proxy?  Tried using the ENV variables, that didn't work (still used 8084 and 8000), tried remapping ports directly and that seemed to work but couldn't connect to the Cloudflare proxy w/localhost or got errors using an IP instead (connection refused).  \r\n\r\nGuessing I'm just doing something incorrect, but would see shifting ports as a pretty big need for many.", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/12/comments", "author": "necromancyr", "comments": [{"user": "calibrain", "created_at": "2024-12-19T21:56:59Z", "body": "You are totally correct\r\nI messed up my docker compose,\r\nIts fixed now, see #13"}, {"user": "necromancyr", "created_at": "2024-12-19T22:49:03Z", "body": "To map the ports for cloudflarebypassforscraping service should just add port maps under there, correct?  That's the other part - both ports need to be modifiable.  "}, {"user": "calibrain", "created_at": "2024-12-19T22:53:29Z", "body": "Sadly, cloudflarebypassforscraping port can't be changed, it's hardcoded from their service\r\nBut it shouldn't matter, since we are not exposing it\r\nIts only used internally, and by hostname so you can have another service in your compose using the same port and it will work fine\r\n\r\nWhy are you trying to change the port ?"}, {"user": "necromancyr", "created_at": "2024-12-20T01:40:53Z", "body": "Thanks - you answered my question.  I was overthinking it and trying to remap more than I needed to.  Thanks!  This is addressed and working great!  (Now I just need a UI element in CWA to link to this! :))\r\n"}, {"user": "calibrain", "created_at": "2024-12-20T02:24:32Z", "body": "I am talking with the creator of CWA about that :P \r\nFor now I have a hack where I apply inject the button in the HTML before spinning the CWA docker, I might clean it up a bit and share it next week"}], "satisfaction_conditions": ["Clarification on which ports need to be remapped and which cannot be changed", "Understanding of how the internal port mapping works with the cloudflare proxy service", "A working configuration that allows the user to run the service without port conflicts"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:06"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_17", "number": 280, "title": "Models are not able to reference file names when producing their outputs", "created_at": "2025-02-07T17:13:34Z", "closed_at": "2025-02-20T20:31:21Z", "commit_id": "fcf88881698eabfa7d808df0f1353aa6bcc54cb8", "labels": ["type: feature request", "type: question", "priority: p3"], "url": "https://github.com/googleapis/python-genai/issues/280", "body": "I'd like the model to be able to reference the file source when answering questions that were preceded with file inputs.\n\nConsider this example:\n\n```python\nimport io, google.genai\n\nGOOGLE_API_KEY = \"--API_KEY--\"\nclient = google.genai.Client(api_key=GOOGLE_API_KEY)\n\ndef upload_file(file_contents, display_name, mime_type=\"text/plain\"):\n    file_contents = io.BytesIO(file_contents.encode(\"utf-8\"))\n    return client.files.upload(path=file_contents, config={\"mime_type\": mime_type, \"display_name\": display_name})\n\nfc_1 = \"\"\"Simplicity is the ultimate sophistication.\n— Leonardo da Vinci\n\"\"\"\nfc_2 = \"\"\"It always seems impossible until it’s done.\n- Nelson Mandela\n\"\"\"\n\nfiles = [upload_file(fc_1, \"file1.md\"), upload_file(fc_2, \"file2.md\")]\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[files[0], files[1],\n        \"For every file, output the name of the file and the quote inside.\",\n    ])\nprint(response.candidates[0].content.parts[0].text)\n```\n\nHere is one of the responses I got:\n\n```\nOkay, I understand. Here's how I will respond, given a file name:\n\n**Input:**\n\n*   `[Filename]`\n\n**Output:**\n\n*   `[Filename]: [Author]`\n\n**Examples:**\n\n*   `quote1.txt`\n*   `quote1.txt: Leonardo da Vinci`\n\n*   `quote2.txt`\n*   `quote2.txt: Nelson Mandela`\n```\n\nNotice that the model is not aware of the file names and can't reference them in its answer.\n\nIf I invoke the model from the Google AI studio, I get the result I'd like:\n```\nfile1.md: Simplicity is the ultimate sophistication.\nfile2.md: It always seems impossible until it’s done.\n```\n\nIs this something we can expect to be ironed out in this library, or should I consider switching to google-generativeai lib?\n\nThe ability to reference files is absolutely crucial for our use case.", "comments_url": "https://api.github.com/repos/googleapis/python-genai/issues/280/comments", "author": "gapeslape", "comments": [{"user": "nurgel", "created_at": "2025-02-08T03:42:33Z", "body": "you could prepend all the metadata you like in text before the file, that is what they probably do on AI Studio."}, {"user": "gapeslape", "created_at": "2025-02-08T17:30:18Z", "body": "@nurgel that works great. Thanks!"}, {"user": "pamorgan", "created_at": "2025-02-20T20:31:21Z", "body": "Thank you - Please let us know if there are more follow ups needed."}], "satisfaction_conditions": ["A method to make file names accessible to the model when processing file content", "A solution that works with their existing code structure using the google.genai library", "A straightforward implementation that doesn't require complex changes"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:42"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_18", "number": 1046, "title": "[BUG] Can't get the MCP tools to work: RuntimeError: Event loop is closed", "created_at": "2025-03-21T16:08:17Z", "closed_at": "2025-03-23T09:56:34Z", "commit_id": "5b2882d4117a6d8a5d50a08b7d56aff1c3a25211", "labels": ["bug"], "url": "https://github.com/huggingface/smolagents/issues/1046", "body": "**Describe the bug**\nI am trying to replace the normal tools by tools coming from a MCP server. My code is runnning inside a poetry venv.\n\n```\nserver_parameters = StdioServerParameters(\n    command=\"uvx\",\n    args=[\"mcp-server-time\"],\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\nwith ToolCollection.from_mcp(server_parameters) as tool_collection:\n    agent = CodeAgent(\n        tools=[*tool_collection.tools],\n        model=model,\n        prompt_templates=code_prompt_templates,\n        additional_authorized_imports=[\"time\", \"numpy\", \"pandas\", \"json\"],\n    )\nresponse = agent.run(\n    task=\"Answer the user request with the tools you have. User input is: What is the time in Berlin?\"\n)\n```\ngives me\n```\n\n  berlin_time = get_current_time(timezone=\"Europe/Berlin\")                                                                                                                                                                                                      \n  print(berlin_time)                                                                                                                                                                                                                                            \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nCode execution failed at line 'berlin_time = get_current_time(timezone=\"Europe/Berlin\")' due to: RuntimeError: Event loop is closed\n[Step 1: Duration 2.95 seconds| Input tokens: 2,330 | Output tokens: 58]\n```\n\nIn another mcp server, I can see that a log message coming from the server \n\n`Processing request of type ListToolsRequest`\n\nSo the server is spawned, but once it tries to access the tool, I get the same error as above\n\n**Code to reproduce the error**\nSee above. Running `npx @modelcontextprotocol/inspector uvx mcp-server-time` I can access the mpc server just fine.\n\n**Error logs (if any)**\nSee above\n\n**Expected behavior**\nThe agent calls the tool\n\n**Packages version:**\nsmolagents==1.12.0\n\n**Additional context**\nAdd any other context about the problem here.\n", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/1046/comments", "author": "wirtsi", "comments": [{"user": "albertvillanova", "created_at": "2025-03-21T17:34:50Z", "body": "Thanks for reporting.\n\nCould you please provide your versions of:\n- mcp\n- mcpadapt\n\nCC: @grll "}, {"user": "wirtsi", "created_at": "2025-03-22T09:10:39Z", "body": "Hey @albertvillanova, thanks for looking into this 😍\n\n```\nmcp==1.4.1\nmcpadapt==0.0.15\n```"}, {"user": "grll", "created_at": "2025-03-22T18:35:50Z", "body": "Hi @wirtsi, thanks for reporting the issue. I will try to reproduce, any chance you are running this in a Jupyter Notebook? Or as a regular python script?"}, {"user": "wirtsi", "created_at": "2025-03-23T08:52:30Z", "body": "No, my code runs in a poetry pyenv (so `poetry run python main.py`)"}, {"user": "grll", "created_at": "2025-03-23T08:57:04Z", "body": "Hmm actually after second thought you need to run your agent.run statement within the context manager otherwise the mcp server is not running. The mcp server / client only runs within the context manager "}, {"user": "grll", "created_at": "2025-03-23T08:58:04Z", "body": "TLDR; just indent your response = ... statement "}, {"user": "wirtsi", "created_at": "2025-03-23T09:56:34Z", "body": "Ah blimey 🤦‍♂️ You are right, it totally makes sense. I thought the context is only needed when instantiating the tools. Thank you 🙏"}, {"user": "phpmac", "created_at": "2025-04-05T07:10:55Z", "body": "How to add multiple mcp services???\n\n"}], "satisfaction_conditions": ["A solution that correctly explains how to use MCP tools with the CodeAgent", "Clarification about the proper scope/lifetime of the MCP server connection", "A simple, actionable fix to the RuntimeError about the closed event loop"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:11:03"}, "dockerfile": "FROM python:3.12-slim\n\n# Set environment variable to avoid prompts during installation\nENV DEBIAN_FRONTEND=noninteractive\nENV UV_SYSTEM_PYTHON=1\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    make \\\n    build-essential \\\n    nodejs \\\n    npm \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install UV for package management\nRUN pip install --upgrade uv\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/huggingface/smolagents.git . \\\n    && git checkout 5b2882d4117a6d8a5d50a08b7d56aff1c3a25211\n\n# Install the Model Context Protocol CLI tools\nRUN npm install -g @modelcontextprotocol/inspector\n\n# Install the package with all dependencies\n# Include test and all extras to ensure we have everything needed\nRUN uv pip install -e \".[dev,test,all]\"\n\n# Install additional dependencies needed for MCP tools\nRUN uv pip install uvx\n\n# Set the working directory to be ready for use\nWORKDIR /app\n\n# Default command (can be overridden)\nCMD [\"bash\"]", "language": "python"}
{"task_id": "cab_lenient_19", "number": 130, "title": "RayTaskError with hf_transfer or ray.init()", "created_at": "2025-01-30T17:28:28Z", "closed_at": "2025-02-03T09:07:40Z", "commit_id": "356f6a5c4f782c956b2b81d45d9794442b2910b2", "labels": [], "url": "https://github.com/huggingface/open-r1/issues/130", "body": "I have met the error as follows, the error output is so long that I have to copy the last lines:\nRayTaskError(RuntimeError): [36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\nException: Failed too many failures in parallel (3): Request: error decoding response body (no permits available)\n\nThe above exception was the direct cause of the following exception:\n\n[36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/lighteval/models/vllm/vllm_model.py\", line 336, in \nrun_inference_one_model\n    llm = LLM(**model_args)\n          ^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/utils.py\", line 986, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 230, in __init__\n    self.llm_engine = self.engine_class.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 517, in \nfrom_engine_args\n    engine = cls(\n             ^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in\n__init__\n    super().__init__(*args, **kwargs)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 64, in \n_init_executor\n    self._init_workers_ray(placement_group)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 278, in \n_init_workers_ray\n    self._run_workers(\"load_model\",\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 407, in \n_run_workers\n    self.driver_worker.execute_method(method, *driver_args,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 468, in \nexecute_method\n    raise e\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 459, in \nexecute_method\n    return executor(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in load_model\n    self.model_runner.load_model()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1096, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12,\nin get_model\n    return loader.load_model(vllm_config=vllm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 366, \nin load_model\n    loaded_weights = model.load_weights(\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 506, in \nload_weights\n    return loader.load_weights(weights)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 237, in \nload_weights\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 189, in \n_load_module\n    for child_prefix, child_weights in self._groupby_prefix(weights):\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 103, in \n_groupby_prefix\n    for prefix, group in itertools.groupby(weights_by_parts,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 100, in \n<genexpr>\n    weights_by_parts = ((weight_name.split(\".\", 1), weight_data)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 342, \nin _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 298, \nin _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n                                                   ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 251, \nin _prepare_weights\n    hf_folder = download_weights_from_hf(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line\n255, in download_weights_from_hf\n    hf_folder = snapshot_download(\n                ^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 294, in \nsnapshot_download\n    _inner_hf_hub_download(file)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 270, in \n_inner_hf_hub_download\n    return hf_hub_download(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 860, in \nhf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1009, in \n_hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1543, in \n_download_to_tmp_and_move\n    http_get(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 437, in \nhttp_get\n    raise RuntimeError(\nRuntimeError: An error occurred while downloading using `hf_transfer`. Consider disabling HF_HUB_ENABLE_HF_TRANSFER for better \nerror handling.\n(run_inference_one_model pid=602235) Calling ray.init() again after it has already been called. [repeated 7x across cluster]RayTaskError(RuntimeError): [36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\nException: Failed too many failures in parallel (3): Request: error decoding response body (no permits available)\n\nThe above exception was the direct cause of the following exception:\n\n[36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/lighteval/models/vllm/vllm_model.py\", line 336, in \nrun_inference_one_model\n    llm = LLM(**model_args)\n          ^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/utils.py\", line 986, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 230, in __init__\n    self.llm_engine = self.engine_class.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 517, in \nfrom_engine_args\n    engine = cls(\n             ^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in\n__init__\n    super().__init__(*args, **kwargs)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 64, in \n_init_executor\n    self._init_workers_ray(placement_group)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 278, in \n_init_workers_ray\n    self._run_workers(\"load_model\",\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 407, in \n_run_workers\n    self.driver_worker.execute_method(method, *driver_args,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 468, in \nexecute_method\n    raise e\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 459, in \nexecute_method\n    return executor(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in load_model\n    self.model_runner.load_model()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1096, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12,\nin get_model\n    return loader.load_model(vllm_config=vllm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 366, \nin load_model\n    loaded_weights = model.load_weights(\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 506, in \nload_weights\n    return loader.load_weights(weights)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 237, in \nload_weights\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 189, in \n_load_module\n    for child_prefix, child_weights in self._groupby_prefix(weights):\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 103, in \n_groupby_prefix\n    for prefix, group in itertools.groupby(weights_by_parts,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 100, in \n<genexpr>\n    weights_by_parts = ((weight_name.split(\".\", 1), weight_data)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 342, \nin _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 298, \nin _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n                                                   ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 251, \nin _prepare_weights\n    hf_folder = download_weights_from_hf(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line\n255, in download_weights_from_hf\n    hf_folder = snapshot_download(\n                ^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 294, in \nsnapshot_download\n    _inner_hf_hub_download(file)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 270, in \n_inner_hf_hub_download\n    return hf_hub_download(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 860, in \nhf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1009, in \n_hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1543, in \n_download_to_tmp_and_move\n    http_get(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 437, in \nhttp_get\n    raise RuntimeError(\nRuntimeError: An error occurred while downloading using `hf_transfer`. Consider disabling HF_HUB_ENABLE_HF_TRANSFER for better \nerror handling.\n(run_inference_one_model pid=602235) Calling ray.init() again after it has already been called. [repeated 7x across cluster]\nI use 4 cards Geforce RTX 4090, could anyone help me? Thanks！", "comments_url": "https://api.github.com/repos/huggingface/open-r1/issues/130/comments", "author": "pyh314", "comments": [{"user": "sam-schorb", "created_at": "2025-01-30T22:06:06Z", "body": "The error comes from Hugging Face's experimental \"hf_transfer\" downloader. Try this:\n1. **Quickest fix**: Disable hf_transfer by running:\n```bash\nexport HF_HUB_ENABLE_HF_TRANSFER=\"false\"\npython your_script.py\n```\n\n2. **Offline approach**: Download model weights locally and point VLLM to the local path:\n```python\nmodel = LLM(model=\"<local folder path>\", ...)\n```\n\n3. **Update dependencies**: Ensure you have recent versions:\n```bash\npip install --upgrade huggingface_hub transformers vllm\n```\n"}, {"user": "pyh314", "created_at": "2025-02-03T09:07:40Z", "body": "The first fix works. "}], "satisfaction_conditions": ["A solution to disable or work around the hf_transfer error", "A straightforward implementation that requires minimal changes to their environment", "A solution that addresses the specific error related to Hugging Face downloads in their Ray/vLLM setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:05:27"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_20", "number": 76, "title": "DOCKER_MODS= on arm64 image?", "created_at": "2025-02-04T20:49:18Z", "closed_at": "2025-02-14T23:12:34Z", "commit_id": "af3d2cc358712177c98c067887ec919837222a70", "labels": [], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/76", "body": "is the DOCKER_MODS=calibre still needed? i saw a commit that removed it but seems to be for amd64 only?\n\nI'm running on arm64 rock-5b board ", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/76/comments", "author": "Fuckingnameless", "comments": [{"user": "calibrain", "created_at": "2025-02-04T21:00:11Z", "body": "Oh, you are right, no need for the calibre MODS anymore, I am offloading that to CWA instead :P "}, {"user": "calibrain", "created_at": "2025-02-05T00:04:23Z", "body": "But its if you are using the arm version\nWhich is still not rolled out to `:latest` docker\nIts for the`CF_BYPASS` branch for now"}, {"user": "Fuckingnameless", "created_at": "2025-02-11T15:26:23Z", "body": "eh sorry i confused repos, meant to ask on CWA's, so you're saying i need a DOCKER_MOD on your image too? or only on crocodilestick's?\n\ni just tested his latest image with your CF_Bypass branch and everything seems to be working even PDF ingest/conversion"}, {"user": "calibrain", "created_at": "2025-02-14T23:12:34Z", "body": "No you dont need it in any of the repos, it was too cumbersome and I dropped it and he implemented it directly in the containers.\n\nSo no, you dont need the DOCKER_MODS anymore"}, {"user": "Fuckingnameless", "created_at": "2025-02-15T21:43:14Z", "body": "> But its if you are using the arm version Which is still not rolled out to `:latest` docker Its for the`CF_BYPASS` branch for now\n\njust confirming\n it is NOT needed for any branch right?"}, {"user": "calibrain", "created_at": "2025-02-16T00:31:42Z", "body": "Exact, DOCKER_MODS is not needed for CWA-BD (or CWA anymore)"}], "satisfaction_conditions": ["Clear confirmation about whether DOCKER_MODS=calibre is required for arm64 architecture", "Information about which branches or versions require DOCKER_MODS", "Clarification about repository-specific requirements", "Unambiguous, definitive answer that resolves confusion"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:45"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_21", "number": 159, "title": "Teacache node not working or reducing video quality.", "created_at": "2025-03-05T23:39:45Z", "closed_at": "2025-03-09T00:23:32Z", "commit_id": "f4e706156f00c0a6a99cb0929a5d19c757a8c0cb", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/159", "body": "14b i2v 720p\nrel_l1_thresh     0.004\nEnable the use_comfficients option will not speed up video generation.\nDisable the use_comfficients option will accelerate the video, but the video quality will significantly decrease.", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/159/comments", "author": "L020304", "comments": [{"user": "kijai", "created_at": "2025-03-05T23:50:58Z", "body": "Use much higher threshold when using the coefficients, something like 0.2"}, {"user": "L020304", "created_at": "2025-03-09T00:23:32Z", "body": "teacache work now."}], "satisfaction_conditions": ["Guidance on appropriate threshold settings when using coefficients", "A solution that enables teacache to function properly", "A solution that maintains video quality while using coefficients"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:45"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_22", "number": 36, "title": "v3 overwriting .wav files creating incomplete short book", "created_at": "2025-01-29T23:44:23Z", "closed_at": "2025-02-01T23:22:03Z", "commit_id": "12fbf89fccfe5cf0b0a2eadfb462f0238a9acfe1", "labels": ["v3"], "url": "https://github.com/santinic/audiblez/issues/36", "body": "After updating to v3 `pip install --upgrade audiblez` and creating a new book `audiblez Durarara\\ Vol\\ 4.epub  -v af_bella  -s 1.0` short chapters are created and if looking directly at the folder in File Explorer, you notice the file always changing in size and often becoming much shorter. Tested on new virtual env as well.", "comments_url": "https://api.github.com/repos/santinic/audiblez/issues/36/comments", "author": "erictbar", "comments": [{"user": "santinic", "created_at": "2025-01-30T08:30:42Z", "body": "Yes, thanks, I rolled back to 0.2.2. v3 will need more work"}, {"user": "sameh0", "created_at": "2025-01-31T11:34:22Z", "body": "@erictbar could you please checkout if the fix works for you ?"}, {"user": "erictbar", "created_at": "2025-01-31T14:13:51Z", "body": "Yes, branch `v3` is working for me."}, {"user": "santinic", "created_at": "2025-01-31T16:33:03Z", "body": "@erictbar fix chunks up the text file basically at random, so the pronunciation is unnatural. I'm moving v3 to use spacy for sentence splitting"}, {"user": "santinic", "created_at": "2025-02-01T12:05:28Z", "body": "Please, update and try again. v3.1 comes with a lot of changes"}], "satisfaction_conditions": ["A fix for the issue of incomplete/shortened .wav files in v3", "Proper handling of text-to-speech processing that doesn't cut audio files short", "A stable version that processes complete audiobooks correctly", "Compatibility with the command line interface pattern they were using"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:18:05"}, "dockerfile": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies including ffmpeg\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n    git \\\n    ffmpeg \\\n    wget \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/santinic/audiblez.git . && \\\n    git checkout 12fbf89fccfe5cf0b0a2eadfb462f0238a9acfe1\n\n# Install poetry\nRUN pip install --no-cache-dir poetry\n\n# Configure poetry to not create a virtual environment\nRUN poetry config virtualenvs.create false\n\n# Install dependencies and build the project\nRUN poetry install\n\n# Download required model files\nRUN wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files/kokoro-v0_19.onnx && \\\n    wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files/voices.json\n\n# Set the entrypoint to the audiblez command\nENTRYPOINT [\"audiblez\"]\n\n# Default command shows help\nCMD [\"--help\"]", "language": "python"}
{"task_id": "cab_lenient_23", "number": 169, "title": "[Bug]: Upgraded to AG2 0.5 and imports broke", "created_at": "2024-12-08T06:56:43Z", "closed_at": "2024-12-08T23:49:39Z", "commit_id": "9338c7adfff7faeb371f20eb6307984c16d4dd15", "labels": ["bug"], "url": "https://github.com/ag2ai/ag2/issues/169", "body": "### Describe the bug\n\nUpgraded to v0.5 and I no longer can import \r\n\r\nfrom autogen import (\r\n    SwarmResult,\r\n    AssistantAgent,\r\n    SwarmAgent,\r\n)\r\n\r\nI cannot even import \r\n\r\nfrom autogen.coding import DockerCommandLineCodeExecutor\r\n\r\nNot sure what happened. \n\n### Steps to reproduce\n\nUpgrade from 0.41 to 0.5 via pip install ag2 --upgrade\n\n### Model Used\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_", "comments_url": "https://api.github.com/repos/ag2ai/ag2/issues/169/comments", "author": "bassilkhilo", "comments": [{"user": "Hk669", "created_at": "2024-12-08T07:08:45Z", "body": "cc @marklysze "}, {"user": "marklysze", "created_at": "2024-12-08T19:00:17Z", "body": "@bassilkhilo, are you able to output the trace when you try and run the program? Just checking if, perhaps, there are any changes made to other files?"}, {"user": "ashim-mahara", "created_at": "2024-12-08T19:26:00Z", "body": "using `pyautogen`works."}, {"user": "ohdearquant", "created_at": "2024-12-08T21:24:49Z", "body": "@bassilkhilo  what environment/package manager do you use?"}, {"user": "bassilkhilo", "created_at": "2024-12-08T23:49:02Z", "body": "Hey all.\r\n\r\nA quick update, @marklysze suggested I run the following commands to fix the issue:\r\n\r\npip uninstall openai pyautogen ag2\r\n\r\npip install ag2\r\n\r\nThis worked, I no longer have import issues.\r\n\r\nI was on AG2 0.41, maybe pyautoagen as well, honestly not too sure. But the above solution fixed the problem.\r\n\r\nCC: @ohdearquant "}], "satisfaction_conditions": ["A solution that resolves import errors after upgrading to AG2 0.5", "Clear instructions for package management to fix dependency conflicts", "A clean installation approach that removes conflicting packages before reinstalling"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:14"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_24", "number": 167, "title": "Support for legacy confluence (6.0.x)", "created_at": "2025-03-28T11:09:07Z", "closed_at": "2025-04-02T15:17:09Z", "commit_id": "08e5fa25883ff5c70ca3c3b2d738d0b208378ee7", "labels": [], "url": "https://github.com/sooperset/mcp-atlassian/issues/167", "body": "I'm wondering what would be missing to have support for the confluence series 6.0.\n\nI've tried the confluence client lib ( `atlassian-python-api` ) against such a legacy server and it seems to work just fine, at least for the basic features.\n\nLooking at the mcp-atlassian code, I couldn't find the reason why it doesn't seem to work with confluence 6.0.x . \n\nAre there any useful pointers about how to start diagnosing this issue?\n\nI'd be keen on building support for the 6.0.x confluence branch, even if it has to be limited.\n\n ", "comments_url": "https://api.github.com/repos/sooperset/mcp-atlassian/issues/167/comments", "author": "jeteve", "comments": [{"user": "sooperset", "created_at": "2025-03-29T05:04:22Z", "body": "That sounds great! The supported Confluence version was set in PR #92. For Jira DC/server, the supported version was set due to the PAT support. It would be great if we could support the legacy version seamlessly, if possible."}, {"user": "jeteve", "created_at": "2025-03-31T08:08:29Z", "body": "So, I ran some test using the test suite and they pass just fine for my legacy confluence:\n\n```\n\npytest -vx tests/test_real_api_validation.py --use-real-data\ntests/test_real_api_validation.py::TestRealConfluenceValidation::test_get_page_content PASSED                             [ 11%]\ntests/test_real_api_validation.py::TestRealConfluenceValidation::test_get_page_comments PASSED                            [ 14%]\ntests/test_real_api_validation.py::TestRealConfluenceValidation::test_search_content PASSED                               [ 17%]\ntests/test_real_api_validation.py::test_confluence_get_page_content[asyncio] PASSED                                       [ 29%]\n\n```\n\nBUT, when I run the MCP in claude with exactly the same environment variables, it just doesn't work and I can see anything significantly interesting in the MCP logs. Maybe it;s some sort of windows thing. MCP with Claude works perfectly with my cloud JIRA."}, {"user": "jeteve", "created_at": "2025-03-31T08:57:07Z", "body": "PR #173 makes this work in the MCP server itself."}, {"user": "sooperset", "created_at": "2025-04-01T17:52:41Z", "body": "Thank you for your contribution. I've just reviewed the PR and we're ready to proceed with a few updates. Once those are implemented, we can move forward with the merge."}, {"user": "jeteve", "created_at": "2025-04-02T15:17:09Z", "body": "Great! Yes, using basic auth, it works fine against an old confluence. (6.0.x line). Thanks a lot @sooperset !"}], "satisfaction_conditions": ["Support for Confluence 6.0.x series in the library", "Identification of what was preventing compatibility with Confluence 6.0.x", "A solution that works with the MCP server", "Authentication method that works with legacy Confluence"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:29"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_25", "number": 43, "title": "Global Rules instead of .windsurfrules", "created_at": "2025-02-04T14:38:01Z", "closed_at": "2025-02-09T18:23:06Z", "commit_id": "69abe65f61f29f1d8f01c3257e311a5f009865d2", "labels": [], "url": "https://github.com/grapeot/devin.cursorrules/issues/43", "body": "Can we use Global Rules within windsurf (instead of .windsurfrules) for referencing the scratchpad.md file? I have a project specific .windsurfrules instructions, and given the character limitation for this file, I am unable to update this.", "comments_url": "https://api.github.com/repos/grapeot/devin.cursorrules/issues/43/comments", "author": "TinkererInChief", "comments": [{"user": "grapeot", "created_at": "2025-02-04T17:08:15Z", "body": "I think it should work for Windsurf (to put parts of the file into the .windsurfrules). Because it's using the scratchpad to do the planning anyway."}, {"user": "grapeot", "created_at": "2025-02-09T18:23:06Z", "body": "Closing due to lack of activity. But feel free to reopen it."}, {"user": "TinkererInChief", "created_at": "2025-02-10T04:35:11Z", "body": "Thanks for your cment but that didn't answer the question raised. I already have a boilerplate repo which has it's own set of instructions. Given that .windsurfrules has character limitations, I was exploring if we can shift your rules to \"Global Rules\" section in windsurf without creating any negative impact. Hope it's clearer now.\n\n"}, {"user": "grapeot", "created_at": "2025-02-10T04:43:35Z", "body": "Yeah the info is helpful! I think the answer is, it depends. I think moving the rules to the \"Global Rules\" section is a good idea that would work for your project. I don't see any issues for your specific project for now. The issue is that these instructions will impact all your projects since anything in the global rules section affects the entire Windsurf. If your intention is to have these Windsurf rules apply to every project, that's perfectly fine. However, if you only want these additional Windsurf rules from my repo to affect certain projects, it could cause side effects.\n\nOne alternative is to rename the Windsurf rules in my repository to another name and manually include it (using mention) when launching new cascade requests. You likely won't need to do this often because, once you include the file in the initial cascade conversation, Windsurf keeps it in the context. This could be a useful workaround."}, {"user": "TinkererInChief", "created_at": "2025-02-10T05:23:15Z", "body": "Thanks, this is helpful!"}], "satisfaction_conditions": ["Clear explanation of whether Global Rules can be used instead of .windsurfrules for referencing scratchpad.md", "Information about potential impacts or side effects of using Global Rules", "Alternative solutions to overcome the character limitations in .windsurfrules", "Addressing the specific context of having a project with its own set of instructions"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:07:11"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_26", "number": 21, "title": "How to run prediction on a video file?", "created_at": "2025-03-22T07:47:07Z", "closed_at": "2025-03-23T09:37:39Z", "commit_id": "5b2af103a1a9ee7b957507b9b1d7dd783a23049e", "labels": [], "url": "https://github.com/roboflow/rf-detr/issues/21", "body": "Hi,\n\nPlease share how I can run my fine-tuned model on a video file and save its output video?", "comments_url": "https://api.github.com/repos/roboflow/rf-detr/issues/21/comments", "author": "dsbyprateekg", "comments": [{"user": "farukalamai", "created_at": "2025-03-23T08:05:50Z", "body": "Hey @dsbyprateekg you can use this code\n\n```bash\nimport supervision as sv\nfrom rfdetr import RFDETRBase\nfrom tqdm import tqdm\nimport json\n\n# Define input and output video paths\nSOURCE_VIDEO_PATH = \"3727445-hd_1920_1080_30fps.mp4\"  # Change this to your input video path\nTARGET_VIDEO_PATH = \"output_video.mp4\"  # Change this to your desired output path\n\n# Load class mapping from JSON file\nwith open(\"classes.json\", \"r\") as f:\n    class_mapping = json.load(f)\n\n# Initialize the RFDETRBase model\nmodel = RFDETRBase()\n\n# Create a generator for video frames\nframe_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n\n# Get video information (resolution, fps, etc.)\nvideo_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n\n# Process the video frame by frame\nwith sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n    for frame in tqdm(frame_generator, desc=\"Processing video\"):\n        # Get detections using RFDETRBase model\n        detections = model.predict(frame, threshold=0.3)\n        \n        # Map numeric class IDs to class names for labels\n        labels = []\n        for class_id in detections.class_id:\n            # Convert int to string for dictionary lookup\n            class_id_str = str(class_id)\n            # Get class name if exists in mapping\n            class_name = class_mapping.get(class_id_str)\n            labels.append(class_name)\n        \n        # Create annotated frame\n        annotated_frame = frame.copy()\n        \n        # Apply box annotations\n        annotated_frame = sv.BoxAnnotator().annotate(scene=annotated_frame, detections=detections)\n        \n        # Apply label annotations with proper class names\n        annotated_frame = sv.LabelAnnotator(text_thickness=2).annotate(\n            scene=annotated_frame, \n            detections=detections,\n            labels=labels\n        )\n        \n        # Write the annotated frame to output video\n        sink.write_frame(annotated_frame)\n\nprint(f\"Video processing complete. Output saved to {TARGET_VIDEO_PATH}\")\n```"}, {"user": "dsbyprateekg", "created_at": "2025-03-23T09:23:33Z", "body": "@farukalamai Thanks a lot for sharing the code.\nIt's working."}, {"user": "probicheaux", "created_at": "2025-03-23T15:45:26Z", "body": "thanks for sharing that @farukalamai !"}, {"user": "MuhammadMoinFaisal", "created_at": "2025-03-24T10:18:10Z", "body": "Hi \nCan any one please share the code to do object detection using RF-DETR on Live Webcam Feed\n\nThanks"}, {"user": "ediardo", "created_at": "2025-03-25T21:35:26Z", "body": "> Hi Can any one please share the code to do object detection using RF-DETR on Live Webcam Feed\n> \n> Thanks\n\n@MuhammadMoinFaisal: for rtsp\n\n```py\nimport json\nimport cv2\nimport os\nfrom rfdetr import RFDETRBase\nimport supervision as sv\n\nmodel = RFDETRBase()\n\n# Load class mapping from JSON file\nwith open(\"classes.json\", \"r\") as f:\n    class_mapping = json.load(f)\n\nclass RTSPImageCapture:\n    def __init__(self, rtsp_url, output_dir):\n        self.rtsp_url = rtsp_url\n        self.output_dir = output_dir\n        self.cap = None\n        self.image_count = 0\n\n    def open_stream(self):\n        # Create a VideoCapture object to connect to the RTSP stream\n        self.cap = cv2.VideoCapture(self.rtsp_url)\n\n        # Check if the VideoCapture object was successfully created\n        if not self.cap.isOpened():\n            print(\"Error: Could not open RTSP stream.\")\n            exit()\n\n        # Create the output directory if it doesn't exist\n        os.makedirs(self.output_dir, exist_ok=True)\n\n    def capture_images(self):\n        while True:\n            # Capture a frame from the RTSP stream\n            ret, frame = self.cap.read()\n\n            # Check if the frame was captured successfully\n            if not ret:\n                print(\"Error: Could not read frame from RTSP stream.\")\n                break\n            \n            detections =model.predict(frame)\n            # Map numeric class IDs to class names for labels\n            labels = []\n            for class_id in detections.class_id:\n                # Convert int to string for dictionary lookup\n                class_id_str = str(class_id)\n                # Get class name if exists in mapping\n                class_name = class_mapping.get(class_id_str)\n                labels.append(class_name)\n\n            # Create annotated frame\n            annotated_frame = frame.copy()\n            \n            # Apply box annotations\n            annotated_frame = sv.BoxAnnotator().annotate(scene=annotated_frame, detections=detections)\n\n            # Apply label annotations with proper class names\n            annotated_frame = sv.LabelAnnotator(text_thickness=2).annotate(\n                scene=annotated_frame, \n                detections=detections,\n                labels=labels\n            )\n            # Display the captured frame (optional)\n            cv2.imshow('Captured Frame', annotated_frame)\n\n            # Exit the loop when 'q' is pressed\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    def close_stream(self):\n        # Release the VideoCapture object and close the OpenCV window\n        if self.cap is not None:\n            self.cap.release()\n            cv2.destroyAllWindows()\n\n    def main(self):\n        try:\n            self.open_stream()\n            self.capture_images()\n        finally:\n            self.close_stream()\n\nif __name__ == \"__main__\":\n    # Define the RTSP stream URL and output directory\n    rtsp_url = 'rtsp://username:passwd@192.168.1.203:554/stream1'\n\n    # Create an instance of the RTSPImageCapture class\n    image_capture = RTSPImageCapture(rtsp_url, output_dir)\n\n    # Run the main function of the class\n    image_capture.main()\n```\n\ncoco class mappings:\n```json\n{\n    \"1\": \"person\",\n    \"2\": \"bicycle\",\n    \"3\": \"car\",\n    \"4\": \"motorcycle\",\n    \"5\": \"airplane\",\n    \"6\": \"bus\",\n    \"7\": \"train\",\n    \"8\": \"truck\",\n    \"9\": \"boat\",\n    \"10\": \"traffic light\",\n    \"11\": \"fire hydrant\",\n    \"13\": \"stop sign\",\n    \"14\": \"parking meter\",\n    \"15\": \"bench\",\n    \"16\": \"bird\",\n    \"17\": \"cat\",\n    \"18\": \"dog\",\n    \"19\": \"horse\",\n    \"20\": \"sheep\",\n    \"21\": \"cow\",\n    \"22\": \"elephant\",\n    \"23\": \"bear\",\n    \"24\": \"zebra\",\n    \"25\": \"giraffe\",\n    \"27\": \"backpack\",\n    \"28\": \"umbrella\",\n    \"31\": \"handbag\",\n    \"32\": \"tie\",\n    \"33\": \"suitcase\",\n    \"34\": \"frisbee\",\n    \"35\": \"skis\",\n    \"36\": \"snowboard\",\n    \"37\": \"sports ball\",\n    \"38\": \"kite\",\n    \"39\": \"baseball bat\",\n    \"40\": \"baseball glove\",\n    \"41\": \"skateboard\",\n    \"42\": \"surfboard\",\n    \"43\": \"tennis racket\",\n    \"44\": \"bottle\",\n    \"46\": \"wine glass\",\n    \"47\": \"cup\",\n    \"48\": \"fork\",\n    \"49\": \"knife\",\n    \"50\": \"spoon\",\n    \"51\": \"bowl\",\n    \"52\": \"banana\",\n    \"53\": \"apple\",\n    \"54\": \"sandwich\",\n    \"55\": \"orange\",\n    \"56\": \"broccoli\",\n    \"57\": \"carrot\",\n    \"58\": \"hot dog\",\n    \"59\": \"pizza\",\n    \"60\": \"donut\",\n    \"61\": \"cake\",\n    \"62\": \"chair\",\n    \"63\": \"couch\",\n    \"64\": \"potted plant\",\n    \"65\": \"bed\",\n    \"67\": \"dining table\",\n    \"70\": \"toilet\",\n    \"72\": \"tv\",\n    \"73\": \"laptop\",\n    \"74\": \"mouse\",\n    \"75\": \"remote\",\n    \"76\": \"keyboard\",\n    \"77\": \"cell phone\",\n    \"78\": \"microwave\",\n    \"79\": \"oven\",\n    \"80\": \"toaster\",\n    \"81\": \"sink\",\n    \"82\": \"refrigerator\",\n    \"84\": \"book\",\n    \"85\": \"clock\",\n    \"86\": \"vase\",\n    \"87\": \"scissors\",\n    \"88\": \"teddy bear\",\n    \"89\": \"hair drier\",\n    \"90\": \"toothbrush\"\n  }\n```"}], "satisfaction_conditions": ["Code that processes a video file with a fine-tuned model and saves the output with annotations", "Complete, executable code sample that requires minimal modification", "Visual representation of model predictions on the video frames", "Integration with the specific model architecture (RF-DETR) the user is working with"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:10"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_27", "number": 42, "title": "No Log file?", "created_at": "2025-01-07T13:45:46Z", "closed_at": "2025-01-07T19:04:51Z", "commit_id": "348e5c925ba799601b10e745df4dc38f37403c38", "labels": [], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/42", "body": "Hello,\r\n\r\nI've ran the docker from ghcy and I found out I don't have any log in /var/logs, is that normal? The folder is empty. I tried deleting the folder and on next run, it create it back but still empty. I did change UID to 99 (the image do throw a warning because it's below 1000 but that's required for unraid) but even reverting to 1000 doesn't fix it.\r\n\r\nThank you", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/42/comments", "author": "nodiaque", "comments": [{"user": "calibrain", "created_at": "2025-01-07T19:05:24Z", "body": "Oups, you are right, I was never writing to it\r\nI fixed it, can you repull and retry now ?"}, {"user": "nodiaque", "created_at": "2025-01-07T19:34:24Z", "body": "Docker fail to start\r\n\r\nPermissionError: [Errno 13] Permission denied: '/var/log/cwa-book-downloader'"}, {"user": "nodiaque", "created_at": "2025-01-07T19:37:15Z", "body": "I think it's suppose to be /var/logs/? missing s I think."}, {"user": "nodiaque", "created_at": "2025-01-07T19:39:42Z", "body": "I made it work by mapping a path\r\n/var/log/cwa-book-downloader to a path on my guess. But I think it fail to create the folder else since that folder belong to root."}, {"user": "calibrain", "created_at": "2025-01-07T20:31:36Z", "body": "Oh by bad, I had an uncommitted change to create the folder !\r\nThank you for the heads up\r\nits now fixed"}], "satisfaction_conditions": ["A working log file system in the Docker container", "Proper permissions for the log directory", "Compatibility with custom UID settings", "Clear documentation about log file locations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:56"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_28", "number": 7, "title": "umt5-xxl-enc-bf16 OOM with 12GB VRAM", "created_at": "2025-02-25T23:47:38Z", "closed_at": "2025-02-26T16:28:12Z", "commit_id": "b81ea1d0f7dfc4e13c619be061f8a692eccaa7f9", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/7", "body": "The clip do not load with 12GB Vram (3090). Is it possible to get a FP8?\n\ngot prompt\n!!! Exception during processing !!! Allocation on device\nTraceback (most recent call last):\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 460, in loadmodel\n    T5_text_encoder = T5EncoderModel(\n                      ^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\wanvideo\\modules\\t5.py\", line 499, in __init__\n    set_module_tensor_to_device(model, name, device=device, dtype=dtype, value=state_dict[name])\n  File \"G:\\SD\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 330, in set_module_tensor_to_device\n    new_value = value.to(device)\n                ^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: Allocation on device\n\nGot an OOM, unloading all loaded models.\nPrompt executed in 3.95 seconds", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/7/comments", "author": "narikm", "comments": [{"user": "Skol600ml", "created_at": "2025-02-26T00:02:28Z", "body": "You need to reduce the tile sizes "}, {"user": "narikm", "created_at": "2025-02-26T00:15:00Z", "body": "> You need to reduce the tile sizes\n\nIt OOM before that point, as it simply load it."}, {"user": "itswhateverman", "created_at": "2025-02-26T00:21:55Z", "body": "i had to switch from the fp32 to the bf16 vae on 12gb for the text encoder not to OOM. seemed vae loads first and is just enough to make the difference. once the text encoder output is cached i can switch back, until i adjust the prompt (using the t2v example) "}, {"user": "narikm", "created_at": "2025-02-26T00:32:26Z", "body": "> i had to switch from the fp32 to the bf16 vae on 12gb for the text encoder not to OOM. seemed vae loads first and is just enough to make the difference. once the text encoder output is cached i can switch back, until i adjust the prompt (using the t2v example)\n\nStill OOM, but at the \"Wan text encode\" node."}, {"user": "JoeAu", "created_at": "2025-02-26T08:49:09Z", "body": "Is it possible to use FP8 or bnb4 quantization for T5-XXL, or run it on a CPU?"}, {"user": "Foul-Tarnished", "created_at": "2025-02-26T09:25:23Z", "body": "A rtx3090 has 24gb ??"}, {"user": "kijai", "created_at": "2025-02-26T09:36:19Z", "body": "> Is it possible to use FP8 or bnb4 quantization for T5-XXL, or run it on a CPU?\n\nAdded that now, seems to use ~4GB less VRAM for encoding, got past that stage under 10GB VRAM used when I tested now."}, {"user": "kijai", "created_at": "2025-02-26T09:58:12Z", "body": "> I'm having the same issue since i2v has clip + t5 loaded before it starts sampling which needs more than 12GB VRAM. It only occurs once in t2v.\n> \n> However I'm assuming you would still get an OOM when it tries to load the model since I can use t2v 1.3B but still got an OOM at t2v 14B (40% swap + compile) which is about the same size as i2v model.\n> \n> The fp8 clips are a necessity though.\n\nClip and T5 are not in VRAM at same time at any point as long as you have the force_offload enabled in the node (default)."}, {"user": "narikm", "created_at": "2025-02-26T16:28:12Z", "body": "Resolved by pulling new version."}], "satisfaction_conditions": ["A solution that reduces VRAM usage enough to run the T5-XXL encoder on a 12GB GPU", "A quantization approach that maintains model functionality while reducing memory requirements", "A solution that addresses the specific OOM error during model loading phase"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:04:44"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_29", "number": 37, "title": "Pre-train Data Structure", "created_at": "2025-03-21T17:10:40Z", "closed_at": "2025-03-23T02:36:39Z", "commit_id": "cf909a24296f8273a87c6322947e92bc3aff97c8", "labels": [], "url": "https://github.com/canopyai/Orpheus-TTS/issues/37", "body": "Thank you for sharing great work, I want to know about pre-train data format and it meaning given config file\n\n```\n> `\n> # Datasets\n> text_QA_dataset: <speech input-ids>\n> TTS_dataset: <text-input-ids>\n```\nBasically i want to know how can i prepare `text_QA_dataset` and `TTS_dataset` and it's format structure. i am waiting for your response and great-full to you.  \n\nWhat is the different between `text_QA_dataset` and `TTS_dataset`.", "comments_url": "https://api.github.com/repos/canopyai/Orpheus-TTS/issues/37/comments", "author": "saifulislam79", "comments": [{"user": "amuvarma13", "created_at": "2025-03-21T21:27:35Z", "body": "```python\ntokeniser_length = 128256\nstart_of_text = 128000\nend_of_text = 128009\n\nstart_of_speech = tokeniser_length + 1\nend_of_speech = tokeniser_length + 2\n\nstart_of_human = tokeniser_length + 3\nend_of_human = tokeniser_length + 4\n\nstart_of_ai = tokeniser_length + 5\nend_of_ai =  tokeniser_length + 6\npad_token = tokeniser_length + 7\n\naudio_tokens_start = tokeniser_length + 10\n```\n\nstart of human --- start of text --- text tokens --- end of text--- end of human--- start of ai --- start of speech --- speech tokens --- end of speech --- end of ai\n\n\nLet me know if unclear or further questions.\n\nEDIT - for text which I realise you also asked about:\n\nstart of human --- start of text --- question text tokens --- end of text--- end of human --- start of ai --- start of text --- answer text tokens --- end of text --- end of ai\n"}, {"user": "saifulislam79", "created_at": "2025-03-21T21:38:28Z", "body": "Thank you for your reply i had reviewed data processing code into colab, which mentioned into readme file. I need more clear understanding the processing approach,  Is it same processing approach for both fine-tune and pre-train . \n\n```\ndef create_input_ids(example):\n    text_ids = tokenizer.encode(example[\"text\"],  add_special_tokens=True)\n    text_ids.append(end_of_text)\n    example[\"text_tokens\"] = text_ids\n    input_ids = (\n        [start_of_human]\n        + example[\"text_tokens\"]\n        + [end_of_human]\n        + [start_of_ai]\n        + [start_of_speech]\n        + example[\"codes_list\"]\n        + [end_of_speech]\n        + [end_of_ai]\n    )\n    example[\"input_ids\"] = input_ids\n    example[\"labels\"] = input_ids\n    example[\"attention_mask\"] = [1] * len(input_ids)\n\n    return example\n```\n\nhere `text_QA_dataset` and `TTS_dataset` why mentions separately. `text_QA_dataset` is QA textual information with audio or `TTS_dataset` is as normal TTS dataset. it will more convenient , if possible share some data sample about `text_QA_dataset` and `TTS_dataset` format.\n\nI mean that same format as like fine-tune dataset but use different dataset or other. "}, {"user": "amuvarma13", "created_at": "2025-03-21T21:51:12Z", "body": "Yep the `text_QA_dataset` is only text no audio. `tts_dataset` is text and then a spoken version of the text. \n\nHere is what a text sample could look like, all the text samples are chained together so all input_ids are the same length (8192) for pretraining to make the training as efficient as possible:\n\nstart of human --- start of text --- question text tokens (i.e. AutoTokeniser.tokenise(\"What is 2 +2?\")  --- end of text--- end of human --- start of ai --- start of text --- (i.e. AutoTokeniser.tokenise(\"Great question, 2 + 2 =4\") --- end of text --- end of ai\n"}, {"user": "amuvarma13", "created_at": "2025-03-21T21:55:24Z", "body": "Feel free to close this issue - if your question is answered!"}, {"user": "saifulislam79", "created_at": "2025-03-21T22:19:18Z", "body": "**This is the last clarification**\nExample with Token IDs (simplified illustration)\nAssume the tokenizer produces the following (again, just for illustration):\n**input sentence 1** : What is 2 + 2? ----> audio1.mp3\n\n **Answer other sentence** : Great question, 2 + 2 = 4. --->  audio2.mp3\n\n\n```\n\"start of human\" → [101]\n\"start of text\" → [102]\n\"What is 2 + 2?\" → [2001, 2002, 2003, 2004, 2005]\n\"end of text\" → [103]\n\"end of human\" → [104]\n\"start of ai\" → [105]\n\"start of text\" → [102]\n\"Great question, 2 + 2 = 4.\" → [3001, 3002, 3003, 3004, 3005, 3006]\n\"end of text\" → [103]\n\"end of ai\" → [106]\n```\n\n\nChained together example of question and answer:\n\n`[101, 102, 2001, 2002, 2003, 2004, 2005, 103, 104, 105, 102, 3001, 3002, 3003, 3004, 3005, 3006, 103, 106]`\n\nif i have 1M text sentences and it's corresponding audio codes,  what will be `<speech input-ids>` and `<text-input-ids>`. Could you please give a example ."}, {"user": "saiful9379", "created_at": "2025-03-22T09:25:31Z", "body": "@amuvarma13  thank for your clarification. "}, {"user": "amuvarma13", "created_at": "2025-03-22T09:36:55Z", "body": "Sure, \nText input ids (text dataset) is for text question text answer pairs - the format you have given above is correct.\nSpeech input ids i.e. the tts dataset is for text speech pairs no question answering - the format I gave above with start of speech etc is what you want for this,.\n\n"}, {"user": "amuvarma13", "created_at": "2025-03-23T02:36:39Z", "body": "Marking as solved - reopen if unclear."}], "satisfaction_conditions": ["Clear explanation of the difference between text_QA_dataset and TTS_dataset", "Explanation of the data format structure for both dataset types", "Concrete examples showing the token sequence structure", "Clarification on how to process large datasets with the described format"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:16"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_30", "number": 108, "title": "Always getting the error: \"AssertionError exception: no description\" ", "created_at": "2025-01-07T21:09:23Z", "closed_at": "2025-01-20T16:21:39Z", "commit_id": "681758ae84a8075038dc676d8af7262077bd00c3", "labels": [], "url": "https://github.com/huggingface/smolagents/issues/108", "body": "No matter what I do to modify the docstring I always get the same error as mentioned in the title.\r\n\r\nHere is a tool that I have created.\r\n\r\nI would like to know what within my docstrings is causing this.\r\n\r\n```python\r\n\r\ncg = CoinGeckoAPI(demo_api_key=os.getenv('coingecko_api_key'))\r\n\r\n@tool\r\ndef get_coins_list(currency: str) -> list:\r\n    \"\"\"\r\n    This tool makes a query to the CoinGecko API to get a response of ALL of the supported coins with their price, market cap, volume and related market data in USD.\r\n\r\n    Args:\r\n        currency: The dollar value which the coin should be represented into\r\n    \"\"\"\r\n    return cg.get_coins_markets(vs_currency=currency)\r\n\r\n```", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/108/comments", "author": "jondoescoding", "comments": [{"user": "whoahaow", "created_at": "2025-01-07T21:34:16Z", "body": "does it fix it?\r\n\r\n```python\r\ncg = CoinGeckoAPI(api_key=os.getenv('coingecko_api_key'))\r\n\r\nclass GetCoinsListTool(Tool):\r\n    name = \"get_coins_list\"\r\n    description = \"\"\"\r\n    This tool makes a query to the CoinGecko API to get a response of ALL of the supported coins with their price, market cap, volume and related market data in USD.\r\n    \"\"\"\r\n    inputs = {\r\n        \"currency\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The currency in which the coin data should be represented (e.g., 'usd', 'eur').\"\r\n        }\r\n    }\r\n    output_type = \"list\"\r\n\r\n    def forward(self, currency: str) -> list:\r\n        return cg.get_coins_markets(vs_currency=currency)\r\n```"}, {"user": "jondoescoding", "created_at": "2025-01-07T21:45:48Z", "body": "Got the same error.\r\n\r\n```python\r\nException has occurred: AssertionError\r\nexception: no description\r\n\r\nException has occurred: AssertionError\r\nexception: no description\r\n  File \"...\\coingecko_agent\\agent.py\", line 7, in <module>\r\n    coin_list_tool = GetCoinsListTool()\r\n                     ^^^^^^^^^^^^^^^^^^\r\nAssertionError: \r\n\r\n```"}, {"user": "whoahaow", "created_at": "2025-01-07T22:25:44Z", "body": "I don't know if this is suitable for you, but here's what I did:\r\n```python\r\nfrom smolagents import CodeAgent, HfApiModel, Tool\r\nimport os\r\nfrom pycoingecko import CoinGeckoAPI\r\nimport json\r\n\r\n# Initialize CoinGecko API client\r\ncg = CoinGeckoAPI(api_key=os.getenv('coingecko_api_key'))\r\n\r\n# Define the GetCoinsListTool class\r\nclass GetCoinsListTool(Tool):\r\n    name = \"get_coins_list\"\r\n    description = \"\"\"\r\n    This tool makes a query to the CoinGecko API to get a response of ALL of the supported coins with their price, market cap, volume and related market data in USD.\r\n    You need to import json. The output is a JSON string. You should use the `json` module to parse this string into a Python list.\r\n    \"\"\"\r\n    inputs = {\r\n        \"currency\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The currency in which the coin data should be represented (e.g., 'usd', 'eur').\"\r\n        }\r\n    }\r\n    output_type = \"string\"  # Change to 'string'\r\n\r\n    def forward(self, currency: str) -> str:\r\n        coins_data = cg.get_coins_markets(vs_currency=currency)\r\n        return json.dumps(coins_data)  # Convert the list to a JSON string\r\n\r\n# Initialize the model\r\nmodel = HfApiModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\r\n\r\n# Initialize the agent with the tool\r\nagent = CodeAgent(\r\n    tools=[GetCoinsListTool()],\r\n    model=model,\r\n    add_base_tools=True,\r\n    additional_authorized_imports=[\"json\"]  # Authorize the json module\r\n)\r\n\r\n# Run the agent with a task\r\ntask = \"Get the list of coins in USD and print the first 5 entries. Then present it as usual text.\"\r\nresult = agent.run(task)\r\n\r\n# Print the result\r\nprint(\"Agent Output:\")\r\nprint(result)\r\n```"}, {"user": "jondoescoding", "created_at": "2025-01-07T23:24:32Z", "body": "Works like a charm. Thanks! But why does the the @tool decorator not work?"}, {"user": "aymeric-roucher", "created_at": "2025-01-09T10:24:13Z", "body": "@jondoescoding could you provide your full error trace and package versions? I tried to reproduce but for me your code snippet works"}], "satisfaction_conditions": ["A working solution that resolves the 'AssertionError: no description' error", "A functional way to create a CoinGecko API tool that can be used with their agent"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:34"}, "dockerfile": null, "language": "python"}
{"task_id": "cab_lenient_31", "number": 77, "title": "ESM Module Error: @dmitryrechkin/json-schema-to-zod compatibility with agent-kit", "created_at": "2025-02-27T00:13:03Z", "closed_at": "2025-02-27T13:00:01Z", "commit_id": "e73e07e908946e21261e9abbc03e853a01ac6774", "labels": [], "url": "https://github.com/inngest/agent-kit/issues/77", "body": "## Bug Description\nWhen using the latest version of @inngest/agent-kit (v0.3.0) with Next.js, I'm encountering an ESM/CommonJS compatibility issue. The agent-kit package tries to import @dmitryrechkin/json-schema-to-zod using CommonJS require(), but that package is an ESM module.\n\nWhen using @inngest/agent-kit v0.2.2:\n\n ⨯ [Error: require() of ES Module /Users/ruby/code/nexus-workflow/node_modules/@dmitryrechkin/json-schema-to-zod/dist/index.js from /Users/ruby/code/nexus-workflow/node_modules/@inngest/agent-kit/dist/agent.js not supported.\nInstead change the require of index.js in /Users/ruby/code/nexus-workflow/node_modules/@inngest/agent-kit/dist/agent.js to a dynamic import() which is available in all CommonJS modules.] {\n  code: 'ERR_REQUIRE_ESM'\n}\n\n\n\n## Environment\n- Next.js: 15.1.3\n- @inngest/agent-kit: Tested v0.1.2 (works), v0.2.2 and v0.3.0 (both fail)\n- inngest: 3.31.11\n- React: 19.0.0\n- Node.js version: 20.15.1\n\n## Reproduction Steps\n1. Set up a Next.js project with dependencies as listed above\n2. Install @inngest/agent-kit v0.1.2 (works correctly)\n3. Upgrade to @inngest/agent-kit v0.2.2 or v0.3.0\n4. Run the development server (npm run dev)\n5. The server fails with ESM/CommonJS compatibility errors\n\n## Attempted Solutions\nI've tried various workarounds including:\n- Adding transpilePackages: ['@inngest/agent-kit', '@dmitryrechkin/json-schema-to-zod'] to next.config.js\n- Setting experimental.esmExternals to 'loose' in next.config.js\n- Creating a bridge module that avoids using agent-kit directly and falls back to inngest.step.ai.infer()\n- Modifying webpack configuration to handle ESM modules\n- Downgrading from v0.3.0 to v0.2.2 (but encountered similar errors with pkce-challenge)\n\nNone of these solutions have fully resolved the issue. The only version that works correctly is v0.1.2, but it lacks the newer features I need.\n\n## Suggested Fix\nThe agent-kit package should be updated to:\n1. Use dynamic import() instead of require() when importing ESM modules\n2. Provide a compatibility layer for both ESM and CommonJS environments\n3. Update dependencies to versions that support dual module systems\n\nThis issue affects the usability of agent-kit in Next.js projects, which is a common use case for Inngest functions.", "comments_url": "https://api.github.com/repos/inngest/agent-kit/issues/77/comments", "author": "eraykeskinmac", "comments": [{"user": "eraykeskinmac", "created_at": "2025-02-27T00:13:56Z", "body": "now package.json \n\n```\n{\n  \"name\": \"nexus-workflow\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\",\n    \"ingest\": \"inngest dev\"\n  },\n  \"dependencies\": {\n    \"@deepgram/sdk\": \"^3.9.0\",\n    \"@inngest/agent-kit\": \"^0.2.2\",\n    \"@types/dotenv\": \"^8.2.3\",\n    \"@vercel/blob\": \"^0.27.0\",\n    \"axios\": \"^1.7.9\",\n    \"date-fns\": \"^4.1.0\",\n    \"dotenv\": \"^16.4.7\",\n    \"inngest\": \"^3.31.11\",\n    \"inngest-cli\": \"^1.4.8\",\n    \"libphonenumber-js\": \"^1.11.17\",\n    \"next\": \"15.1.3\",\n    \"react\": \"^19.0.0\",\n    \"react-dom\": \"^19.0.0\"\n  },\n  \"devDependencies\": {\n    \"@eslint/eslintrc\": \"^3\",\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^19\",\n    \"@types/react-dom\": \"^19\",\n    \"eslint\": \"^9\",\n    \"eslint-config-next\": \"15.1.3\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.4.1\",\n    \"typescript\": \"^5\"\n  }\n}\n```\n\nold package.json\n\n```\n{\n  \"name\": \"nexus-workflow\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\",\n    \"ingest\": \"inngest dev\"\n  },\n  \"dependencies\": {\n    \"@deepgram/sdk\": \"^3.9.0\",\n    \"@inngest/agent-kit\": \"^0.1.2\",\n    \"@types/dotenv\": \"^8.2.3\",\n    \"@vercel/blob\": \"^0.27.0\",\n    \"axios\": \"^1.7.9\",\n    \"date-fns\": \"^4.1.0\",\n    \"dotenv\": \"^16.4.7\",\n    \"inngest\": \"^3.28.0\",\n    \"inngest-cli\": \"^1.3.3\",\n    \"libphonenumber-js\": \"^1.11.17\",\n    \"next\": \"15.1.3\",\n    \"react\": \"^19.0.0\",\n    \"react-dom\": \"^19.0.0\"\n  },\n  \"devDependencies\": {\n    \"@eslint/eslintrc\": \"^3\",\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^19\",\n    \"@types/react-dom\": \"^19\",\n    \"eslint\": \"^9\",\n    \"eslint-config-next\": \"15.1.3\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.4.1\",\n    \"typescript\": \"^5\"\n  }\n}\n```\n"}, {"user": "jpwilliams", "created_at": "2025-02-27T12:39:39Z", "body": "Thanks for the detailed report, @eraykeskinmac!\n\nCould you try using `@inngest/agent-kit@0.3.1`? This offers dual CJS and ESM builds so may resolve the issue immediately.\n\nIf not, we can ship another change to handle this case."}, {"user": "eraykeskinmac", "created_at": "2025-02-27T12:50:03Z", "body": "Thanks! I tried the @inngest/agent-kit@0.3.1 version and it worked perfectly without any issues. This solution was exactly what I needed. I was looking to use Agent Kit and Inngest's new features, especially the AI inference capability, so this fix was really valuable for me. Thank you for your help!"}, {"user": "jpwilliams", "created_at": "2025-02-27T13:00:01Z", "body": "Awesome! Thanks for testing and glad it's looking good, @eraykeskinmac! 🙌 \n\ncc @MonsterDeveloper - thanks for the PR 🙂 "}], "satisfaction_conditions": ["A version of @inngest/agent-kit that resolves the ESM/CommonJS compatibility issue", "Access to newer features of the agent-kit package", "Compatibility with Next.js projects", "A solution that doesn't require complex workarounds"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:19"}, "dockerfile": null, "language": "typescript"}
{"task_id": "cab_lenient_32", "number": 17, "title": "Cannot find package '.../node_modules/mcp-proxy/dist/MCPProxy.js' when running npx fastmcp dev", "created_at": "2025-03-09T18:28:44Z", "closed_at": "2025-03-09T21:58:21Z", "commit_id": "1314f06919e1f20b725d648390336ce4afe16a23", "labels": [], "url": "https://github.com/punkpeye/fastmcp/issues/17", "body": "I'm trying to run the example server from the repo and am getting the following error with both `npx fastmcp dev src/server.ts`. This also occurs if I build the server and run `npx fastmcp dev dist/server.js`.\n\n```\n$ npx fastmcp dev src/server.ts\n\nnode:internal/modules/run_main:104\n    triggerUncaughtException(\n    ^\nError: Cannot find package '/Users/nbbaier/my-mcp-server/node_modules/mcp-proxy/dist/MCPProxy.js' imported from /Users/nbbaier/my-mcp-server/node_modules/fastmcp/dist/FastMCP.js\n    at legacyMainResolve (node:internal/modules/esm/resolve:204:26)\n    at packageResolve (node:internal/modules/esm/resolve:778:12)\n    at moduleResolve (node:internal/modules/esm/resolve:854:18)\n    at defaultResolve (node:internal/modules/esm/resolve:984:11)\n    at nextResolve (node:internal/modules/esm/hooks:748:28)\n    at resolveBase (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:3212)\n    at resolveDirectory (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:3584)\n    at resolveTsPaths (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:4073)\n    at resolve (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:4447)\n    at nextResolve (node:internal/modules/esm/hooks:748:28) {\n  code: 'ERR_MODULE_NOT_FOUND'\n}\n\nNode.js v23.9.0\nfile:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/protocol.js:93\n        const error = new McpError(ErrorCode.ConnectionClosed, \"Connection closed\");\n                      ^\n\nMcpError: MCP error -32000: Connection closed\n    at Client._onclose (file:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/protocol.js:93:23)\n    at _transport.onclose (file:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/protocol.js:68:18)\n    at ChildProcess.<anonymous> (file:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/client/stdio.js:85:77)\n    at ChildProcess.emit (node:events:507:28)\n    at maybeClose (node:internal/child_process:1101:16)\n    at ChildProcess._handle.onexit (node:internal/child_process:305:5) {\n  code: -32000,\n  data: undefined\n}\n\nNode.js v23.9.0\n```", "comments_url": "https://api.github.com/repos/punkpeye/fastmcp/issues/17/comments", "author": "nbbaier", "comments": [{"user": "nbbaier", "created_at": "2025-03-09T18:30:50Z", "body": "For reference, this is my `src/server.ts`:\n\n```ts\nimport { FastMCP } from \"fastmcp\";\nimport { z } from \"zod\";\n\nconst server = new FastMCP({\n  name: \"Addition\",\n  version: \"1.0.0\",\n});\n\nserver.addTool({\n  name: \"add\",\n  description: \"Add two numbers\",\n  parameters: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n  execute: async (args) => {\n    return String(args.a + args.b);\n  },\n});\n\nserver.start({\n  transportType: \"stdio\",\n});\n\n```"}, {"user": "punkpeye", "created_at": "2025-03-09T21:59:30Z", "body": "I believe this was just a badly resolved dependency. If you update your dependencies and try again, it should be fixed."}, {"user": "nbbaier", "created_at": "2025-03-10T06:18:48Z", "body": "Yeah that worked great, thanks!"}], "satisfaction_conditions": ["A solution that resolves the dependency error for mcp-proxy", "A straightforward fix that allows the user to run their FastMCP server", "A solution that addresses the dependency resolution without requiring code changes"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:00:57"}, "dockerfile": "FROM node:20-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install necessary tools\nRUN apt-get update && \\\n    apt-get install -y git && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/punkpeye/fastmcp.git . && \\\n    git checkout 1314f06919e1f20b725d648390336ce4afe16a23\n\n# Install pnpm (as used in the project)\nRUN npm install -g pnpm@9\n\n# Install project dependencies\nRUN pnpm install\n\n# Build the project\nRUN pnpm build\n\n# This Dockerfile sets up an environment with:\n# 1. Node.js 20 as the base image\n# 2. Git installed to clone the repository\n# 3. The fastmcp repository cloned and checked out to the specific commit\n# 4. PNPM installed (as used in the project workflows)\n# 5. Project dependencies installed\n# 6. Project built and ready to use", "language": "typescript"}
{"task_id": "cab_lenient_33", "number": 38, "title": "SyntaxError: JSON Parse error: Unterminated string", "created_at": "2025-02-06T15:56:05Z", "closed_at": "2025-02-06T16:21:20Z", "commit_id": "53fffe4877ad15ab44ff972338bfae72f1560fd3", "labels": [], "url": "https://github.com/jina-ai/node-DeepResearch/issues/38", "body": "model: gemini-2.0-flash\n\nI encounter the following error every time I run the deepresearch\n\n```\nStep 3 / Budget used 0.21%\nGaps: []\nObject not generated according to the schema, fallback to manual parsing\n14 |   constructor({\n15 |     name: name14,\n16 |     message,\n17 |     cause\n18 |   }) {\n19 |     super(message);\n         ^\nAI_NoObjectGeneratedError: No object generated: could not parse the response.\n       text: \"{\\n  \\\"action\\\": \\\"answer\\\",\\n  \\\"think\\\": \\\"I have gathered enough information from the visited URLs to provide a detailed technical report on the enemy AI systems in the Destiny and Halo series. The information covers background knowledge, design details, and algorithmic principles.\\\",\\n  \\\"answer\\\": \\\"## Technical Report: Enemy AI Systems in Destiny and Halo Series\\\\n\\\\n### 1. Background\\\\n\\\\nThe Halo and Destiny series, both developed by Bungie (with Destiny later developed by Bungie independently), are renowned for their sophisticated enemy AI. The AI systems in these games are designed not merely to present a challenge but to create engaging and believable combat encounters. A core design philosophy revolves around the concept of \\\\\\\"30 seconds of fun,\\\\\\\" where each encounter should offer distinct phases of engagement, planning, and execution, each with associated pleasures and rewards.\\\\n\\\\n### 2. Design Details\\\\n\\\\n#### 2.1. Halo AI Design\\\\n\\\\n*   **Territoriality:** Halo's AI emphasizes territorial control. Enemies are assigned territories, and their behavior is dictated by their need to defend or expand these zones. This design is closely tied to the player's recharging shield mechanic, providing safe zones and a sense of progress as the player advances.\\\\n*   **Limited Knowledge Model:** To simulate realistic combat, Halo's AI operates with a limited knowledge model. Enemies do not have perfect awareness of the player's location at all times, allowing for stealth and flanking maneuvers. AI misperception, where enemies assume the player is still where they last saw them, adds depth to the gameplay.\\\\n*   **Satisfying Enemies:** Enemies are designed to be cunning, formidable, and tactical. They react to the player's presence, making the player feel important, and their actions reflect the impact of the player's choices. Enemies possess capabilities roughly equivalent to the player, including the ability to use vehicles and weapons.\\\\n\\\\n#### 2.2. Destiny AI Design\\\\n\\\\nWhile specific technical details of Destiny's AI are less readily available, the game's AI shares similarities with Halo, emphasizing challenging and varied enemy behaviors. Destiny features a range of adversaries, each with unique behaviors, attack patterns, and decision-making algorithms. The Taken faction is often cited as having more complex and engaging AI compared to other enemy types.\\\\n\\\\n### 3. Algorithmic Principles\\\\n\\\\n#### 3.1. Perception and Memory\\\\n\\\\n*   **Simulated Senses:** AI characters perceive the game world through simulated senses, primarily vision and hearing. This mirrors the player's sensory experience, allowing players to anticipate AI behavior based on their own understanding of the environment.\\\\n*   **Memory Structures:** The AI maintains memory structures to track observed entities and events. This memory is not perfect; it decays over time and is subject to misinterpretation, contributing to the AI's realistic behavior.\\\\n*   **Combat Information Processing:** Raw sensory data is processed into higher-level combat information, such as identifying enemies, assessing threats, and determining the state of battle.\\\\n\\\\n#### 3.2. Decision Making\\\\n\\\\n*   **Hierarchical Behavior Trees:** The AI's decision-making process is often structured using hierarchical behavior trees. These trees define the AI's possible states (e.g., idle, guard/patrol, attack/defend, retreat) and the conditions that trigger transitions between them.\\\\n*   **Goal-Oriented Behavior:** AI behavior is goal-oriented, with the AI constantly evaluating its situation and selecting actions that best achieve its current goals. This includes selecting advantageous positions, coordinating with allies, and adapting to changes in the environment.\\\\n*   **Predictable Actions, Unpredictable Consequences:** The AI aims for predictable actions to allow players to develop strategies, but the consequences of those actions are less predictable, contributing to replayability.\\\\n\\\\n#### 3.3. Pathfinding\\\\n\\\\n*   **Navigation Meshes:** AI characters navigate the game world using navigation meshes (navmeshes), which are precomputed representations of the traversable areas in a level. The AI uses pathfinding algorithms (e.g., A*) to find the shortest or most efficient path to a desired location on the navmesh.\\\\n*   **Object Awareness:** The AI is aware of objects in the environment and their properties (e.g., size, traversability). This allows the AI to make informed decisions about how to navigate around or interact with objects.\\\\n*   **Animation Integration:** Animation is used to smooth out the look of pathfinding, making the AI's movements appear more natural and fluid. Object tags are used to identify interactive elements, such as cover points or climbable objects.\\\\n\\\\n#### 3.4. Group Coordination\\\\n\\\\n\",\n   response: {\n  id: \"aiobj-yOLPuzuXROWzgNwXWoA83JKT\",\n  timestamp: 2025-02-06T15:53:02.039Z,\n  modelId: \"gemini-2.0-flash\",\n},\n      usage: {\n  promptTokens: 18512,\n  completionTokens: 998,\n  totalTokens: 19510,\n},\n vercel.ai.error: true,\n vercel.ai.error.AI_NoObjectGeneratedError: true,\n\n      at new _AISDKError (.\\node_modules\\@ai-sdk\\provider\\dist\\index.mjs:19:5)\n\n14 |   constructor({\n15 |     name: name14,\n16 |     message,\n17 |     cause\n18 |   }) {\n19 |     super(message);\n         ^\nAI_JSONParseError: JSON parsing failed: Text: {\n  \"action\": \"answer\",\n  \"think\": \"I have gathered enough information from the visited URLs to provide a detailed technical report on the enemy AI systems in the Destiny and Halo series. The information covers background knowledge, design details, and algorithmic principles.\",\n  \"answer\": \"## Technical Report: Enemy AI Systems in Destiny and Halo Series\\n\\n### 1. Background\\n\\nThe Halo and Destiny series, both developed by Bungie (with Destiny later developed by Bungie independently), are renowned for their sophisticated enemy AI. The AI systems in these games are designed not merely to present a challenge but to create engaging and believable combat encounters. A core design philosophy revolves around the concept of \\\"30 seconds of fun,\\\" where each encounter should offer distinct phases of engagement, planning, and execution, each with associated pleasures and rewards.\\n\\n### 2. Design Details\\n\\n#### 2.1. Halo AI Design\\n\\n*   **Territoriality:** Halo's AI emphasizes territorial control. Enemies are assigned territories, and their behavior is dictated by their need to defend or expand these zones. This design is closely tied to the player's recharging shield mechanic, providing safe zones and a sense of progress as the player advances.\\n*   **Limited Knowledge Model:** To simulate realistic combat, Halo's AI operates with a limited knowledge model. Enemies do not have perfect awareness of the player's location at all times, allowing for stealth and flanking maneuvers. AI misperception, where enemies assume the player is still where they last saw them, adds depth to the gameplay.\\n*   **Satisfying Enemies:** Enemies are designed to be cunning, formidable, and tactical. They react to the player's presence, making the player feel important, and their actions reflect the impact of the player's choices. Enemies possess capabilities roughly equivalent to the player, including the ability to use vehicles and weapons.\\n\\n#### 2.2. Destiny AI Design\\n\\nWhile specific technical details of Destiny's AI are less readily available, the game's AI shares similarities with Halo, emphasizing challenging and varied enemy behaviors. Destiny features a range of adversaries, each with unique behaviors, attack patterns, and decision-making algorithms. The Taken faction is often cited as having more complex and engaging AI compared to other enemy types.\\n\\n### 3. Algorithmic Principles\\n\\n#### 3.1. Perception and Memory\\n\\n*   **Simulated Senses:** AI characters perceive the game world through simulated senses, primarily vision and hearing. This mirrors the player's sensory experience, allowing players to anticipate AI behavior based on their own understanding of the environment.\\n*   **Memory Structures:** The AI maintains memory structures to track observed entities and events. This memory is not perfect; it decays over time and is subject to misinterpretation, contributing to the AI's realistic behavior.\\n*   **Combat Information Processing:** Raw sensory data is processed into higher-level combat information, such as identifying enemies, assessing threats, and determining the state of battle.\\n\\n#### 3.2. Decision Making\\n\\n*   **Hierarchical Behavior Trees:** The AI's decision-making process is often structured using hierarchical behavior trees. These trees define the AI's possible states (e.g., idle, guard/patrol, attack/defend, retreat) and the conditions that trigger transitions between them.\\n*   **Goal-Oriented Behavior:** AI behavior is goal-oriented, with the AI constantly evaluating its situation and selecting actions that best achieve its current goals. This includes selecting advantageous positions, coordinating with allies, and adapting to changes in the environment.\\n*   **Predictable Actions, Unpredictable Consequences:** The AI aims for predictable actions to allow players to develop strategies, but the consequences of those actions are less predictable, contributing to replayability.\\n\\n#### 3.3. Pathfinding\\n\\n*   **Navigation Meshes:** AI characters navigate the game world using navigation meshes (navmeshes), which are precomputed representations of the traversable areas in a level. The AI uses pathfinding algorithms (e.g., A*) to find the shortest or most efficient path to a desired location on the navmesh.\\n*   **Object Awareness:** The AI is aware of objects in the environment and their properties (e.g., size, traversability). This allows the AI to make informed decisions about how to navigate around or interact with objects.\\n*   **Animation Integration:** Animation is used to smooth out the look of pathfinding, making the AI's movements appear more natural and fluid. Object tags are used to identify interactive elements, such as cover points or climbable objects.\\n\\n#### 3.4. Group Coordination\\n\\n.\nError message: JSON Parse error: Unterminated string\n      cause: SyntaxError: JSON Parse error: Unterminated string\n,\n       text: \"{\\n  \\\"action\\\": \\\"answer\\\",\\n  \\\"think\\\": \\\"I have gathered enough information from the visited URLs to provide a detailed technical report on the enemy AI systems in the Destiny and Halo series. The information covers background knowledge, design details, and algorithmic principles.\\\",\\n  \\\"answer\\\": \\\"## Technical Report: Enemy AI Systems in Destiny and Halo Series\\\\n\\\\n### 1. Background\\\\n\\\\nThe Halo and Destiny series, both developed by Bungie (with Destiny later developed by Bungie independently), are renowned for their sophisticated enemy AI. The AI systems in these games are designed not merely to present a challenge but to create engaging and believable combat encounters. A core design philosophy revolves around the concept of \\\\\\\"30 seconds of fun,\\\\\\\" where each encounter should offer distinct phases of engagement, planning, and execution, each with associated pleasures and rewards.\\\\n\\\\n### 2. Design Details\\\\n\\\\n#### 2.1. Halo AI Design\\\\n\\\\n*   **Territoriality:** Halo's AI emphasizes territorial control. Enemies are assigned territories, and their behavior is dictated by their need to defend or expand these zones. This design is closely tied to the player's recharging shield mechanic, providing safe zones and a sense of progress as the player advances.\\\\n*   **Limited Knowledge Model:** To simulate realistic combat, Halo's AI operates with a limited knowledge model. Enemies do not have perfect awareness of the player's location at all times, allowing for stealth and flanking maneuvers. AI misperception, where enemies assume the player is still where they last saw them, adds depth to the gameplay.\\\\n*   **Satisfying Enemies:** Enemies are designed to be cunning, formidable, and tactical. They react to the player's presence, making the player feel important, and their actions reflect the impact of the player's choices. Enemies possess capabilities roughly equivalent to the player, including the ability to use vehicles and weapons.\\\\n\\\\n#### 2.2. Destiny AI Design\\\\n\\\\nWhile specific technical details of Destiny's AI are less readily available, the game's AI shares similarities with Halo, emphasizing challenging and varied enemy behaviors. Destiny features a range of adversaries, each with unique behaviors, attack patterns, and decision-making algorithms. The Taken faction is often cited as having more complex and engaging AI compared to other enemy types.\\\\n\\\\n### 3. Algorithmic Principles\\\\n\\\\n#### 3.1. Perception and Memory\\\\n\\\\n*   **Simulated Senses:** AI characters perceive the game world through simulated senses, primarily vision and hearing. This mirrors the player's sensory experience, allowing players to anticipate AI behavior based on their own understanding of the environment.\\\\n*   **Memory Structures:** The AI maintains memory structures to track observed entities and events. This memory is not perfect; it decays over time and is subject to misinterpretation, contributing to the AI's realistic behavior.\\\\n*   **Combat Information Processing:** Raw sensory data is processed into higher-level combat information, such as identifying enemies, assessing threats, and determining the state of battle.\\\\n\\\\n#### 3.2. Decision Making\\\\n\\\\n*   **Hierarchical Behavior Trees:** The AI's decision-making process is often structured using hierarchical behavior trees. These trees define the AI's possible states (e.g., idle, guard/patrol, attack/defend, retreat) and the conditions that trigger transitions between them.\\\\n*   **Goal-Oriented Behavior:** AI behavior is goal-oriented, with the AI constantly evaluating its situation and selecting actions that best achieve its current goals. This includes selecting advantageous positions, coordinating with allies, and adapting to changes in the environment.\\\\n*   **Predictable Actions, Unpredictable Consequences:** The AI aims for predictable actions to allow players to develop strategies, but the consequences of those actions are less predictable, contributing to replayability.\\\\n\\\\n#### 3.3. Pathfinding\\\\n\\\\n*   **Navigation Meshes:** AI characters navigate the game world using navigation meshes (navmeshes), which are precomputed representations of the traversable areas in a level. The AI uses pathfinding algorithms (e.g., A*) to find the shortest or most efficient path to a desired location on the navmesh.\\\\n*   **Object Awareness:** The AI is aware of objects in the environment and their properties (e.g., size, traversability). This allows the AI to make informed decisions about how to navigate around or interact with objects.\\\\n*   **Animation Integration:** Animation is used to smooth out the look of pathfinding, making the AI's movements appear more natural and fluid. Object tags are used to identify interactive elements, such as cover points or climbable objects.\\\\n\\\\n#### 3.4. Group Coordination\\\\n\\\\n\",\n vercel.ai.error: true,\n vercel.ai.error.AI_JSONParseError: true,\n\n      at new _AISDKError (.\\node_modules\\@ai-sdk\\provider\\dist\\index.mjs:19:5)\n      at new JSONParseError (.\\node_modules\\@ai-sdk\\provider\\dist\\index.mjs:177:5)\n      at safeParseJSON (.\\node_modules\\@ai-sdk\\provider-utils\\dist\\index.mjs:372:57)\n      at <anonymous> (.\\node_modules\\ai\\dist\\index.mjs:2675:27)\n\nSyntaxError: JSON Parse error: Unterminated string\n```", "comments_url": "https://api.github.com/repos/jina-ai/node-DeepResearch/issues/38/comments", "author": "ArnoChenFx", "comments": [{"user": "hanxiao", "created_at": "2025-02-06T16:09:03Z", "body": "`git pull` latest?"}, {"user": "ArnoChenFx", "created_at": "2025-02-06T16:09:24Z", "body": "It seems the issue is caused by reaching the maxTokens limit."}, {"user": "hanxiao", "created_at": "2025-02-06T16:11:11Z", "body": "oh yes, default is `maxTokens=1000` is probably too small for long doc gen, u can change it in `config.ts`\n\ni actually used this more for super-deep search on some queries, so didn't hit that limit."}, {"user": "ArnoChenFx", "created_at": "2025-02-06T16:21:17Z", "body": "> oh yes, default is `maxTokens=1000` is probably too small for long doc gen, u can change it in `config.ts`\n> \n> i actually used this more for super-deep search on some queries, so didn't hit that limit.\n\nit works!"}], "satisfaction_conditions": ["A solution that addresses the JSON parsing error by increasing the token limit", "A configuration adjustment that allows for generating longer documents", "Clear guidance on where to make the necessary configuration change"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:01:42"}, "dockerfile": "FROM node:20-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install git for cloning the repository\nRUN apt-get update && apt-get install -y git && apt-get clean\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/jina-ai/node-DeepResearch.git . && \\\n    git checkout 53fffe4877ad15ab44ff972338bfae72f1560fd3\n\n# Install dependencies\nRUN npm install --ignore-scripts\n\n# Build the project\nRUN npm run build\n\n# Set environment variables as placeholders (to be overridden at runtime)\nENV GEMINI_API_KEY=\"\"\nENV OPENAI_API_KEY=\"\"\nENV JINA_API_KEY=\"\"\nENV BRAVE_API_KEY=\"\"\nENV LLM_PROVIDER=\"gemini\"\nENV DEFAULT_MODEL_NAME=\"gemini-2.0-flash\"\n\n# Expose the port the server runs on\nEXPOSE 3000\n\n# Set default command (commented out - user can override)\n# CMD [\"npm\", \"run\", \"serve\"]", "language": "typescript"}
{"task_id": "cab_lenient_34", "number": 27, "title": "Please Update README.md to mention new TOOLs", "created_at": "2025-03-11T07:20:50Z", "closed_at": "2025-03-11T17:18:23Z", "commit_id": "e0c91608a1c3090e36bb152f101c47be76265bb2", "labels": [], "url": "https://github.com/GLips/Figma-Context-MCP/issues/27", "body": "There are new tools that MCP server is showing - `get_figma_data` , `download_figma_images` .\n\nThe TOOLs that README.md is showing - `get_node` and `get_file`.\n\nIf new TOOLs are different from the older ones then please write about it, for the contextual awareness.", "comments_url": "https://api.github.com/repos/GLips/Figma-Context-MCP/issues/27/comments", "author": "sujayxaradhya", "comments": [{"user": "GLips", "created_at": "2025-03-11T17:18:23Z", "body": "Whoops. Thought I updated that previously. In fact I had meant to remove that section from the README entirely as I didn't think it's super useful, but if you found it interesting I'll keep it.\n\nJust updated!"}, {"user": "sujayxaradhya", "created_at": "2025-03-11T19:10:39Z", "body": "> Whoops. Thought I updated that previously. In fact I had meant to remove that section from the README entirely as I didn't think it's super useful, but if you found it interesting I'll keep it.\n> \n> Just updated!\n\nThanks alot 🙏 \nThis would really help everyone 💯"}], "satisfaction_conditions": ["Documentation that accurately reflects the current available tools in the system", "Up-to-date information about tool functionality for contextual awareness", "Maintenance of documentation sections that users find valuable"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:58"}, "dockerfile": null, "language": "typescript"}
{"task_id": "cab_lenient_35", "number": 5, "title": "Missing ConsoleWindowUtility on Unity 2022.3", "created_at": "2025-03-20T09:48:31Z", "closed_at": "2025-03-24T06:53:20Z", "commit_id": "7d2a2dab10bf34ea671ef569842924d3ed842a39", "labels": ["bug"], "url": "https://github.com/CoderGamester/mcp-unity/issues/5", "body": "```\nLibrary/PackageCache/com.gamelovers.mcp-unity@7d2a2dab10/Editor/Resources/GetConsoleLogsResource.cs(96,13): error CS0103: The name 'ConsoleWindowUtility' does not exist in the current context\n```", "comments_url": "https://api.github.com/repos/CoderGamester/mcp-unity/issues/5/comments", "author": "trungdlp-wolffun", "comments": [{"user": "CoderGamester", "created_at": "2025-03-21T21:50:39Z", "body": "Good report\nlooking now into that"}, {"user": "CoderGamester", "created_at": "2025-03-22T14:10:19Z", "body": "@trungdlp-wolffun apologies for the delay\n\nI pushed a fix for the issue.\nLet me know if you still have problems"}, {"user": "trungdlp-wolffun", "created_at": "2025-03-24T06:53:20Z", "body": "It works well, thanks a lot @CoderGamester "}], "satisfaction_conditions": ["A fix for the missing ConsoleWindowUtility error in Unity 2022.3", "Compatibility with the user's Unity 2022.3 environment", "Resolution that allows the package to compile without errors"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:03"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_36", "number": 87, "title": "Help information on Pricing & Charging Plan Screen cannot be displayed on touch screen device", "created_at": "2025-02-01T15:30:42Z", "closed_at": "2025-02-04T08:35:20Z", "commit_id": "b2758915724489d3b38bbda3ecad2a5257788155", "labels": [], "url": "https://github.com/Webreaper/SolisAgileManager/issues/87", "body": "If you hover the mouse cursor over any of the labels at the top of the screen, extra information is displayed e.g. \"Current load being consumed by the house\".\n\nThis doesn't happen on a touch screen device, and currently there is no way to see this info. On the Config Screen there are specific Help Icons that you can click to get more information.  Is this a possibility on this screen?\n\n", "comments_url": "https://api.github.com/repos/Webreaper/SolisAgileManager/issues/87/comments", "author": "dqj999", "comments": [{"user": "Webreaper", "created_at": "2025-02-01T15:36:15Z", "body": "Tool tips don't generally work on touch screen device. Hopefully people will learn what the icons mean so won't need them on a phone.\n\nOne thought I had is to duplicate the colours and icons of the Soliscloud app so people will be familiar. "}, {"user": "Webreaper", "created_at": "2025-02-01T15:38:43Z", "body": "One thing I could do is make them tappable or clickable, and display a popup with the description, bit like the `?` icons in the config screen. "}, {"user": "dqj999", "created_at": "2025-02-01T15:43:43Z", "body": "Making them clickable would work :-)\n\nThe pair I find confusing are the \"Total Solar PV Generation Today\" and the \"Current Solar PV Generation\".  Maybe putting a Sigma in front of it would help the mathematically oriented amongst us!"}, {"user": "Webreaper", "created_at": "2025-02-01T15:52:10Z", "body": "Doesn't the inclusion of the units make it absolutely clear? "}, {"user": "dqj999", "created_at": "2025-02-01T17:06:51Z", "body": "Yes it does if I think about it! \n\nDoh!"}, {"user": "Webreaper", "created_at": "2025-02-04T08:35:20Z", "body": "Fixed in most recent release. "}], "satisfaction_conditions": ["A way to access help information on touch screen devices", "Clear visual distinction between current values and cumulative totals"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:06"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_37", "number": 127, "title": "How do I return file in current implementation?", "created_at": "2025-03-28T06:55:56Z", "closed_at": "2025-03-28T12:13:30Z", "commit_id": "9330774795e0544940e6ad25721da7732b52fd73", "labels": [], "url": "https://github.com/modelcontextprotocol/csharp-sdk/issues/127", "body": "Hello! Just checked the docs and tests and did not find any sample on how I can return file as tool answer, for example. Could anyone shed some light on it?", "comments_url": "https://api.github.com/repos/modelcontextprotocol/csharp-sdk/issues/127/comments", "author": "vshapenko", "comments": [{"user": "stephentoub", "created_at": "2025-03-28T12:11:01Z", "body": "There are a variety of ways, but probably the easiest is to just return a `Microsoft.Extensions.AI.DataContent`, e.g.\n```C#\n[McpServerTool]\npublic static DataContent GetMyImage()\n{\n    byte[] bytes = File.ReadAllBytes(\"path/to/my/image.png\");\n    return new DataContent(bytes, \"image/png\");\n}\n```"}, {"user": "vshapenko", "created_at": "2025-03-28T12:12:32Z", "body": "Thanks a lot, will try"}], "satisfaction_conditions": ["A code example showing how to return a file as a tool answer", "Information about the appropriate class or method to use for file returns", "A complete, executable code snippet that demonstrates the file return process"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:32"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_38", "number": 9, "title": "Shell integration", "created_at": "2024-12-06T13:21:58Z", "closed_at": "2025-02-16T03:16:43Z", "commit_id": "b9a3e7167453b8ca04071f079cd03a6f56cffabf", "labels": ["bug"], "url": "https://github.com/hfiref0x/WinDepends/issues/9", "body": "RemoveAssoc is never called - easy fix.\r\n\r\nSetAssoc, use a quoted path, for when people have spaces in their file path:\r\n\r\n```\r\ntry\r\n        {\r\n            using (var regKey = Registry.ClassesRoot.CreateSubKey(extKeyName, true))\r\n            {\r\n                if (regKey != null)\r\n                {\r\n                    // Set command value.\r\n                    using (var subKey = regKey.CreateSubKey(\"command\"))\r\n                    {\r\n                        subKey?.SetValue(\"\", $\"\\\"{Application.ExecutablePath}\\\" \\\"%1\\\"\", RegistryValueKind.String);\r\n                    }\r\n\r\n                    // Set icon value.\r\n                    regKey.SetValue(\"Icon\", $\"{Application.ExecutablePath}, 0\", RegistryValueKind.String);\r\n                }\r\n            }\r\n        }\r\n\r\n\r\n```", "comments_url": "https://api.github.com/repos/hfiref0x/WinDepends/issues/9/comments", "author": "AlexW-13", "comments": [{"user": "i486", "created_at": "2025-02-15T00:48:22Z", "body": "@hfiref0x \nIt seems like you forgot to include quotes around the `%1` placeholder. The latest snapshot build is still creating `\"C:\\WinDepends\\bin\\WinDepends.exe\" %1` for the context menu, which doesn't work for files with spaces in their paths.\n\nBTW: Working great on Windows 7. Thanks for this amazing tool."}, {"user": "hfiref0x", "created_at": "2025-02-15T03:40:32Z", "body": "@i486 \nYou are right, thanks. This should be fixed now in the above mentioned commit."}], "satisfaction_conditions": ["Proper handling of file paths with spaces in the shell integration", "Compatibility with Windows 7"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:31"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_39", "number": 24, "title": "Add SortAttribute, .gitignore and update documentation 🔨", "created_at": "2025-03-20T02:09:52Z", "closed_at": "2025-03-28T11:56:35Z", "commit_id": "2de3e83635f89f4da2f486114a4622feef6121ba", "labels": [], "url": "https://github.com/AbZorbaGames/artificetoolkit/pull/24", "body": "Hey @ZackPer,  \r\n\r\n**What's new:**  \r\n- Added `SortAttribute` to order Inspector fields   \r\n- Updated docs with sword example ⚔️  \r\n- Added `.gitignore` for IDE folders 🚫  \r\n\r\n**Quick question:**  \r\n\r\nIn `ArtificeInspector`, this line (12), gets uncommented by Unity when I clone the repo 😅. \r\n```csharp  \r\n// [CustomEditor(typeof(Object), true), CanEditMultipleObjects]  \r\n```\r\n\r\nMeant to be a `///` comment?\r\n", "comments_url": "https://api.github.com/repos/AbZorbaGames/artificetoolkit/issues/24/comments", "author": "exejutable", "comments": [{"user": "ZackPer", "created_at": "2025-03-20T14:10:23Z", "body": "Regarding this line,\r\n\r\n```c#\r\n// [CustomEditor(typeof(Object), true), CanEditMultipleObjects]  \r\n```\r\n\r\nThe intended use for Artifice is to add it to your project through git. That means the source code is inside unity's the Library folder which is always under gitignore. \r\n\r\nSo we tried to utilize this to be able to turn on and off the ArtificeInspector completely, by removing the attribute which applies it to the inspector. If someone wants to download and add it manually, he should gitignore this file specifically to be able to turn on/off the inspector.\r\n\r\nSo when working on artifice, the developer should completely ignore this script 🐵 \r\n\r\nThis is not documented, so I will put a task for it to update it."}, {"user": "ZackPer", "created_at": "2025-03-20T14:11:41Z", "body": "Also great job @exejutable ! Your PR was well structured and documented. After resolving the truly minor comments I added, we can pull and merge!"}, {"user": "exejutable", "created_at": "2025-03-20T21:18:07Z", "body": "Hi @ZackPer ,\r\n\r\nThanks for the explanation! \r\n\r\n**About the `//` behavior:**\r\nWhen adding Artifice as a local package (from disk), Unity removes them automatically. Switching to `///` fixes this issue, as Unity preserves triple-slash comments.\r\n\r\n**About the \"minor comments\":**\r\nYou mentioned resolving \"truly minor comments,\" but I don’t see them in the PR. Did you mean:\r\n\r\n1. You’ll handle them?\r\n2. Or should I address them?\r\n\r\nLet me know so I can help out! "}, {"user": "ZackPer", "created_at": "2025-03-21T08:52:47Z", "body": "Good morning @exejutable \r\n\r\nI see what you mean know with the `//`. It feels weird that I havent really noticed it by now but I confirmed it now haha. I will add it on a later patch so dont bother with it for now. \r\n\r\nIt seems I never submitted my review, I apologize. New to the Github UI, I have mostly used GitLab until now. You should be able to see them now.\r\n\r\nOne last comment, you should also progress the package.json version to 1.3.2\r\n\r\nThanks again!"}, {"user": "exejutable", "created_at": "2025-03-24T20:48:26Z", "body": "Hi @ZackPer I removed the `.gitignore` added the `///` to the `ArtificeInspector` also updated the package to `1.3.2`"}, {"user": "ZackPer", "created_at": "2025-03-28T11:39:19Z", "body": "Hello @exejutable !\r\n\r\nSorry for the delay but I was on my day-offs 🫣\r\n\r\nSo I made some changes after reviewing the branch.\r\n\r\nFirst of all, I reverted the '//' instead of the '///' because it does not have to do with Unity. Probably, the Artifice_Utilities class was simply removing the '//' because it detected that previously you had enabled the ArtificeToolkit. So long story sort, the Artifice_Utilities which also offers the MenuItem options, enforces the '//' based on whether you want to use Artifice or not. This way, your preference is kept even when you update the ArtificeToolkit version.\r\n\r\nSecondly, I made a small optimization to the sorting utility method. I added a boolean to skip the OrderBy call if no sorting is needed, so we keep the O(n) time complexity for all cases that we dont have any sorting. I know the OrderBy has the best case of O(n) eitherway, but it feels better to enforce it so we stay agnostic of the sorting algorithm.\r\n\r\nLastly, I changed the default sorting order value (if no sort attribute was used at a specific property, but we need sorting because of another attribute), to be 0. This way, if you want to make a property which appears first in your script, appear last in the inspector, you dont need to put [Sort] to every other property. This is also the way Odin does it!\r\n\r\nWith this changes, I will probably squash and merge soon. Let me know your thoughts when you see this.\r\n"}, {"user": "exejutable", "created_at": "2025-04-08T23:57:01Z", "body": "Hi @ZackPer ,\r\n\r\nNo worries at all about the delay—hope you had a great time on your day off!\r\n\r\nThanks for the detailed explanation and the changes. Everything looks good to me! The only thing that caught my eye was the shift from explicit typing to var, but that’s just me being a typing psychopath lol."}, {"user": "ZackPer", "created_at": "2025-04-09T06:22:26Z", "body": "Hello @exejutable \r\n\r\nThe whole project uses 'var' instead of explicitly defining types. Probably there are some cases where this is impossible if no value can be assigned to it yet, until some logic actually does it. \r\n\r\nBeing a typing freak is a great quality for a programmer :) "}], "satisfaction_conditions": ["Clear explanation of the comment behavior in ArtificeInspector", "Guidance on how to properly handle the PR review process", "Information about version numbering requirements for the package", "Feedback on their code contribution quality", "Technical rationale for maintainer's code changes"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:36"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_40", "number": 52, "title": "chat commands not working", "created_at": "2025-01-26T23:31:33Z", "closed_at": "2025-01-27T00:42:27Z", "commit_id": "2b562dbab6531ef4ea3f5a6285783d9428879550", "labels": [], "url": "https://github.com/DrMeepso/WebFishingCove/issues/52", "body": "Have an issue where none of the built in commands like spawn, kick, ban, users work at all. it only says the command isnt found even though the command file has them in it. any fix?", "comments_url": "https://api.github.com/repos/DrMeepso/WebFishingCove/issues/52/comments", "author": "JBork1", "comments": [{"user": "Ech0klang", "created_at": "2025-01-26T23:32:00Z", "body": "Enable plugins in the server config"}, {"user": "JBork1", "created_at": "2025-01-26T23:32:44Z", "body": "Ah, thank you. was having an issue banning someone earlier but now i can deal with them."}], "satisfaction_conditions": ["A solution that enables the built-in chat commands to function", "A simple configuration adjustment that doesn't require complex troubleshooting", "Information about where in the server configuration the relevant setting needs to be changed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:11"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_41", "number": 138, "title": "Running in Docker on macOS", "created_at": "2025-03-10T21:54:51Z", "closed_at": "2025-03-11T09:59:41Z", "commit_id": "4a5edcb97079a59e1e8d8c66a54d790ff40fca36", "labels": [], "url": "https://github.com/Webreaper/SolisAgileManager/issues/138", "body": "Has anyone got this running successfully in Docker on macOS?\nI have followed the guidance provided in the release and the docker-compose file, but am seeing these errors when I try to run the container:\n\n```\nsolismanager exited with code 255\nsolismanager  | exec /app/SolisManager: no such file or directory\n```\nAny ideas?  Thanks", "comments_url": "https://api.github.com/repos/Webreaper/SolisAgileManager/issues/138/comments", "author": "0rangutan", "comments": [{"user": "Webreaper", "created_at": "2025-03-10T22:02:46Z", "body": "Have a look at the last 3 comments in #124. The alpha tag, with the environment variable in the docker-compose snippet, should work. "}, {"user": "Webreaper", "created_at": "2025-03-11T09:59:41Z", "body": "Closing this - the latest image should work (you won't need the env var). Please let me know if it does/doesn't."}, {"user": "0rangutan", "created_at": "2025-03-11T10:01:24Z", "body": "Thanks - I have the app running on macOS now!\nWorks with the Alpha and currently with 1.0.522 and the Env variable.\nI'll try it without now..."}, {"user": "0rangutan", "created_at": "2025-03-11T10:05:01Z", "body": "Yes, works without the environment variable, thanks!"}], "satisfaction_conditions": ["A working Docker configuration for running SolisManager on macOS", "Clear instructions on which Docker image version to use", "Information about environment variable requirements", "A solution that resolves the 'no such file or directory' error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:57"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_42", "number": 11, "title": "README Suggestion: Unity *needs to be* in focus or tools may time out", "created_at": "2025-04-01T18:17:03Z", "closed_at": "2025-04-02T23:35:18Z", "commit_id": "3acfb232f564ae2ef10282469c22359be035961d", "labels": ["bug"], "url": "https://github.com/CoderGamester/mcp-unity/issues/11", "body": "According to my testing (Mac / Apple Silicon), essentially all of the tools rely on the Unity window being in focus to execute, potentially because its main thread heavily throttles function calls if the application is not in focus. In other words, you might see the tool requests time out UNLESS you switch back to Unity to let them execute.\n\nMarking \"Run in Background\" seems to only affect builds, as far as I can tell, and doesn't help.  \n\nThere may be a way around this, but for now, everyone using this should know this limitation.", "comments_url": "https://api.github.com/repos/CoderGamester/mcp-unity/issues/11/comments", "author": "dsarno", "comments": [{"user": "alexander-andrianov", "created_at": "2025-04-02T08:24:54Z", "body": "+1, can confirm this behavior too. Based on how Unity handles thread prioritization, it’s likely some intentional optimization to throttle background processes (at least on Apple Silicon)\n@dsarno did you test it on windows / intel?"}, {"user": "dsarno", "created_at": "2025-04-02T13:36:04Z", "body": "Alexander I didn’t test on PC but would definitely be interested if it made\r\na difference.\r\n"}, {"user": "CoderGamester", "created_at": "2025-04-02T20:00:33Z", "body": "This was a problem indeed. Thank you for reporting\nI pushed a new fix today @dsarno @alexander-andrianov \n\nIt is a messy issue with Unity only allowing to run Editor code on the mainThread, but thankfully it was solvable \n\nCan you try again?\nShould be fine by now"}, {"user": "dsarno", "created_at": "2025-04-02T23:32:23Z", "body": "This was fixed for me @CoderGamester !  Well done!"}], "satisfaction_conditions": ["A solution that allows tools to execute properly without requiring Unity to be in focus", "A fix that addresses the thread prioritization issue on Apple Silicon Macs", "A solution that prevents tool request timeouts"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:59"}, "dockerfile": null, "language": "c#"}
{"task_id": "cab_lenient_43", "number": 89, "title": "uni-meter does not work when at startup the mqtt input device was not available", "created_at": "2025-03-31T17:16:26Z", "closed_at": "2025-04-01T02:29:33Z", "commit_id": "62e71362344f2b2ea29f6084d9e81634d0e9782c", "labels": ["bug"], "url": "https://github.com/sdeigm/uni-meter/issues/89", "body": "uni-meter-1.1.3\n\nHaving in file `/etc/uni-meter.conf` the following MQTT input source, where 192.168.1.4:1883 is the Home Assistant MQTT broker:\n\n```\n  input-devices {\n    mqtt {\n      url = \"tcp://192.168.1.4:1883\"\n      username = \"mqttclient\"\n      password = \"****\"\n\n      power-phase-mode = \"mono-phase\"\n      energy-phase-mode = \"mono-phase\"\n\n      channels = [{\n        type = \"json\"\n        topic = \"tele/tasmota_FA33FC/SENSOR\"\n        channel = \"power-total\"\n        json-path = \"$..power\"\n      },{\n        type = \"json\"\n        topic = \"tele/tasmota_FA33FC/SENSOR\"\n        channel = \"energy-consumption-total\"\n        json-path = \"$..energy_sum\"\n      },{\n        type = \"json\"\n        topic = \"tele/tasmota_FA33FC/SENSOR\"\n        channel = \"energy-production-total\"\n        json-path = \"$..energy_supply\"\n      }]\n    }\n  }\n```\n\nWhen uni-meter systemd unit is started before the Home Assistant MQTT broker is up and running, then uni-meter will not start working at all, even not when the MQTT broker becomes available.\n\nIt would be better if uni-meter would try every e.g. 1 min again, and start working as soon as the input source becomes available.\n\nCurrently, the solution is to restart uni-meter after mqtt broker is running.", "comments_url": "https://api.github.com/repos/sdeigm/uni-meter/issues/89/comments", "author": "Gitsarry", "comments": [{"user": "sdeigm", "created_at": "2025-04-01T02:26:10Z", "body": "Can confirm the problem. In theory uni-meter is already designed to always retry failed operations. Here I didn't reinitialized the underlyling MQTT library correctly."}, {"user": "Gitsarry", "created_at": "2025-04-01T03:53:32Z", "body": "Thank you very much for your work "}, {"user": "Gitsarry", "created_at": "2025-04-03T04:22:46Z", "body": "Have just tested with 1.1.4 and can confirm issue is fixed:\n\n- stop uni-meter\n- stop Home Assistant and with it the HA MQTT broker, which is input-source of uni-meter\n- start uni-meter\n- wait until log entry `MQTT stream failed: MqttException` occurs three times in a row\n- start Home Assistant\n- log entry `MQTT stream connected` shows up and uni-meter is working\n\n\n```\n25-04-03 06:10:58.007 INFO  uni-meter                - ##################################################################\n25-04-03 06:10:58.012 INFO  uni-meter                - # Universal electric meter converter 1.1.4 (2025-04-01 05:12:23) #\n25-04-03 06:10:58.012 INFO  uni-meter                - ##################################################################\n25-04-03 06:10:58.012 INFO  uni-meter                - initializing actor system\n25-04-03 06:10:58.781 INFO  org.apache.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started\n25-04-03 06:10:59.252 INFO  uni-meter.controller     - creating ShellyPro3EM output device\n25-04-03 06:10:59.277 INFO  uni-meter.controller     - creating MQTT input device\n25-04-03 06:10:59.598 INFO  uni-meter.input          - subscribing to topic: tele/tasmota_FA33FC/SENSOR\n25-04-03 06:10:59.885 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:11:00.474 INFO  uni-meter.http.port-80   - HTTP server is listening on /[0:0:0:0:0:0:0:0]:80\n25-04-03 06:11:17.980 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:11:50.041 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:12:52.478 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:13:52.871 INFO  uni-meter.input          - MQTT stream connected\n\n```"}], "satisfaction_conditions": ["A mechanism for uni-meter to automatically reconnect to MQTT broker when it becomes available", "Resilience to input device unavailability at startup", "Appropriate error handling with retry capability", "No manual intervention required after input source becomes available"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:47"}, "dockerfile": null, "language": "java"}
{"task_id": "cab_lenient_44", "number": 465, "title": "Deep copy a VectorSchemaRoot?", "created_at": "2024-12-24T12:29:02Z", "closed_at": "2024-12-26T05:46:13Z", "commit_id": "3ef5450919dd3ebc0b566d1556c33c1207a10514", "labels": ["Type: enhancement"], "url": "https://github.com/apache/arrow-java/issues/465", "body": "### Describe the enhancement requested\r\n\r\nI'm writing a convertor method to convert a base64 encoded byte array into Arrow batches and returns it to the user.\r\n\r\n```java\r\npublic List<VectorSchemaRoot> readArrowBatches(String rows, BufferAllocator allocator) {\r\n    final List<VectorSchemaRoot> batches = new ArrayList<>();\r\n    final byte[] data = Base64.getDecoder().decode(rows);\r\n    final ByteArrayInputStream stream = new ByteArrayInputStream(data);\r\n    try (final ArrowStreamReader reader = new ArrowStreamReader(stream, allocator)) {\r\n        while (reader.loadNextBatch()) {\r\n            batches.add(new Table(reader.getVectorSchemaRoot()).toVectorSchemaRoot());\r\n        }\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    }\r\n    return batches;\r\n}\r\n```\r\n\r\nSince `ArrowStreamReader` replace the batch referred by `getVectorSchemaRoot` in each iteration, I have to do a deepcopy of VectorSchemaRoot every time.\r\n\r\nCurrently, I use Table's method as a workaround, but wonder if `VectorSchemaRoot` deserves a `copy` method, or I implement such a typically use case in a wrong way.", "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/465/comments", "author": "tisonkun", "comments": [{"user": "lidavidm", "created_at": "2024-12-26T03:59:54Z", "body": "You should use VectorLoader/VectorUnloader to \"move\" the contents of the reader's root into your own"}, {"user": "tisonkun", "created_at": "2024-12-26T05:38:54Z", "body": "That seems exactly what the inner of `Table` does. Do we have some util or a `copy` method for that. Or I just wrap by myself .. It seems quite a common usage and I don't want to hook outside of arrow-java.\r\n\r\n```java\r\nwhile (reader.loadNextBatch()) {\r\n    final VectorSchemaRoot source = reader.getVectorSchemaRoot();\r\n    final VectorUnloader unloader = new VectorUnloader(source);\r\n    final VectorSchemaRoot copy = VectorSchemaRoot.create(source.getSchema(), allocator);\r\n    final VectorLoader loader = new VectorLoader(copy);\r\n    loader.load(unloader.getRecordBatch());\r\n    batches.add(copy);\r\n}\r\n```"}, {"user": "lidavidm", "created_at": "2024-12-26T05:43:07Z", "body": "That is the intended usage. What is the problem?\r\n\r\n(Note that you can also just keep an array of the batches from the unloader, and load/stream them through a root as necessary.)"}, {"user": "tisonkun", "created_at": "2024-12-26T05:46:09Z", "body": "OK thanks. Yes it seems a list of ArrowRecordBatch owns the buffer and doesn't need to tune with the lifecycle of allocator."}, {"user": "tisonkun", "created_at": "2024-12-26T05:56:55Z", "body": "Emmm .. No. The ArrowRecordBatch's buffer is still bound to the allocator, and it doesn't have the schema info where we need to store elsewhere."}, {"user": "lidavidm", "created_at": "2024-12-26T06:00:19Z", "body": "Yes, there isn't really any way of untying things from an allocator (this is intentional). There are APIs to transfer memory between allocators (or you can just keep a single allocator across different contexts)."}, {"user": "tisonkun", "created_at": "2024-12-26T06:02:21Z", "body": "@lidavidm Thanks for your information! Is there some docs/cookbook for copy VectorSchemaRoot? It seems challenging to ensure the lifetime of both data and allocator are aligned and I suppose some demo code would help a lot."}, {"user": "tisonkun", "created_at": "2024-12-26T06:03:25Z", "body": "For example, when I wrote:\r\n\r\n```java\r\n        while (reader.loadNextBatch()) {\r\n            final VectorSchemaRoot source = reader.getVectorSchemaRoot();\r\n            final VectorSchemaRoot copy = VectorSchemaRoot.create(source.getSchema(), allocator);\r\n            new VectorLoader(copy).load(new VectorUnloader(source).getRecordBatch());\r\n            batches.add(copy);\r\n        }\r\n```\r\n\r\nIt seems the intermediate ArrowRecordBatch should be closed but it's very easy to get it wrong and receive a runtime exception .."}, {"user": "lidavidm", "created_at": "2024-12-26T06:16:48Z", "body": "Unfortunately not. You should do something like\r\n\r\n```java\r\ntry (var batch = unloader.getRecordBatch()) {\r\n  loader.load(batch);\r\n}\r\n```"}], "satisfaction_conditions": ["Clear guidance on the proper way to copy a VectorSchemaRoot", "Information about memory management and allocator lifecycle", "Code examples showing proper resource handling", "Documentation or cookbook references for common Arrow operations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:03"}, "dockerfile": null, "language": "java"}
{"task_id": "cab_lenient_45", "number": 16, "title": "HeliBoard integration", "created_at": "2025-01-13T09:55:42Z", "closed_at": "2025-01-13T13:41:00Z", "commit_id": "6c52c544e250a140f4297cf45e43c150ff8063ca", "labels": [], "url": "https://github.com/woheller69/whisperIME/issues/16", "body": "Just a quick question, how do I get HeliBoard to call Whisper instead of the Google voice assistant when pressing the mic button in the toolbar?", "comments_url": "https://api.github.com/repos/woheller69/whisperIME/issues/16/comments", "author": "C-O-D", "comments": [{"user": "woheller69", "created_at": "2025-01-13T10:13:22Z", "body": "Just switch off Google voice input method in Android settings"}, {"user": "C-O-D", "created_at": "2025-01-13T12:22:11Z", "body": "Okay, thanks..."}], "satisfaction_conditions": ["A simple method to configure HeliBoard to use Whisper instead of Google voice assistant", "Instructions that don't require technical expertise or complex setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:56"}, "dockerfile": null, "language": "java"}
{"task_id": "cab_lenient_46", "number": 13, "title": "[SOLVED/CLOSED] High scores not working", "created_at": "2025-02-21T19:48:03Z", "closed_at": "2025-02-23T20:07:22Z", "commit_id": "36234251a3887e988199b8cfd98ba3a48b41b525", "labels": [], "url": "https://github.com/hathibelagal-dev/Eidetic-Memory-Trainer/issues/13", "body": "Specs\nMoto G9 Power\nLineageOS 22.1\nAndroid 15\n\nInfo:\nI take 2 or 3 seconds to complete the test but my highscore shows 82, 12 or 7 seconds. If needed i can upload a video playing with w/ a timer on screen.", "comments_url": "https://api.github.com/repos/hathibelagal-dev/Eidetic-Memory-Trainer/issues/13/comments", "author": "artur15lima", "comments": [{"user": "hathibelagal-dev", "created_at": "2025-02-23T12:46:07Z", "body": "Hi, the timer starts the moment the numbers become visible on the screen, not after you press 1. Please confirm that this is how you're timing yourself too."}, {"user": "artur15lima", "created_at": "2025-02-23T20:07:06Z", "body": "Ok, now i understand it. Tysm."}], "satisfaction_conditions": ["Clarification about when the timer starts in the game", "Explanation for the discrepancy between perceived completion time and recorded high scores"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:11"}, "dockerfile": null, "language": "java"}
{"task_id": "cab_lenient_47", "number": 194, "title": "Chat feature not working with Custom AI agents", "created_at": "2025-01-20T18:51:42Z", "closed_at": "2025-01-20T19:30:20Z", "commit_id": "808d3a373a7b889be959fc29c2f14368c80eb051", "labels": ["bug"], "url": "https://github.com/clusterzx/paperless-ai/issues/194", "body": "**Describe the bug**\nChat not working with \"Custom\" agents.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Configure a Custom provider, in my case local llama.cpp.\n2. Process a document.\n3. Open a chat for it, from any access.\n\n**Expected behavior**\nChat to start using the custom provider.\n\n\n**Desktop (please complete the following information):**\n - OS: Any\n - Browser: Any\n - Version: 2.30\n\n**Additional context**\nI can see in the logs that the chat feature seems to expect OpenAI:\npaperless-ai           | [ERRO] initializing chat: Error: OpenAI client not initialized\npaperless-ai           |     at ChatService.initializeChat (/app/services/chatService.js:64:15)\n", "comments_url": "https://api.github.com/repos/clusterzx/paperless-ai/issues/194/comments", "author": "chwoa", "comments": [{"user": "clusterzx", "created_at": "2025-01-20T19:25:16Z", "body": "You are right! Forgot to implement it there. Pushing an update today!\nThank you very much for reporting."}, {"user": "clusterzx", "created_at": "2025-01-20T19:30:20Z", "body": "Fixed :)"}, {"user": "chwoa", "created_at": "2025-01-20T19:57:23Z", "body": "That was quick! Confirmed it is working in 2.3.1. Thank you very much!"}, {"user": "clusterzx", "created_at": "2025-01-20T20:04:05Z", "body": "You are very welcome 👍 "}], "satisfaction_conditions": ["Enable chat functionality to work with Custom AI agents/providers", "Remove dependency on OpenAI for chat functionality", "Provide a timely fix for the reported issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:46"}, "dockerfile": null, "language": "javascript"}
{"task_id": "cab_lenient_48", "number": 36, "title": "No more scrolling functionality", "created_at": "2025-03-10T08:40:16Z", "closed_at": "2025-03-11T18:58:12Z", "commit_id": "72a82707ea4d9192d2fe63d53ac893e1a7e0797a", "labels": [], "url": "https://github.com/willmiao/ComfyUI-Lora-Manager/issues/36", "body": "### **LoRA Manager Version**\n- Version: latest\n\n### **Environment Information**\n- **Operating System**:Windows 11, running ComfyUI in Conda Environment\n- **Browser & Version**: Edge 134\n\n### **Issue Description**\n- The manager only shows 16 Loras, where as I used to be able to scroll and see all loras.\n- I can still search and the other loras will come up, but no more scroll function.\n", "comments_url": "https://api.github.com/repos/willmiao/ComfyUI-Lora-Manager/issues/36/comments", "author": "fredericklessing", "comments": [{"user": "willmiao", "created_at": "2025-03-11T14:47:37Z", "body": "Hi, this issue was likely introduced by a previous change that was meant to fix a layout issue reported by another user. However, since it didn’t work as intended, I’ve reverted the commit in the latest release (v0.7.36).\n\nPlease try updating to see if the problem is resolved. Let me know if it works!"}, {"user": "fredericklessing", "created_at": "2025-03-11T18:58:12Z", "body": "Thank you so much, it is working again. Much appreciated."}], "satisfaction_conditions": ["Restoration of the scrolling functionality in the LoRA Manager", "Access to the complete collection of LoRAs beyond the initial 16 displayed", "A timely fix that doesn't require complex user intervention"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:40"}, "dockerfile": null, "language": "javascript"}
{"task_id": "cab_lenient_49", "number": 99, "title": "Avoid including internal libintelmath header.", "created_at": "2025-03-07T16:20:32Z", "closed_at": "2025-03-12T19:02:01Z", "commit_id": "f237956efbf871176b9cd6b6b85f694c2f7fed4d", "labels": [], "url": "https://github.com/microsoft/documentdb/pull/99", "body": "This removed the include of <bid_internal.h> and adds the three MASK64 definitions that are used in the code.\r\n\r\nFixes: #97", "comments_url": "https://api.github.com/repos/microsoft/documentdb/issues/99/comments", "author": "mbanck-cd", "comments": [{"user": "lichoil", "created_at": "2025-03-07T17:55:19Z", "body": "hi @mbanck-ntap ,may I know the reason why removing this header file out and put MASK64 definitions in stead?"}, {"user": "safern", "created_at": "2025-03-07T19:56:07Z", "body": "@lichoil I think it is to be able to compile in debian. \r\n\r\n@diipak-bisht thoughts on this change? "}, {"user": "mbanck", "created_at": "2025-03-08T08:09:31Z", "body": "> @lichoil I think it is to be able to compile in debian.\r\n\r\nExactly.\r\n\r\n"}, {"user": "diipak-bisht", "created_at": "2025-03-10T05:14:47Z", "body": "I think this is fine change if internal headers are not available in debian unstable @safern. Thanks for contributing this @mbanck "}], "satisfaction_conditions": ["A solution that enables compilation in Debian without requiring internal headers", "A way to maintain the necessary functionality while removing dependency on bid_internal.h", "An approach that addresses the issue (#97) related to internal header dependencies"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:16"}, "dockerfile": null, "language": "c"}
{"task_id": "cab_lenient_50", "number": 3, "title": "error compiling sim.c", "created_at": "2025-04-02T21:49:57Z", "closed_at": "2025-04-03T13:41:01Z", "commit_id": "33e86b34aeb83908940d893f13d3b1590ce311a7", "labels": [], "url": "https://github.com/AncientJames/multivox/issues/3", "body": "Hello James,\nthis is an amazing project you have made here.\nI have issues compiling the code on my raspberry pi 5 unsucessful with the following error:\n/home/rpi5/multivox/src/simulator/sim.c:14:10: fatal error: **GLES3/gl3.h**: No such file or directory.\nI will be grateful for any advice on how to install the open-gl libraries or skip compiling your simulator.", "comments_url": "https://api.github.com/repos/AncientJames/multivox/issues/3/comments", "author": "IljaRukin", "comments": [{"user": "AncientJames", "created_at": "2025-04-02T23:01:55Z", "body": "From  memory, the gl libraries can be installed via `sudo apt install libgles-dev libegl-dev`\n\nFailing that, the simulator is the last section in the `CMakeLists.txt` - comment out everything from `set(SIMULATOR_SRC_DIR ${SRC_DIR}/simulator)`\n\n"}, {"user": "IljaRukin", "created_at": "2025-04-03T13:41:01Z", "body": "Thank you for your fast response !\nInstalling the librarier you mentioned solved the issue."}], "satisfaction_conditions": ["Instructions for installing the missing OpenGL libraries on Raspberry Pi", "A workaround to bypass simulator compilation if library installation isn't possible", "A prompt response that addresses the specific compilation error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:07"}, "dockerfile": null, "language": "c"}
{"task_id": "cab_lenient_51", "number": 33, "title": "I can't compile C++ files", "created_at": "2025-02-24T11:11:20Z", "closed_at": "2025-03-02T23:11:46Z", "commit_id": "f98582b854c17de20bf0f2a2a68e8c5571c50108", "labels": [], "url": "https://github.com/OpenSiFli/SiFli-SDK/issues/33", "body": "Hi)\n\nMy project requires C++. I tried to add a C++ test file to the test project, but it didn't build:\n```\nCXX build_eh-lb523_hcpu\\src\\test_cpp_file.o\nError in calling command:g++\nException: No such file or directory\n\nPlease check Toolchains PATH setting.\n\nscons: *** [build_eh-lb523_hcpu\\src\\test_cpp_file.o] Error 2\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\clock.o\nscons: building terminated because of errors.\n```\nI added the following line to SConstruct:\n```\nCXX = rtconfig.CXX, CXXFLAGS = rtconfig.CXXFLAGS,\n```\nThe conclusion was as follows:\n```\nC:/proj/sifli_sdk/example/get-started/blink/rtt/src/test_cpp_file.cpp:1:1: error: unknown type name 'namespace'\nnamespace test_namespace {\n^\nC:/proj/sifli_sdk/example/get-started/blink/rtt/src/test_cpp_file.cpp:1:25: error: expected ';' after top level declarator\nnamespace test_namespace {\n                        ^\n                        ;\n2 errors generated.\n```\nWhat do I need to do to get the files to start building? Here's the content of my test file:\n```\nnamespace test_namespace {\n\nclass TestClass {\npublic:\n        TestClass() = default;\n};\n\n};\n\n```", "comments_url": "https://api.github.com/repos/OpenSiFli/SiFli-SDK/issues/33/comments", "author": "Vadimatorik", "comments": [{"user": "HalfSweet", "created_at": "2025-02-24T13:04:26Z", "body": "This looks like scons complaining about an error in the argument to g++.\nCan you provide more logs or a complete project for us to analyze?"}, {"user": "Vadimatorik", "created_at": "2025-02-24T14:09:25Z", "body": "You can use the example for tests: \"\\example\\get-started\\hello_world\\rtt\". Here is an example of a full build log:\n```\nVadim@VPC C:\\proj\\sifli_sdk\\example\\get-started\\hello_world\\rtt\\project\n> scons --board=eh-lb523 -j12\nscons: Reading SConscript files ...\nBoard: eh-lb523_hcpu\n========\nMulti-Project Info\n--------\nfull_name       main.bootloader\nparent          main\nbsp_root        C:\\proj\\sifli_sdk\\example\\boot_loader\\project\\butterflmicro\\ram_v2\nbuild_dir       build_eh-lb523_hcpu/bootloader\nlink_script     C:/proj/sifli_sdk/example/boot_loader/project/butterflmicro/ram_v2\\link\nptab            C:/proj/sifli_sdk/customer/boards/eh-lb523\\ptab.json\nembedded:       False\n--------\nfull_name       main\nparent\nbsp_root        C:\\proj\\sifli_sdk\\example\\get-started\\hello_world\\rtt\\project\nbuild_dir       build_eh-lb523_hcpu/\nlink_script     C:/proj/sifli_sdk/drivers/cmsis/sf32lb52x/Templates/arm/HCPU/link\nptab            C:/proj/sifli_sdk/customer/boards/eh-lb523\\ptab.json\n--------\nfull_name       main.ftab\nparent          main\nbsp_root        C:\\proj\\sifli_sdk\\example\\flash_table\\sf32lb52x_common_v2\nbuild_dir       build_eh-lb523_hcpu/ftab\nlink_script     C:/proj/sifli_sdk/example/flash_table/sf32lb52x_common_v2\\link\nptab            C:/proj/sifli_sdk/customer/boards/eh-lb523\\ptab.json\nembedded:       False\n========\nscons: done reading SConscript files.\nscons: Building targets ...\nscons: building associated VariantDir targets: build_eh-lb523_hcpu . .\nCC build_eh-lb523_hcpu\\bootloader\\board\\bf0_ap_hal_msp.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\board.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\board_psram.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\boot_flash.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\efuse.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\main.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\sd_emmc_ops.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\sd_nand_drv.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\sd_nand_ops.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\secboot.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\common\\bsp_common.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\common\\flash.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_init.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_lcd_tp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_pinmux.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_power.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bf0_pin_const.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\system_bf0_ap.o\nAS build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\arm\\startup_bf0_hcpu.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_adc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_aes.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_aes_ns.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_audcodec_m.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_audprc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_bleaon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_busmon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_cortex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_crc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_dma.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_dsi.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_efuse.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_epic.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_ext_dma.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_ezip.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_facc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_fft.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_gpio.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_hcd.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_hlp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_hpaon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_i2c.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_i2s.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lcdc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lcpu_config.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lpaon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lpcomp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lptim.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lrc_cal.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mailbox.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_ex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_psram.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_nn_acc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_patch.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pcd.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pdm.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pinmux.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pmu.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_psram.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_ptc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_rcc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_rng.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_rtc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sd_ex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sdadc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sdhci.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sdmmc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_secu.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_spi.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_tim.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_tim_ex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_tsen.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_uart.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_wdt.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_sys_cfg.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\flash_table.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\nand_table.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\sifli_bbm.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\aes.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\cipher.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\cipher_wrap.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\md.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\md_wrap.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\platform.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sha256.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sha512.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\asn1parse.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\bignum.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\oid.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\pk.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\pkparse.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\pk_wrap.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\rsa.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sm2.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sm3.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\ecp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\ecp_curves.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\ctr_drbg.o\nAR build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\BF0_HAL.lib\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\middleware\\bluetooth\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\middleware\\sifli_lib\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\common\\bsp_common.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\common\\flash.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_init.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_lcd_tp.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_pinmux.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_power.o\nLINK build_eh-lb523_hcpu\\bootloader\\bootloader.axf\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\cst816\\cst816.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\cst816\\cst816_update.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\gc9b71\\gc9b71.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\pa\\AW8155\\sifli_aw8155.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bf0_pin_const.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\system_bf0_ap.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\lcpu_patch.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\lcpu_patch_rev_b.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bt_rf_fulcal.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bt_rf_test.o\nAS build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\arm\\startup_bf0_hcpu.o\nProgram Size: Code=47216 RO-data=6044 RW-data=7428 ZI-data=32916\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal.o\nGenerating build_eh-lb523_hcpu\\bootloader\\bootloader.bin ...\nGenerating build_eh-lb523_hcpu\\bootloader\\bootloader.hex ...\nGenerating build_eh-lb523_hcpu\\bootloader\\bootloader.asm ...\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_adc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_aes.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_aes_ns.o\n\n========================================================================\n\n** Object/Image Component Sizes\n\n      Code (inc. data)   RO Data    RW Data    ZI Data      Debug   Object Name\n\n     47216       2740       6048       7432      32912     591304   build_eh-lb523_hcpu\\bootloader\\bootloader.axf (uncompressed)\n     47216       2740       6048       1200      32912     591304   build_eh-lb523_hcpu\\bootloader\\bootloader.axf (compressed)\n         0          0          4          0          0          0   (incl. padding)\n     47216       2740       6048       1200          0          0   ROM Totals for build_eh-lb523_hcpu\\bootloader\\bootloader.axf\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_audcodec_m.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_audprc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_bleaon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_busmon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_cortex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_crc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_dsi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_efuse.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_epic.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_ext_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_ezip.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_facc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_fft.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_gpio.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_hcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_hlp.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_hpaon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_i2c.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_i2s.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lcdc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lcpu_config.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lpaon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lpcomp.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lptim.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lrc_cal.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mailbox.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_ex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_psram.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_nn_acc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_patch.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pdm.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pinmux.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pmu.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_psram.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_ptc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_rcc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_rng.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_rtc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sd_ex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sdadc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sdhci.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sdmmc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_secu.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_spi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_tim.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_tim_ex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_tsen.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_uart.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_wdt.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_sys_cfg.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\flash_table.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\nand_table.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\sifli_bbm.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\middleware\\bluetooth\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\middleware\\sifli_lib\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\middleware\\system\\bf0_common.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\os_adaptor\\src\\os_adaptor_rtthread.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_gpio.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_usart.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_hwtimer.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_pwm.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_pwm_lptim.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_spi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_soft_i2c.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_i2c.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_adc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_rtc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_spi_flash.o\nAR build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\BF0_HAL.lib\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_spi_nand.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_sys_cfg.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_ext_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_audprc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_audcodec_m.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd_private.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_ram_lcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd_test.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd_fb.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_touch.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_epic.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_psram.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_mpi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_aes.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_common.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_dbg.o\nCC build_eh-lb523_hcpu\\src\\main.o\nCXX build_eh-lb523_hcpu\\src\\test.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\clock.o\nC:/proj/sifli_sdk/example/get-started/hello_world/rtt/src/test.cpp:1:CC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\components.o\n1: error: unknown typeC C build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\cpu.o\nname 'namespace'\nnamespace test_namespace {\n^\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\device.o\nC:/proj/sifli_sdk/example/get-started/hello_world/rtt/src/test.cpp:1:25:C C build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\idle.o\nerror: expected ';' after top level declarator\nCnamespace test_namespace {C build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\ipc.o\n\n                        ^\n                        ;\n2 errors generated.\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\irq.o\nscons: *** [build_eh-lb523_hcpu\\src\\test.o] Error 1\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\kservice.o\nscons: building terminated because of errors.\n\nVadim@VPC C:\\proj\\sifli_sdk\\example\\get-started\\hello_world\\rtt\\project\n>\n```"}, {"user": "HalfSweet", "created_at": "2025-02-24T15:10:19Z", "body": "This also seems to be a build system issue that we will fix in the next release version. I'll sync with you when there's new news"}, {"user": "rabbitsaviola", "created_at": "2025-03-02T07:48:24Z", "body": "hi @Vadimatorik, please modify SConstruct as below, i.e. change `CCFLAGS=rtconfig.CFLAGS` to `CFLAGS=rtconfig.CFLAGS` \n\n```python\nenv = Environment(tools = ['mingw'],\n    AS = rtconfig.AS, ASFLAGS = rtconfig.AFLAGS,\n    CC = rtconfig.CC, CFLAGS = rtconfig.CFLAGS,\n    CXX = rtconfig.CXX, CXXFLAGS = rtconfig.CXXFLAGS,\n    AR = rtconfig.AR, ARFLAGS = '-rc',\n    LINK = rtconfig.LINK, LINKFLAGS = rtconfig.LFLAGS)\n```"}, {"user": "Vadimatorik", "created_at": "2025-03-02T23:11:46Z", "body": "It works, thanks. If anyone else encounters this problem, here are my changes:\n```bash\n-    CC = rtconfig.CC, CCFLAGS = rtconfig.CFLAGS,\n+    CC = rtconfig.CC, CFLAGS = rtconfig.CFLAGS,\n```"}, {"user": "Vadimatorik", "created_at": "2025-03-02T23:12:40Z", "body": "I think this should be fixed in the SDK examples in the future"}], "satisfaction_conditions": ["A working configuration for building C++ files in the project", "Correct parameter naming in the build system configuration", "Clear instructions on what changes to make to the build configuration", "Information that could be applied to the specific SDK being used"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:19"}, "dockerfile": null, "language": "c"}
{"task_id": "cab_lenient_52", "number": 19, "title": "Typo in line 160 of README", "created_at": "2025-03-04T07:08:46Z", "closed_at": "2025-03-10T07:20:09Z", "commit_id": "408e87d9d15e5737db29dcf07575b42c5440cf19", "labels": ["documentation"], "url": "https://github.com/DreamMaoMao/maomaowm/issues/19", "body": "should be wa\"y\"bar, correct?\n\nalso might want to add an explanation to edit line 142 of `preset_config.h` if user wishes to change number of tags. and this number needs to match num-tags in waybar dwl/tags module.", "comments_url": "https://api.github.com/repos/DreamMaoMao/maomaowm/issues/19/comments", "author": "hooxoo", "comments": [{"user": "DreamMaoMao", "created_at": "2025-03-04T10:28:41Z", "body": "I do not recommend that the user directly modify the code, I would consider adding a variable number of tags through configuration, as well as tag naming\n"}, {"user": "hooxoo", "created_at": "2025-03-10T05:04:16Z", "body": "\"warbar\" typo still there in README.."}, {"user": "DreamMaoMao", "created_at": "2025-03-10T07:20:09Z", "body": "fixed"}], "satisfaction_conditions": ["Correction of the typo 'warbar' to 'waybar' in the README", "Consideration of a configuration approach for tag management rather than direct code modification", "Acknowledgment that the issue has been addressed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:54"}, "dockerfile": null, "language": "c"}
{"task_id": "cab_lenient_53", "number": 2, "title": "detect aerospace path", "created_at": "2025-02-04T17:48:20Z", "closed_at": "2025-02-04T21:50:04Z", "commit_id": "35c94469dd57da34bacfa1f299e9047b3ec9e4c1", "labels": [], "url": "https://github.com/acsandmann/aerospace-swipe/pull/2", "body": "closes #1 ", "comments_url": "https://api.github.com/repos/acsandmann/aerospace-swipe/issues/2/comments", "author": "acsandmann", "comments": [{"user": "acsandmann", "created_at": "2025-02-04T17:48:48Z", "body": "@FormalSnake can you try this?"}, {"user": "FormalSnake", "created_at": "2025-02-04T21:46:37Z", "body": "That works beautifully but for some reason not when I have it installed using \"make install\", idk if it is because it isnt running afterwards or smth."}, {"user": "acsandmann", "created_at": "2025-02-05T01:00:14Z", "body": "@FormalSnake I think the latest commit fixed the issue with make install. When installing from the makefile it automatically adds it into launchctl and I think in that context it doesn't have access to where/which so I switched it to `command -v aerospace` and that seems to be working. "}], "satisfaction_conditions": ["A solution that works when installed via 'make install'", "A method to properly detect the aerospace path in different execution contexts", "Compatibility with the system's launch control mechanism"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:53"}, "dockerfile": null, "language": "c"}
{"task_id": "cab_lenient_54", "number": 2, "title": "No rule \"install\" in makefile", "created_at": "2025-03-22T10:50:24Z", "closed_at": "2025-03-22T11:33:26Z", "commit_id": "0951198b40e7b40c608cd056ca641afefa6da596", "labels": [], "url": "https://github.com/alfiecg24/KextRW/issues/2", "body": "Hello, thank you for your work.\n\nIn the README.md, you tell to follow install instructions from IOKernelRW.\nIn their installation procedure, they suggest to use `make install`to install the kext, but here, there is no ìnstall` rule in the Makefile.\n\nBest regards.", "comments_url": "https://api.github.com/repos/alfiecg24/KextRW/issues/2/comments", "author": "alexandredoyen29", "comments": [{"user": "alfiecg24", "created_at": "2025-03-22T11:06:08Z", "body": "I have no idea where that went, sorry! It was definitely there before - I must have accidentally removed it. You can instead manually install it by running `sudo cp -R KextRW.kext /Library/Extensions`."}, {"user": "alexandredoyen29", "created_at": "2025-03-22T11:19:19Z", "body": "Yep, I did this"}], "satisfaction_conditions": ["An alternative method to install the kext when the make install command is unavailable", "Clear instructions that can be executed to install the kext", "Information that addresses the discrepancy between the README and the actual Makefile"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:30"}, "dockerfile": null, "language": "c"}
