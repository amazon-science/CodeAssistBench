{"number": 19, "title": "Typo in line 160 of README", "created_at": "2025-03-04T07:08:46Z", "closed_at": "2025-03-10T07:20:09Z", "commit_id": "408e87d9d15e5737db29dcf07575b42c5440cf19", "labels": ["documentation"], "url": "https://github.com/DreamMaoMao/maomaowm/issues/19", "body": "should be wa\"y\"bar, correct?\n\nalso might want to add an explanation to edit line 142 of `preset_config.h` if user wishes to change number of tags. and this number needs to match num-tags in waybar dwl/tags module.", "comments_url": "https://api.github.com/repos/DreamMaoMao/maomaowm/issues/19/comments", "author": "hooxoo", "comments": [{"user": "DreamMaoMao", "created_at": "2025-03-04T10:28:41Z", "body": "I do not recommend that the user directly modify the code, I would consider adding a variable number of tags through configuration, as well as tag naming\n"}, {"user": "hooxoo", "created_at": "2025-03-10T05:04:16Z", "body": "\"warbar\" typo still there in README.."}, {"user": "DreamMaoMao", "created_at": "2025-03-10T07:20:09Z", "body": "fixed"}], "satisfaction_conditions": ["Correction of the typo 'warbar' to 'waybar' in the README", "Consideration of a configuration approach for tag management rather than direct code modification", "Acknowledgment that the issue has been addressed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:54"}, "dockerfile": null, "language": "c"}
{"number": 138, "title": "ProMicro (faketec) sx1262 firmvare V1.4.1 ?", "created_at": "2025-03-25T13:04:16Z", "closed_at": "2025-03-31T06:07:29Z", "commit_id": "88b88cbc901f2a1dd5329f84901dde4546d82c44", "labels": [], "url": "https://github.com/ripplebiz/MeshCore/issues/138", "body": "Will firmware version 1.4.1 be released for ProMicro (faketec) sx1262? Version 1.4 has disappeared from Web Flasher.", "comments_url": "https://api.github.com/repos/ripplebiz/MeshCore/issues/138/comments", "author": "sebikolo", "comments": [{"user": "adrian-immel", "created_at": "2025-03-25T22:50:37Z", "body": "#144 should fix this issue"}, {"user": "sebikolo", "created_at": "2025-03-26T07:07:20Z", "body": "Thank you. Will ProMicro support be added back to Web Flasher?"}, {"user": "adrian-immel", "created_at": "2025-03-26T16:44:43Z", "body": "It should reappear with the next release."}, {"user": "sebikolo", "created_at": "2025-03-26T16:57:52Z", "body": "Thank you for the information. I will be waiting impatiently :-)"}, {"user": "oltaco", "created_at": "2025-03-31T00:02:15Z", "body": "It's built again for v1.4.2 so this can be closed."}, {"user": "sebikolo", "created_at": "2025-03-31T06:04:51Z", "body": "Yes :-)"}], "satisfaction_conditions": ["Confirmation that firmware support for ProMicro (faketec) sx1262 will be available in a future release", "Information about when ProMicro support will return to Web Flasher", "Actual availability of the firmware for their device in Web Flasher"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:36"}, "dockerfile": null, "language": "c"}
{"number": 113, "title": "Station G2 issues", "created_at": "2025-03-16T10:27:09Z", "closed_at": "2025-03-16T12:59:57Z", "commit_id": "882377e4d6db73d3987c935b439b28bf1f558f56", "labels": [], "url": "https://github.com/ripplebiz/MeshCore/issues/113", "body": "Station G2 issues with 1.3 meshcore-firmware\nNo display in repeater mode\nNo display in Roomserver mode\nNo possibility to log in, standard password does not apply.\nBluetooth in both webclients do not work - laptop W10\nSerial access does not work, (possible cause: Laptop does not support 15V on usb-c port, 15 V needed for Station G2\n\nMartin pd0zz", "comments_url": "https://api.github.com/repos/ripplebiz/MeshCore/issues/113/comments", "author": "Martje63", "comments": [{"user": "recrof", "created_at": "2025-03-16T10:33:16Z", "body": "display is not implemented in any role, will get implemented later.\nSerial access should work even when you're connected to the 5V usb. Only thing that doesn't work when you use 5V is PA - everything will get TXed, but without extra power. did you restart Station G2 after flashing?\n"}, {"user": "recrof", "created_at": "2025-03-16T10:40:15Z", "body": "> Bluetooth in both webclients do not work - laptop W10\n\nyou can't connect to repeater / room server via bluetooth. you can administer them from serial console or using t-deck or companion device + MeshCore mobile app via LoRa."}, {"user": "Martje63", "created_at": "2025-03-16T11:54:36Z", "body": "OK, but the standard password for admin connect to **Station G2 Room** via android app does NOT work , no 123456 login... How to solve that problem?\n\nI can access the general Room with password 'hello' but when trying to Remote Management it says i need to login as Admin..."}, {"user": "recrof", "created_at": "2025-03-16T11:56:54Z", "body": "standard default password for managing the room server is `password` and access for users is `hello`."}, {"user": "Martje63", "created_at": "2025-03-16T12:59:57Z", "body": "Can be closed, could not find the correct password, solved! \nOther issues solved for now! \n\nThanks for the answers!"}], "satisfaction_conditions": ["Information about the current implementation status of display functionality in different modes", "Clarification on power requirements for different Station G2 functionalities", "Explanation of supported connection methods for different device modes", "Correct authentication credentials for accessing administrative functions"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:40"}, "dockerfile": null, "language": "c"}
{"number": 33, "title": "I can't compile C++ files", "created_at": "2025-02-24T11:11:20Z", "closed_at": "2025-03-02T23:11:46Z", "commit_id": "f98582b854c17de20bf0f2a2a68e8c5571c50108", "labels": [], "url": "https://github.com/OpenSiFli/SiFli-SDK/issues/33", "body": "Hi)\n\nMy project requires C++. I tried to add a C++ test file to the test project, but it didn't build:\n```\nCXX build_eh-lb523_hcpu\\src\\test_cpp_file.o\nError in calling command:g++\nException: No such file or directory\n\nPlease check Toolchains PATH setting.\n\nscons: *** [build_eh-lb523_hcpu\\src\\test_cpp_file.o] Error 2\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\clock.o\nscons: building terminated because of errors.\n```\nI added the following line to SConstruct:\n```\nCXX = rtconfig.CXX, CXXFLAGS = rtconfig.CXXFLAGS,\n```\nThe conclusion was as follows:\n```\nC:/proj/sifli_sdk/example/get-started/blink/rtt/src/test_cpp_file.cpp:1:1: error: unknown type name 'namespace'\nnamespace test_namespace {\n^\nC:/proj/sifli_sdk/example/get-started/blink/rtt/src/test_cpp_file.cpp:1:25: error: expected ';' after top level declarator\nnamespace test_namespace {\n                        ^\n                        ;\n2 errors generated.\n```\nWhat do I need to do to get the files to start building? Here's the content of my test file:\n```\nnamespace test_namespace {\n\nclass TestClass {\npublic:\n        TestClass() = default;\n};\n\n};\n\n```", "comments_url": "https://api.github.com/repos/OpenSiFli/SiFli-SDK/issues/33/comments", "author": "Vadimatorik", "comments": [{"user": "HalfSweet", "created_at": "2025-02-24T13:04:26Z", "body": "This looks like scons complaining about an error in the argument to g++.\nCan you provide more logs or a complete project for us to analyze?"}, {"user": "Vadimatorik", "created_at": "2025-02-24T14:09:25Z", "body": "You can use the example for tests: \"\\example\\get-started\\hello_world\\rtt\". Here is an example of a full build log:\n```\nVadim@VPC C:\\proj\\sifli_sdk\\example\\get-started\\hello_world\\rtt\\project\n> scons --board=eh-lb523 -j12\nscons: Reading SConscript files ...\nBoard: eh-lb523_hcpu\n========\nMulti-Project Info\n--------\nfull_name       main.bootloader\nparent          main\nbsp_root        C:\\proj\\sifli_sdk\\example\\boot_loader\\project\\butterflmicro\\ram_v2\nbuild_dir       build_eh-lb523_hcpu/bootloader\nlink_script     C:/proj/sifli_sdk/example/boot_loader/project/butterflmicro/ram_v2\\link\nptab            C:/proj/sifli_sdk/customer/boards/eh-lb523\\ptab.json\nembedded:       False\n--------\nfull_name       main\nparent\nbsp_root        C:\\proj\\sifli_sdk\\example\\get-started\\hello_world\\rtt\\project\nbuild_dir       build_eh-lb523_hcpu/\nlink_script     C:/proj/sifli_sdk/drivers/cmsis/sf32lb52x/Templates/arm/HCPU/link\nptab            C:/proj/sifli_sdk/customer/boards/eh-lb523\\ptab.json\n--------\nfull_name       main.ftab\nparent          main\nbsp_root        C:\\proj\\sifli_sdk\\example\\flash_table\\sf32lb52x_common_v2\nbuild_dir       build_eh-lb523_hcpu/ftab\nlink_script     C:/proj/sifli_sdk/example/flash_table/sf32lb52x_common_v2\\link\nptab            C:/proj/sifli_sdk/customer/boards/eh-lb523\\ptab.json\nembedded:       False\n========\nscons: done reading SConscript files.\nscons: Building targets ...\nscons: building associated VariantDir targets: build_eh-lb523_hcpu . .\nCC build_eh-lb523_hcpu\\bootloader\\board\\bf0_ap_hal_msp.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\board.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\board_psram.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\boot_flash.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\efuse.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\main.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\sd_emmc_ops.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\sd_nand_drv.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\sd_nand_ops.o\nCC build_eh-lb523_hcpu\\bootloader\\board\\secboot.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\common\\bsp_common.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\common\\flash.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_init.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_lcd_tp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_pinmux.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_power.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bf0_pin_const.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\system_bf0_ap.o\nAS build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\arm\\startup_bf0_hcpu.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_adc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_aes.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_aes_ns.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_audcodec_m.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_audprc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_bleaon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_busmon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_cortex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_crc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_dma.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_dsi.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_efuse.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_epic.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_ext_dma.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_ezip.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_facc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_fft.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_gpio.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_hcd.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_hlp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_hpaon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_i2c.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_i2s.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lcdc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lcpu_config.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lpaon.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lpcomp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lptim.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_lrc_cal.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mailbox.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_ex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_psram.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_nn_acc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_patch.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pcd.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pdm.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pinmux.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_pmu.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_psram.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_ptc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_rcc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_rng.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_rtc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sd_ex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sdadc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sdhci.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_sdmmc.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_secu.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_spi.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_tim.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_tim_ex.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_tsen.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_uart.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_hal_wdt.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\bf0_sys_cfg.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\flash_table.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\nand_table.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\sifli_bbm.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\aes.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\cipher.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\cipher_wrap.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\md.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\md_wrap.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\platform.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sha256.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sha512.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\asn1parse.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\bignum.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\oid.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\pk.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\pkparse.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\pk_wrap.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\rsa.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sm2.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\sm3.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\ecp.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\ecp_curves.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\external\\mbedtls\\library\\ctr_drbg.o\nAR build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\drivers\\hal\\BF0_HAL.lib\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\middleware\\bluetooth\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\bootloader\\sifli_sdk\\middleware\\sifli_lib\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\common\\bsp_common.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\common\\flash.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_init.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_lcd_tp.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_pinmux.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\boards\\eh-lb52xu\\bsp_power.o\nLINK build_eh-lb523_hcpu\\bootloader\\bootloader.axf\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\cst816\\cst816.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\cst816\\cst816_update.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\gc9b71\\gc9b71.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\customer\\peripherals\\pa\\AW8155\\sifli_aw8155.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bf0_pin_const.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\system_bf0_ap.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\lcpu_patch.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\lcpu_patch_rev_b.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bt_rf_fulcal.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\bt_rf_test.o\nAS build_eh-lb523_hcpu\\sifli_sdk\\drivers\\cmsis\\sf32lb52x\\Templates\\arm\\startup_bf0_hcpu.o\nProgram Size: Code=47216 RO-data=6044 RW-data=7428 ZI-data=32916\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal.o\nGenerating build_eh-lb523_hcpu\\bootloader\\bootloader.bin ...\nGenerating build_eh-lb523_hcpu\\bootloader\\bootloader.hex ...\nGenerating build_eh-lb523_hcpu\\bootloader\\bootloader.asm ...\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_adc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_aes.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_aes_ns.o\n\n========================================================================\n\n** Object/Image Component Sizes\n\n      Code (inc. data)   RO Data    RW Data    ZI Data      Debug   Object Name\n\n     47216       2740       6048       7432      32912     591304   build_eh-lb523_hcpu\\bootloader\\bootloader.axf (uncompressed)\n     47216       2740       6048       1200      32912     591304   build_eh-lb523_hcpu\\bootloader\\bootloader.axf (compressed)\n         0          0          4          0          0          0   (incl. padding)\n     47216       2740       6048       1200          0          0   ROM Totals for build_eh-lb523_hcpu\\bootloader\\bootloader.axf\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_audcodec_m.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_audprc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_bleaon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_busmon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_cortex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_crc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_dsi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_efuse.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_epic.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_ext_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_ezip.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_facc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_fft.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_gpio.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_hcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_hlp.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_hpaon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_i2c.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_i2s.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lcdc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lcpu_config.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lpaon.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lpcomp.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lptim.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_lrc_cal.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mailbox.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_ex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_mpi_psram.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_nn_acc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_patch.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pdm.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pinmux.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_pmu.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_psram.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_ptc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_rcc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_rng.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_rtc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sd_ex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sdadc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sdhci.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_sdmmc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_secu.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_spi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_tim.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_tim_ex.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_tsen.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_uart.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_hal_wdt.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\bf0_sys_cfg.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\flash_table.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\nand_table.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\sifli_bbm.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\middleware\\bluetooth\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\middleware\\sifli_lib\\lib\\dummy.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\middleware\\system\\bf0_common.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\os_adaptor\\src\\os_adaptor_rtthread.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_gpio.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_usart.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_hwtimer.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_pwm.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_pwm_lptim.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_spi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_soft_i2c.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_i2c.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_adc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_rtc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_spi_flash.o\nAR build_eh-lb523_hcpu\\sifli_sdk\\drivers\\hal\\BF0_HAL.lib\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_spi_nand.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_sys_cfg.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_ext_dma.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_audprc.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_audcodec_m.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd_private.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_ram_lcd.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd_test.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_lcd_fb.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_touch.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_epic.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_psram.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_mpi.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_aes.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_common.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\rtthread\\bsp\\sifli\\drivers\\drv_dbg.o\nCC build_eh-lb523_hcpu\\src\\main.o\nCXX build_eh-lb523_hcpu\\src\\test.o\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\clock.o\nC:/proj/sifli_sdk/example/get-started/hello_world/rtt/src/test.cpp:1:CC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\components.o\n1: error: unknown typeC C build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\cpu.o\nname 'namespace'\nnamespace test_namespace {\n^\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\device.o\nC:/proj/sifli_sdk/example/get-started/hello_world/rtt/src/test.cpp:1:25:C C build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\idle.o\nerror: expected ';' after top level declarator\nCnamespace test_namespace {C build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\ipc.o\n\n                        ^\n                        ;\n2 errors generated.\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\irq.o\nscons: *** [build_eh-lb523_hcpu\\src\\test.o] Error 1\nCC build_eh-lb523_hcpu\\sifli_sdk\\rtos\\kernel\\src\\kservice.o\nscons: building terminated because of errors.\n\nVadim@VPC C:\\proj\\sifli_sdk\\example\\get-started\\hello_world\\rtt\\project\n>\n```"}, {"user": "HalfSweet", "created_at": "2025-02-24T15:10:19Z", "body": "This also seems to be a build system issue that we will fix in the next release version. I'll sync with you when there's new news"}, {"user": "rabbitsaviola", "created_at": "2025-03-02T07:48:24Z", "body": "hi @Vadimatorik, please modify SConstruct as below, i.e. change `CCFLAGS=rtconfig.CFLAGS` to `CFLAGS=rtconfig.CFLAGS` \n\n```python\nenv = Environment(tools = ['mingw'],\n    AS = rtconfig.AS, ASFLAGS = rtconfig.AFLAGS,\n    CC = rtconfig.CC, CFLAGS = rtconfig.CFLAGS,\n    CXX = rtconfig.CXX, CXXFLAGS = rtconfig.CXXFLAGS,\n    AR = rtconfig.AR, ARFLAGS = '-rc',\n    LINK = rtconfig.LINK, LINKFLAGS = rtconfig.LFLAGS)\n```"}, {"user": "Vadimatorik", "created_at": "2025-03-02T23:11:46Z", "body": "It works, thanks. If anyone else encounters this problem, here are my changes:\n```bash\n-    CC = rtconfig.CC, CCFLAGS = rtconfig.CFLAGS,\n+    CC = rtconfig.CC, CFLAGS = rtconfig.CFLAGS,\n```"}, {"user": "Vadimatorik", "created_at": "2025-03-02T23:12:40Z", "body": "I think this should be fixed in the SDK examples in the future"}], "satisfaction_conditions": ["A working configuration for building C++ files in the project", "Correct parameter naming in the build system configuration", "Clear instructions on what changes to make to the build configuration", "Information that could be applied to the specific SDK being used"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:19"}, "dockerfile": null, "language": "c"}
{"number": 26, "title": "Deprecated parameter \"BSP_USING_MOTOR\"", "created_at": "2025-02-17T14:17:04Z", "closed_at": "2025-02-18T08:02:53Z", "commit_id": "933aadee94290bceb6009a55bb07515a77cde710", "labels": [], "url": "https://github.com/OpenSiFli/SiFli-SDK/issues/26", "body": "In my project I use a vibration motor. I thought that this module creates a rhythmic pattern. However, this parameter is not used at all:\n```bash\n> grep -nR \"BSP_USING_MOTOR\" *\ncustomer/boards/Kconfig_drv:1691:    config BSP_USING_MOTOR\nexample/ble/ancs_dualcore/project/eh-lb555/hcpu/.config:447:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/ancs_dualcore/project/eh-lb555/lcpu/.config:105:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/ancs_dualcore/project/eh-ss6600_551/lcpu/.config:109:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/lcpu_general/project/eh-6500/.config:62:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/lcpu_general/project/eh-lb523/.config:63:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/lcpu_general/project/eh-lb555/.config:330:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/lcpu_general/project/eh-lb561/.config:107:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/lcpu_general/project/eh-lb563/.config:107:# CONFIG_BSP_USING_MOTOR is not set\nexample/ble/lcpu_general/project/eh-ss6600_551/.config:359:# CONFIG_BSP_USING_MOTOR is not set\nexample/boot_loader/project/butterflmicro/ram/.config:90:# CONFIG_BSP_USING_MOTOR is not set\nexample/boot_loader/project/ec-lb561xxxx001_nand/.config:118:# CONFIG_BSP_USING_MOTOR is not set\nexample/boot_loader/project/ec-lb567xxxx001/.config:125:# CONFIG_BSP_USING_MOTOR is not set\nexample/boot_loader/project/ec-lb583xxxx001_v11/.config:126:# CONFIG_BSP_USING_MOTOR is not set\nexample/boot_loader/project/ec-lb587xxxx001_v11/.config:141:# CONFIG_BSP_USING_MOTOR is not set\nexample/get-started/blink/rtt/project/build_eh-lb523_hcpu/.config:519:# CONFIG_BSP_USING_MOTOR is not set\nexample/get-started/blink/rtt/project/build_eh-lb523_hcpu/bootloader/.config:266:# CONFIG_BSP_USING_MOTOR is not set\nexample/get-started/blink/rtt/project/build_eh-lb523_hcpu/ftab/.config:296:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/lvgl_v9_demos/project/build_vape_hcpu/.config:992:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/lvgl_v9_demos/project/build_vape_hcpu/bootloader/.config:266:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/lvgl_v9_demos/project/build_vape_hcpu/ftab/.config:296:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/lvgl_v9_examples/project/build_vape_hcpu/.config:999:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/lvgl_v9_examples/project/build_vape_hcpu/bootloader/.config:266:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/lvgl_v9_examples/project/build_vape_hcpu/ftab/.config:296:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/watch/project/build_vape_hcpu/.config:922:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/watch/project/build_vape_hcpu/.config.old:922:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/watch/project/build_vape_hcpu/bootloader/.config:266:# CONFIG_BSP_USING_MOTOR is not set\nexample/multimedia/lvgl/watch/project/build_vape_hcpu/ftab/.config:296:# CONFIG_BSP_USING_MOTOR is not set\n```", "comments_url": "https://api.github.com/repos/OpenSiFli/SiFli-SDK/issues/26/comments", "author": "Vadimatorik", "comments": [{"user": "sz30370017", "created_at": "2025-02-18T07:56:52Z", "body": "currently，there is not the demo code for the vibration moto work as a rhythmic pattern, you may refer to the pwm demo code  to finish it.\nexample\\rt_device\\pwm\\project or \\example\\hal\\pwm\\project"}, {"user": "Vadimatorik", "created_at": "2025-02-18T08:02:53Z", "body": "Thanks for the answer) I think that in this case, it is better to remove this item for now. It is confusing)\n\nI used the example `customer\\peripherals\\vibrator` for this task. This uses the system timer, but for my task it was enough."}, {"user": "sz30370017", "created_at": "2025-02-18T08:39:29Z", "body": "ok, thanks for your advices."}], "satisfaction_conditions": ["Clarification about the purpose and implementation status of the BSP_USING_MOTOR parameter", "Alternative approaches for implementing vibration motor functionality", "Feedback acknowledgment regarding confusing/unused configuration options"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:25"}, "dockerfile": null, "language": "c"}
{"number": 102, "title": "Вернул локальное обновление для уже прошитого устройства", "created_at": "2025-03-17T08:00:56Z", "closed_at": "2025-03-25T19:11:38Z", "commit_id": "ffffdb1d80a95988ffacae7424c99f689bff66ed", "labels": [], "url": "https://github.com/slacky1965/tuya_thermostat_zrd/pull/102", "body": null, "comments_url": "https://api.github.com/repos/slacky1965/tuya_thermostat_zrd/issues/102/comments", "author": "ilkh", "comments": [{"user": "ixmax", "created_at": "2025-03-20T06:42:03Z", "body": "Запятая в конце строки лишняя, должно быть так:\r\n`    {\r\n        \"url\": \"images/6565-0391-10143001-tuya_thermostat_zrd.zigbee\"\r\n    }`\r\nИначе вылетает ошибка при проверке обновления\r\n\r\n> z2m: Failed to check if update available for '%device_name%' (Expected double-quoted property name in JSON at position %num% (line 21 column 5))"}, {"user": "ilkh", "created_at": "2025-03-20T09:13:32Z", "body": "> Запятая в конце строки лишняя, должно быть так: ` { \"url\": \"images/6565-0391-10143001-tuya_thermostat_zrd.zigbee\" }` Иначе вылетает ошибка при проверке обновления\r\n> \r\n> > z2m: Failed to check if update available for '%device_name%' (Expected double-quoted property name in JSON at position %num% (line 21 column 5))\r\n\r\nСпасибо, поправил"}, {"user": "slacky1965", "created_at": "2025-03-20T16:10:40Z", "body": "Да это скоро станет не важно. Обновления будут придлетать от самого z2m, специально прописывать ничего не нужно будет ..."}], "satisfaction_conditions": ["Correction of JSON syntax error by removing the trailing comma", "Information that resolves the error message about double-quoted property names in JSON", "A working JSON configuration that allows the system to check for updates without errors"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:23"}, "dockerfile": null, "language": "c"}
{"number": 58, "title": "I Cannot update to 1.0.08", "created_at": "2025-01-06T13:43:23Z", "closed_at": "2025-01-06T15:44:21Z", "commit_id": "3e48a815a77c7e99d046bf9b3d213a5b67fb7e90", "labels": [], "url": "https://github.com/slacky1965/tuya_thermostat_zrd/issues/58", "body": "Hi,\r\n\r\nThank you for this firmware, I have a couple of these (Tuya_Thermostat_r01) and they are working well for me on 1.0.07.\r\n\r\nI am trying to update them to the 1.0.08 version. I have `local_ota_index.json` as specified and have confirmed it is being used by z2m. I have the `1141-d3a3-1111114b-tuya_thermostat_zrd.zigbee` present in the images subfolder.\r\n\r\nDespite this, z2m says there is no update available when checking these devices.\r\n\r\nI have since tried updating my z2m and the configuration from this repo to the 2.0 version however the new `local_ota_index.json` specifies an extra entry for `tuya_thermostat_zrd.zigbee` \r\n\r\n```\r\n    {\r\n        \"url\": \"images/tuya_thermostat_zrd.zigbee\"\r\n    }\r\n```\r\n\r\nWhich gives an error in z2m as this file doesn't exist. What should this file be? A copy of the full name `1141-d3a3-1111114b-tuya_thermostat_zrd.zigbee` or something else?\r\n\r\nIf I remove the new entry in `local_ota_index.json` I have the same behaviour as the pre-2.0 version, no updates available for the devices.\r\n\r\nThanks again!", "comments_url": "https://api.github.com/repos/slacky1965/tuya_thermostat_zrd/issues/58/comments", "author": "csutcliff", "comments": [{"user": "devbis", "created_at": "2025-01-06T15:15:13Z", "body": "@csutcliff  To upgrade custom firmware you need to put \r\n6565-0391-10083001-tuya_thermostat_zrd.zigbee  to the corresponding folder (e.g. images in your case) and add it to local_ota file:\r\n\r\n```json\r\n{\r\n        \"url\": \"images/6565-0391-10083001-tuya_thermostat_zrd.zigbee\"\r\n}\r\n```\r\n\r\nd3a3 file is used for the initial flashing only. Now for upgraded devices the manufacturer code is 6565"}, {"user": "csutcliff", "created_at": "2025-01-06T15:44:21Z", "body": "excellent, that's what I was missing, thank you!"}, {"user": "slacky1965", "created_at": "2025-01-06T15:55:33Z", "body": "For an easy upgrade, you need to rename the file `6565-0391-XXXXXXXX-tuya_thermostat_zrd.zigbee` to `tuya_thermostat_zrd.zigbee` and do this every time you want to get the next version of the firmware. And leave the previous entry in **local_ota_index.json**. \r\n\r\n```\r\n    {\r\n        \"url\": \"images/tuya_thermostat_zrd.zigbee\"\r\n    }\r\n```\r\n\r\nThen you won't need to restart **zigbee2mqtt** every time before updating."}], "satisfaction_conditions": ["Clear explanation of the correct firmware file naming convention for updates", "Instructions for properly configuring the local_ota_index.json file", "Information about the difference between initial flashing and subsequent updates", "A working solution that enables firmware updates to version 1.0.08"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:32"}, "dockerfile": null, "language": "c"}
{"number": 6, "title": "如何修改容器80端口", "created_at": "2025-03-13T15:23:17Z", "closed_at": "2025-03-13T15:57:02Z", "commit_id": "49855b7b7cde8d8b330f64d1b5964b0c88092022", "labels": [], "url": "https://github.com/levywang/avhub/issues/6", "body": "80，81被NPM占用", "comments_url": "https://api.github.com/repos/levywang/avhub/issues/6/comments", "author": "Hansen1018", "comments": [{"user": "levywang", "created_at": "2025-03-13T15:56:52Z", "body": "举例\n```bash\ndocker run -d -p 8080:80 -v $PWD:/app --name avhub levywang/avhub:latest \n``` \n"}, {"user": "Hansen1018", "created_at": "2025-03-13T16:04:04Z", "body": "> 举例\n> \n> docker run -d -p 8080:80 -v $PWD:/app --name avhub levywang/avhub:latest\n\n是docker内部端口"}, {"user": "levywang", "created_at": "2025-03-14T01:23:42Z", "body": "你这个需求太小众，需要手动构建一个自己的镜像：\n克隆仓库后，修改`nginx.example.conf`中的端口为你自己想要的端口\n再修改`Dockerfile`中的`EXPOSE 80`端口，与上面的保持一致\n最后手动构建\n```bash\ndocker build -t <your_image_name> .\ndocker run ... <your_image_name>\n```\n"}, {"user": "Hansen1018", "created_at": "2025-03-14T02:51:08Z", "body": "> 你这个需求太小众，需要手动构建一个自己的镜像： 克隆仓库后，修改`nginx.example.conf`中的端口为你自己想要的端口 再修改`Dockerfile`中的`EXPOSE 80`端口，与上面的保持一致 最后手动构建\n> \n> docker build -t <your_image_name> .\n> docker run ... <your_image_name>\n\n好的，感谢"}], "satisfaction_conditions": ["Instructions for modifying the container's internal port configuration", "A workaround for port conflicts with NPM", "Step-by-step guidance for creating a custom Docker image with modified port settings", "Specific file locations and parameters that need modification to change container ports"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:44"}, "dockerfile": null, "language": "c"}
{"number": 5, "title": "Fixed example code in README.md", "created_at": "2024-11-24T12:44:22Z", "closed_at": "2024-11-24T12:52:47Z", "commit_id": "8155eade51a897da19d7e3758e68251f2bd0b066", "labels": [], "url": "https://github.com/lvntky/fbgl/pull/5", "body": "Example code now correctly declares a fbgl_t buffer and passes it to relevant functions, also, fbgl_get_{width,height} are renamed to the kernel functions fb_get_{width,height} as a library-provided wrapper is non-existent (and probably unnecessary).", "comments_url": "https://api.github.com/repos/lvntky/fbgl/issues/5/comments", "author": "dario-loi", "comments": [{"user": "lvntky", "created_at": "2024-11-24T12:52:25Z", "body": "Thanks @dario-loi im merging the commits and also will add you in contributors list in .h file when i log in to my computer, much appreciated 🙏 "}], "satisfaction_conditions": ["Acknowledgment of the code corrections in the README.md file", "Acceptance of the pull request containing the fixes", "Recognition of the contributor's effort"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:50"}, "dockerfile": null, "language": "c"}
{"number": 4, "title": "Embedded shaders", "created_at": "2025-02-27T19:21:06Z", "closed_at": "2025-02-28T14:58:58Z", "commit_id": "6d5d96b804c9b8ec19f69a9a7d908b4d2cc77113", "labels": [], "url": "https://github.com/Bigfoot71/r3d/issues/4", "body": "When trying to run either examples or own projects linking to the library it woudl seem that the default shaders were not embedded properly as it fails to load them in all cases resulting in a black window. Here is an example of the output from the basic example with it being the same for my own built.\n\n```\nINFO: SHADER: [ID 4] Vertex shader compiled successfully\nINFO: SHADER: [ID 5] Fragment shader compiled successfully\nWARNING: SHADER: [ID 6] Failed to link shader program\nWARNING: SHADER: [ID 6] Link error: ERROR: Linking vertex stage: Missing entry point: Each stage requires one entry point\nERROR: Linking fragment stage: Missing entry point: Each stage requires one entry point\n\nWARNING: SHADER: Failed to load custom shader code, using default shader\nINFO: SHADER: [ID 4] Vertex shader compiled successfully\nINFO: SHADER: [ID 5] Fragment shader compiled successfully\nWARNING: SHADER: [ID 6] Failed to link shader program\nWARNING: SHADER: [ID 6] Link error: ERROR: Linking vertex stage: Missing entry point: Each stage requires one entry point\nERROR: Linking fragment stage: Missing entry point: Each stage requires one entry point\n\nWARNING: SHADER: Failed to load custom shader code, using default shader\n```\n\nBuilding on windows using cmake and Mingw. Only special flags for cmake differing from the build instrcutions are `-G \"MinGW Makefiles\" -DPYTHON_EXECUTABLE=python`. As it would seem that when building it was looking for python3 while I do indeed have python 3.12 the naming was different.", "comments_url": "https://api.github.com/repos/Bigfoot71/r3d/issues/4/comments", "author": "R2Sam", "comments": [{"user": "Bigfoot71", "created_at": "2025-02-27T22:23:00Z", "body": "~Can you directly copy/paste the generated file or tell me what’s inside?\nIt should be located in your build directory at `generated/src/embedded/r3d_shaders.c`\nIf there was an error with Python, it should be present in the strings instead of the minified GLSL code~\n\n**EDIT**: I just tried with the same command as you: `-G \"MinGW Makefiles\" -DPYTHON_EXECUTABLE=python`\n\nThe issue seems to come from `-DPYTHON_EXECUTABLE=python`\n\nEven though `python` appears to be an alias for `python3` on my system, for some reason, this prevents the generation, no errors, nothing...\n\nIn any case, you shouldn’t need to specify python in cmake.\nIt should be found automatically if it’s in your `PATH` variable:  \n```cmake\nfind_program(PYTHON_EXECUTABLE python3 REQUIRED)\n```\n\nUnless you have a particular setup with your installation?\n\nLet me know if removing `-DPYTHON_EXECUTABLE=python` solves the issue\n\nAnd just to be sure, check the generated file in your cmake build directory:  `generated/src/embedded/r3d_shaders.c`  \n\nMake sure you’re getting the same result as me, empty strings..."}, {"user": "R2Sam", "created_at": "2025-02-28T14:59:13Z", "body": "Perfect that was it thanks"}], "satisfaction_conditions": ["Identification of the root cause preventing shader embedding", "A working configuration for building the library with proper shader embedding", "Clear guidance on CMake configuration for the library"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 00:59:51"}, "dockerfile": "FROM ubuntu:22.04\n\n# Set environment variables to avoid interactive prompts during installation\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    git \\\n    python3 \\\n    python3-pip \\\n    libgl1-mesa-dev \\\n    libx11-dev \\\n    libxcursor-dev \\\n    libxinerama-dev \\\n    libxrandr-dev \\\n    libxi-dev \\\n    libxext-dev \\\n    libasound2-dev \\\n    mesa-common-dev \\\n    xorg-dev \\\n    libglu1-mesa-dev \\\n    wget \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install newer CMake version (3.25+)\nRUN wget -qO- \"https://cmake.org/files/v3.25/cmake-3.25.0-linux-x86_64.tar.gz\" | \\\n    tar --strip-components=1 -xz -C /usr/local\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/Bigfoot71/r3d.git . && \\\n    git checkout 6d5d96b804c9b8ec19f69a9a7d908b4d2cc77113 && \\\n    git submodule update --init --recursive\n\n# Make sure the Python scripts are executable\nRUN chmod +x scripts/bin2c.py scripts/glsl_minifier.py\n\n# Fix the shader embedding issue by ensuring the build process can find the embedded shaders\nRUN mkdir -p build && \\\n    cd build && \\\n    cmake .. -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=$(which python3) && \\\n    cd ../embedded/shaders && \\\n    python3 ../../scripts/glsl_minifier.py . && \\\n    cd ../../build && \\\n    cmake --build . -j$(nproc) || echo \"Build completed with some warnings\"\n\n# Set the working directory back to the project root\nWORKDIR /app\n\n# The container is now ready with the r3d library built\nCMD [\"/bin/bash\"]", "language": "c"}
{"number": 1, "title": "希望增加文字长度上限", "created_at": "2024-12-10T09:29:12Z", "closed_at": "2024-12-11T04:55:56Z", "commit_id": "483a2e9d7e38bec610e9f6a9f3026241fdfddc14", "labels": ["enhancement"], "url": "https://github.com/bestZwei/ciallo-tts/issues/1", "body": "目前有字符限制，大概看了一下您的代码\r\n发现请求api使用的是get方法，长度过长会报error\r\n期待您的优化\r\n", "comments_url": "https://api.github.com/repos/bestZwei/ciallo-tts/issues/1/comments", "author": "uniqueww", "comments": [{"user": "bestZwei", "created_at": "2024-12-10T13:48:34Z", "body": "问题是，api 返回的音频最长10分钟"}, {"user": "bestZwei", "created_at": "2024-12-10T17:38:49Z", "body": "你试试，做了个智能分段，2500中文字符，或者5000其他字符，分成一段。长文本将自动切分，优先根据分段-句号-逗号切分"}, {"user": "uniqueww", "created_at": "2024-12-11T00:37:08Z", "body": "> 你试试，做了个智能分段，2500中文字符，或者5000其他字符，分成一段。长文本将自动切分，优先根据分段-句号-逗号切分\r\n\r\n好的，原来是api的返回限制了字符，我尝试优化一下，感谢你的回复\r\n"}], "satisfaction_conditions": ["A solution that handles text input exceeding the character limit", "Automatic text segmentation that preserves meaning", "Understanding of the underlying API limitations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:01"}, "dockerfile": null, "language": "c"}
{"number": 1, "title": "no BTF found for kernel version 5.10.43", "created_at": "2025-03-18T10:30:43Z", "closed_at": "2025-03-18T12:55:40Z", "commit_id": "b1b05ff2a5be4485c2a450024a56b37d87b67c91", "labels": [], "url": "https://github.com/ShinoLeah/eDBG/issues/1", "body": "Hello, thank you so much for making and sharing this tool! :)\n\nI encountered an issue and would appreciate some help. \n\n\n**./eDBG -p packagename -l somelib.so -b 0x66758**\n\nModule start Failed:  ProbeHandler.Run(): couldn't init manager error:program probe_9: apply CO-RE relocations: no BTF found for kernel version 5.10.43-android12-9-00007-g9771767708df-ab8009062: not supported , couldn't load eBPF programs, cs:&{map[event_map:PerCPUArray(keySize=4, valueSize=288, maxEntries=1, flags=0) events:PerfEventArray(keySize=4, valueSize=4, maxEntries=8, flags=0)] map[probe_0:0x4000642630 probe_1:0x4000642a20 probe_10:0x40006421b0 probe_11:0x40006427e0 probe_12:0x4000642bd0 probe_13:0x4000642c60 probe_14:0x4000642870 probe_15:0x4000642240 probe_16:0x4000642cf0 probe_17:0x4000642900 probe_18:0x4000642990 probe_19:0x4000642510 probe_2:0x4000642ab0 probe_20:0x40006425a0 probe_3:0x40006426c0 probe_4:0x40006422d0 probe_5:0x4000642b40 probe_6:0x4000642360 probe_7:0x40006423f0 probe_8:0x4000642750 probe_9:0x4000642480] 0x4000096000 LittleEndian}\n\n**uname -a**\n\nLinux localhost 5.10.43-android12-9-00007-g9771767708df-ab8009062 #1 SMP PREEMPT Thu Dec 16 04:22:18 UTC 2021 aarch64\n\n**zcat /proc/config.gz | grep \"KALLSYMS\"**\n\nCONFIG_KALLSYMS=y\nCONFIG_KALLSYMS_ALL=y\nCONFIG_KALLSYMS_BASE_RELATIVE=y\n\n\nI'm running on a pixel 6, and I manage to run eBPF command line tools like stackPLZ and eCapture. \nIf it helps, my phone is rooted with Apatch.\n\nAny idea what seems to be the problem?\n\nThanks again!", "comments_url": "https://api.github.com/repos/ShinoLeah/eDBG/issues/1/comments", "author": "noobexon1", "comments": [{"user": "ShinoLeah", "created_at": "2025-03-18T10:47:27Z", "body": "When loading eBPF bytecode on machines where the kernel option CONFIG_DEBUG_INFO_BTF is not enabled, additional BTF files need to be loaded. The project currently does not support this scenario, but I will address this issue shortly."}, {"user": "noobexon1", "created_at": "2025-03-18T11:03:56Z", "body": "Thank you so much! I appreciate it :)"}, {"user": "ShinoLeah", "created_at": "2025-03-18T12:55:40Z", "body": "Fixed on v1.2.1"}], "satisfaction_conditions": ["A solution that enables the tool to work on kernels without BTF support", "A fix that works specifically on the user's Android device (Pixel 6)", "A solution that allows the tool to run despite the kernel configuration limitations", "A timely resolution to the reported issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:59"}, "dockerfile": null, "language": "c"}
{"number": 2, "title": "Link with GCC", "created_at": "2025-01-14T22:06:16Z", "closed_at": "2025-01-24T20:19:58Z", "commit_id": "3bb30eb33783f3bbdf471c5b43cb1944138804b3", "labels": [], "url": "https://github.com/tsoding/coroutines/issues/2", "body": "You needn't link manually with ld. Everything links and runs fine with GCC so long as you provide the `-no-pie` flag when linking the `.o` files:\r\n\r\n```Makefile\r\nmain: coroutine.o main.o\r\n    gcc -no-pie -o main main.o coroutine.o\r\n```\r\n\r\nI remember you typing `-fno-pic` on stream instead, which is why it didn't work.", "comments_url": "https://api.github.com/repos/tsoding/coroutines/issues/2/comments", "author": "El-Wumbus", "comments": [{"user": "rexim", "created_at": "2025-01-24T20:19:56Z", "body": "Thank you for the suggestion. Resolved by f6edda4f6fdc97310061ca28e97eb71aaeb31f25"}], "satisfaction_conditions": ["A working solution for linking object files with GCC instead of manual linking with ld", "Proper flag identification for preventing position-independent code generation during linking", "A working Makefile configuration for the build process"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:42"}, "dockerfile": null, "language": "c"}
{"number": 22, "title": "Support for lib / fun / c struct / global vars / type", "created_at": "2024-12-10T06:28:25Z", "closed_at": "2024-12-12T14:38:03Z", "commit_id": "83b07256a2fbf5044c779cf404fca436191bbbf3", "labels": [], "url": "https://github.com/crystal-lang-tools/tree-sitter-crystal/pull/22", "body": "Closes #18\r\n\r\nStill some edge cases and may not have done everything the best way, feedback welcome.\r\n\r\nWith this, we're at ~60% stdlib coverage.", "comments_url": "https://api.github.com/repos/crystal-lang-tools/tree-sitter-crystal/issues/22/comments", "author": "nobodywasishere", "comments": [{"user": "keidax", "created_at": "2024-12-10T17:28:01Z", "body": "By getting rid of the changes in `conflicts`, I got this  conflict:\r\n\r\n```\r\nUnresolved conflict for symbol sequence:\r\n\r\n  'fun'  identifier  '('  identifier  •  ')'  …\r\n\r\nPossible interpretations:\r\n\r\n  1:  'fun'  identifier  '('  (_expression  identifier)  •  ')'  …\r\n  2:  'fun'  identifier  '('  (fun_param  identifier)  •  ')'  …\r\n\r\nPossible resolutions:\r\n\r\n  1:  Specify a higher precedence in `_expression` than in the other rules.\r\n  2:  Specify a higher precedence in `fun_param` than in the other rules.\r\n  3:  Add a conflict for these rules: `_expression`, `fun_param`\r\n```\r\n\r\nThis is confusing at first. Tree-sitter is supposed to only allow tokens in valid locations, so why would `$._expression` be valid in the parameter list?\r\n\r\nAfter considering `$.top_level_fun_def` I realized what's happening: if the `fun` definition doesn't have any params, then the body could start immediately. In that case, `( _expression )` could be legitimately parsed as a parenthesized statement.\r\n\r\nThese are both valid syntax (and we'll want test cases to distinguish them):\r\n``` crystal\r\n# equivalent to a_little_fun()\r\nfun a_little_fun;\r\n(a : Int32)\r\nend\r\n\r\n# equivalent to more_fun(a : Int32)\r\nfun more_fun\r\n(a : Int32)\r\nend\r\n```\r\n\r\nNote that this is the opposite of how `def`s are parsed:\r\n\r\n``` crystal\r\n# equivalent to a_def()\r\ndef a_def\r\n(a : Int32)\r\nend\r\n```\r\n\r\nI think this should be sufficient to resolve the conflict:\r\n\r\n----------------------------------------\r\n\r\n``` javascript\r\n    top_level_fun_def: $ => {\r\n      const params = seq(\r\n        '(', field('params', $.fun_param_list), ')',\r\n      )\r\n      const real_name = seq('=',\r\n        field('real_name', choice($.identifier, $.constant, $.string)),\r\n      )\r\n      const return_type = field('type', seq(/[ \\t]:\\s/, $._bare_type))\r\n\r\n      return seq(\r\n        prec.right(seq(\r\n          'fun',\r\n          field('name', $.identifier),\r\n          optional(real_name),\r\n          optional(params),\r\n          optional(return_type),\r\n        )),\r\n        field('body', optional($._statements)),\r\n        'end',\r\n      )\r\n    },\r\n\r\n    // [...]\r\n\r\n    fun_param: $ => {\r\n      const type = field('type', seq(/[ \\t]:\\s/, $._bare_type))\r\n\r\n      return seq(\r\n        choice($.identifier, $.constant),\r\n        type,\r\n      )\r\n    },\r\n```\r\n\r\nThe two key changes are making the type mandatory on `fun_param`, and using `prec.right`.\r\n\r\nMaking the type mandatory means that `fun foo ( a )` isn't ambiguous any more -- `a` can't be a parameter because it doesn't have a type attached.\r\n\r\n`prec.right` ensures that `fun identifier (<possible param list>)` is always parsed as a param list and not a parenthesized statement.\r\n\r\n(I also noticed that extracting the top-level fun signature to its own rule helps, but I can't really explain why. This is where my understanding of LR parsers gets shaky.)\r\n\r\nThis still doesn't perfectly match what the Crystal parser is doing. For example, with the above change, tree-sitter parses this fine:\r\n``` crystal\r\nfun foo\r\n(a)\r\nend\r\n\r\n# (top_level_fun_def\r\n#   name: (identifier)\r\n#   body: (identifier))\r\n```\r\nBut Crystal doesn't accept it:\r\n```\r\nIn scratch.cr:27:3\r\n\r\n 27 | (a)\r\n        ^\r\nError: expecting token ':', not ')'\r\n```\r\n\r\nFor now, I would say that being more lenient is good. But it's worth exploring some more edge cases around how `fun` gets parsed.\r\n"}, {"user": "nobodywasishere", "created_at": "2024-12-10T21:29:48Z", "body": "Thank you for this write-up and the fix for the issue! I really appreciate it. If we write up a blog post about the tree sitter parser when it's more done, I think it'd be cool for that to be a part of it as a deeper dive into the edge cases of Crystal syntax."}], "satisfaction_conditions": ["A clear explanation of the parsing conflict and why it occurs", "A working solution to resolve the parsing conflict", "Technical insights that could be educational for future reference", "Detailed analysis of edge cases in the language syntax"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:37"}, "dockerfile": null, "language": "c"}
{"number": 99, "title": "Avoid including internal libintelmath header.", "created_at": "2025-03-07T16:20:32Z", "closed_at": "2025-03-12T19:02:01Z", "commit_id": "f237956efbf871176b9cd6b6b85f694c2f7fed4d", "labels": [], "url": "https://github.com/microsoft/documentdb/pull/99", "body": "This removed the include of <bid_internal.h> and adds the three MASK64 definitions that are used in the code.\r\n\r\nFixes: #97", "comments_url": "https://api.github.com/repos/microsoft/documentdb/issues/99/comments", "author": "mbanck-cd", "comments": [{"user": "lichoil", "created_at": "2025-03-07T17:55:19Z", "body": "hi @mbanck-ntap ,may I know the reason why removing this header file out and put MASK64 definitions in stead?"}, {"user": "safern", "created_at": "2025-03-07T19:56:07Z", "body": "@lichoil I think it is to be able to compile in debian. \r\n\r\n@diipak-bisht thoughts on this change? "}, {"user": "mbanck", "created_at": "2025-03-08T08:09:31Z", "body": "> @lichoil I think it is to be able to compile in debian.\r\n\r\nExactly.\r\n\r\n"}, {"user": "diipak-bisht", "created_at": "2025-03-10T05:14:47Z", "body": "I think this is fine change if internal headers are not available in debian unstable @safern. Thanks for contributing this @mbanck "}], "satisfaction_conditions": ["A solution that enables compilation in Debian without requiring internal headers", "A way to maintain the necessary functionality while removing dependency on bid_internal.h", "An approach that addresses the issue (#97) related to internal header dependencies"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:16"}, "dockerfile": null, "language": "c"}
{"number": 3, "title": "error compiling sim.c", "created_at": "2025-04-02T21:49:57Z", "closed_at": "2025-04-03T13:41:01Z", "commit_id": "33e86b34aeb83908940d893f13d3b1590ce311a7", "labels": [], "url": "https://github.com/AncientJames/multivox/issues/3", "body": "Hello James,\nthis is an amazing project you have made here.\nI have issues compiling the code on my raspberry pi 5 unsucessful with the following error:\n/home/rpi5/multivox/src/simulator/sim.c:14:10: fatal error: **GLES3/gl3.h**: No such file or directory.\nI will be grateful for any advice on how to install the open-gl libraries or skip compiling your simulator.", "comments_url": "https://api.github.com/repos/AncientJames/multivox/issues/3/comments", "author": "IljaRukin", "comments": [{"user": "AncientJames", "created_at": "2025-04-02T23:01:55Z", "body": "From  memory, the gl libraries can be installed via `sudo apt install libgles-dev libegl-dev`\n\nFailing that, the simulator is the last section in the `CMakeLists.txt` - comment out everything from `set(SIMULATOR_SRC_DIR ${SRC_DIR}/simulator)`\n\n"}, {"user": "IljaRukin", "created_at": "2025-04-03T13:41:01Z", "body": "Thank you for your fast response !\nInstalling the librarier you mentioned solved the issue."}], "satisfaction_conditions": ["Instructions for installing the missing OpenGL libraries on Raspberry Pi", "A workaround to bypass simulator compilation if library installation isn't possible", "A prompt response that addresses the specific compilation error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:07"}, "dockerfile": null, "language": "c"}
{"number": 96, "title": "[Bug]: 当用 PotPlayer 视频播放器全屏播放视频时，置顶的时间会被挡掉", "created_at": "2025-04-02T14:38:01Z", "closed_at": "2025-04-07T09:03:06Z", "commit_id": "9867b68c05235778715a14bdc60d86f37986f684", "labels": [], "url": "https://github.com/vladelaina/Catime/issues/96", "body": "希望在用 PotPlayer 全屏播放视频时，不会挡掉置顶的时间\n\nPotPlayer 和 Catime 均为最新版", "comments_url": "https://api.github.com/repos/vladelaina/Catime/issues/96/comments", "author": "Drink-medicine", "comments": [{"user": "vladelaina", "created_at": "2025-04-02T14:58:15Z", "body": "> 希望在用 PotPlayer 全屏播放视频时，不会挡掉置顶的时间\n> \n> PotPlayer 和 Catime 均为最新版\n\n非常感谢反馈🍻"}, {"user": "Drink-medicine", "created_at": "2025-04-07T09:02:34Z", "body": "> > 希望在用 PotPlayer 全屏播放视频时，不会挡掉置顶的时间\n> > PotPlayer 和 Catime 均为最新版\n> \n> 非常感谢反馈🍻\n\n问题已解决，这个应该不算 bug，在 Potplayer 配置里可以设置这个软件的置顶方式，改一下就解决了 Potplayer 总是在最前端的情况了。抱歉之前没有仔细研究，麻烦作者了 💦💦💦\n软件非常好用，感谢作者的开发 ❤️❤️❤️"}, {"user": "vladelaina", "created_at": "2025-04-07T13:38:47Z", "body": "> > > 希望在用 PotPlayer 全屏播放视频时，不会挡掉置顶的时间\n> > > PotPlayer 和 Catime 均为最新版\n> > \n> > \n> > 非常感谢反馈🍻\n> \n> 问题已解决，这个应该不算 bug，在 Potplayer 配置里可以设置这个软件的置顶方式，改一下就解决了 Potplayer 总是在最前端的情况了。抱歉之前没有仔细研究，麻烦作者了 💦💦💦 软件非常好用，感谢作者的开发 ❤️❤️❤️\n\n💖💖💖没事哒没事哒~非常感谢认可🍻"}], "satisfaction_conditions": ["A solution that prevents PotPlayer from blocking the pinned time display when in fullscreen mode", "A configuration approach that allows both applications to coexist visually", "Information about how to adjust window layering or z-order priority between applications"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:41"}, "dockerfile": null, "language": "c"}
{"number": 2, "title": "detect aerospace path", "created_at": "2025-02-04T17:48:20Z", "closed_at": "2025-02-04T21:50:04Z", "commit_id": "35c94469dd57da34bacfa1f299e9047b3ec9e4c1", "labels": [], "url": "https://github.com/acsandmann/aerospace-swipe/pull/2", "body": "closes #1 ", "comments_url": "https://api.github.com/repos/acsandmann/aerospace-swipe/issues/2/comments", "author": "acsandmann", "comments": [{"user": "acsandmann", "created_at": "2025-02-04T17:48:48Z", "body": "@FormalSnake can you try this?"}, {"user": "FormalSnake", "created_at": "2025-02-04T21:46:37Z", "body": "That works beautifully but for some reason not when I have it installed using \"make install\", idk if it is because it isnt running afterwards or smth."}, {"user": "acsandmann", "created_at": "2025-02-05T01:00:14Z", "body": "@FormalSnake I think the latest commit fixed the issue with make install. When installing from the makefile it automatically adds it into launchctl and I think in that context it doesn't have access to where/which so I switched it to `command -v aerospace` and that seems to be working. "}], "satisfaction_conditions": ["A solution that works when installed via 'make install'", "A method to properly detect the aerospace path in different execution contexts", "Compatibility with the system's launch control mechanism"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:53"}, "dockerfile": null, "language": "c"}
{"number": 2, "title": "No rule \"install\" in makefile", "created_at": "2025-03-22T10:50:24Z", "closed_at": "2025-03-22T11:33:26Z", "commit_id": "0951198b40e7b40c608cd056ca641afefa6da596", "labels": [], "url": "https://github.com/alfiecg24/KextRW/issues/2", "body": "Hello, thank you for your work.\n\nIn the README.md, you tell to follow install instructions from IOKernelRW.\nIn their installation procedure, they suggest to use `make install`to install the kext, but here, there is no ìnstall` rule in the Makefile.\n\nBest regards.", "comments_url": "https://api.github.com/repos/alfiecg24/KextRW/issues/2/comments", "author": "alexandredoyen29", "comments": [{"user": "alfiecg24", "created_at": "2025-03-22T11:06:08Z", "body": "I have no idea where that went, sorry! It was definitely there before - I must have accidentally removed it. You can instead manually install it by running `sudo cp -R KextRW.kext /Library/Extensions`."}, {"user": "alexandredoyen29", "created_at": "2025-03-22T11:19:19Z", "body": "Yep, I did this"}], "satisfaction_conditions": ["An alternative method to install the kext when the make install command is unavailable", "Clear instructions that can be executed to install the kext", "Information that addresses the discrepancy between the README and the actual Makefile"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:30"}, "dockerfile": null, "language": "c"}
{"number": 2, "title": "Unable to rotate the gizmo without the scaling flag", "created_at": "2025-03-23T05:14:47Z", "closed_at": "2025-03-23T09:48:55Z", "commit_id": "ec3d59e4113ef1d5491ab914cc590a06ba61e1ce", "labels": [], "url": "https://github.com/cloudofoz/raylib-gizmo/issues/2", "body": "If I draw the gizmo using the following flags, i am unable to rotate it (the visuals change when i click the circle but it does not rotate): \n```cpp\nDrawGizmo3D(GIZMO_ROTATE  , &transform);\nDrawGizmo3D(GIZMO_ROTATE | GIZMO_TRANSLATE , &transform);\n```\nWhile if we draw the gizmo with the following flags it does rotate:\n```cpp\nDrawGizmo3D(GIZMO_ROTATE | GIZMO_SCALE , &transform);\nDrawGizmo3D(GIZMO_ROTATE | GIZMO_SCALE | GIZMO_TRANSLATE , &transform);\n```\n\nFrom what I tried to understand from the source code is that the gizmo detects it has to do a rotation but (i.e. the action is correctly registered), but it does not apply any rotation from the mouse movement.", "comments_url": "https://api.github.com/repos/cloudofoz/raylib-gizmo/issues/2/comments", "author": "eduardo98m", "comments": [{"user": "cloudofoz", "created_at": "2025-03-23T05:37:32Z", "body": "Hi @eduardo98m!\n\nLet me explain what's happening.\n\n**Short answer:** To make gizmo \"rotation\" work when scaling isn't enabled, just add the local orientation flag:  \n```cpp\nGIZMO_TRANSLATE | GIZMO_ROTATE | GIZMO_LOCAL\n```\n\n**Long answer:**  \n`raylib-gizmo` supports three orientation modes:  \n- **Global**: The gizmo doesn't align with the transform's orientation.  \n- **Local**: The gizmo aligns with the object's transform.  \n- **View**: The gizmo faces the camera/view.\n\nBy default, the gizmo uses **global** orientation. However, as mentioned in the README, when **scaling is enabled**, the gizmo *requires* local orientation, so it automatically switches to **local** mode in those cases.\n\nThat's why when you use:\n```cpp\nGIZMO_ROTATE | GIZMO_SCALE\n```\nit behaves as expected because local mode is active. But without the scale component, it stays in global mode by default, and rotation won’t apply as you’d expect. To fix that, just explicitly enable `GIZMO_LOCAL`.\n\nLet me know if that clears things up!"}, {"user": "eduardo98m", "created_at": "2025-03-23T09:48:55Z", "body": "Thanks for the explanation! (I was not really paying attention to the global and local mode flags ._.).\n\nI also happen to look for the specific line in the source code that forced the flag change for the scaling.\n\n`raygizmo.c` line : `494`\n```cpp\nstatic void ComputeAxisOrientation(GizmoData* gizmoData)\n{\n\tint flags = gizmoData->flags;\n\n\t// Scaling is currently supported only in local mode\n\tif (flags & GIZMO_SCALE)\n\t{\n\t\tflags &= ~GIZMO_VIEW;\n\t\tflags |= GIZMO_LOCAL;\n\t}\n...\n```\n\nNote: I wasn't noticing the change in the transform rotation because i was using the gizmo without any model attached.\n\nAgain thanks for your help, really like this project."}], "satisfaction_conditions": ["An explanation of why the rotation gizmo doesn't work without the scaling flag", "A solution to make the rotation gizmo work without requiring the scaling flag", "Information about the orientation modes of the gizmo system", "Reference to relevant implementation details in the source code"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:00:12"}, "dockerfile": "FROM ubuntu:22.04\n\n# Set environment variables and avoid interactive prompts\nENV DEBIAN_FRONTEND=noninteractive\n\n# Add labels for metadata\nLABEL maintainer=\"Docker Builder\"\nLABEL description=\"Environment for building and validating raylib-gizmo\"\n\n# Install dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    git \\\n    cmake \\\n    pkg-config \\\n    libgl1-mesa-dev \\\n    libx11-dev \\\n    libxrandr-dev \\\n    libxi-dev \\\n    libxcursor-dev \\\n    libxinerama-dev \\\n    libasound2-dev \\\n    libwayland-dev \\\n    libxkbcommon-dev \\\n    libdecor-0-dev \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a directory for the project\nWORKDIR /app\n\n# Clone raylib repository and build it\nRUN git clone https://github.com/raysan5/raylib.git && \\\n    cd raylib && \\\n    git checkout 5.0 && \\\n    mkdir build && \\\n    cd build && \\\n    cmake -DBUILD_SHARED_LIBS=OFF -DCMAKE_BUILD_TYPE=Release .. && \\\n    make -j$(nproc) && \\\n    make install && \\\n    ldconfig\n\n# Clone the raylib-gizmo repository and checkout the specific commit\nRUN git clone https://github.com/cloudofoz/raylib-gizmo.git && \\\n    cd raylib-gizmo && \\\n    git checkout ec3d59e4113ef1d5491ab914cc590a06ba61e1ce\n\n# Build the examples to validate the project\nWORKDIR /app/raylib-gizmo\nRUN gcc -o example_01 examples/gizmo/example_01_getting_started.c src/raygizmo.c -I./src -lraylib -lGL -lm -lpthread -ldl -lrt -lX11 && \\\n    gcc -o example_02 examples/gizmo/example_02_gizmo_types.c src/raygizmo.c -I./src -lraylib -lGL -lm -lpthread -ldl -lrt -lX11\n\n# Set the working directory to the project root\nWORKDIR /app/raylib-gizmo\n\n# The container is ready to be used for validation\nCMD [\"bash\"]", "language": "c"}
{"number": 155, "title": "#113: Semantic Kernel", "created_at": "2025-02-06T04:43:11Z", "closed_at": "2025-02-25T06:57:30Z", "commit_id": "b581325fc3d0717d4284142330e0b08016c0dabf", "labels": [], "url": "https://github.com/microsoft/ai-dev-gallery/pull/155", "body": "fixes #113 \r\n\r\nNeeded to add a dependency and update another for this, so would appreciate a double check that nothing went awry.\r\n\r\nAlso, this sample takes *forever* to load. Not sure if there is any way around it.", "comments_url": "https://api.github.com/repos/microsoft/ai-dev-gallery/issues/155/comments", "author": "zateutsch", "comments": [{"user": "zateutsch", "created_at": "2025-02-06T19:44:55Z", "body": "@nmetulev this sample has some problems, I'm investigating"}, {"user": "azchohfi", "created_at": "2025-02-07T22:27:52Z", "body": "@zateutsch I've fixed the sample with a more generic solution (using IChatClient's overload). This will also work well will PhiSilica, so its only a plus :) The package you were referencing have its own implementation of the equivalent of IChatClient for ORT, so we should not use it (it was fixed to the CPU version)."}, {"user": "zateutsch", "created_at": "2025-02-07T23:23:53Z", "body": "I don't think we should merge this until I've double checked that that memory leak is gone"}, {"user": "zateutsch", "created_at": "2025-02-10T21:02:58Z", "body": "> I don't think we should merge this until I've double checked that that memory leak is gone\r\n\r\nOkay, I took a look at this and everything seems to get garbage collected a lot quicker with `AsChatCompletionService`. Something about cancellation during `Unloaded` is still weird with this sample, and it stays in memory for 10-15s compared to the other samples that get collected almost right away. This only happens if you navigate during generation, it works as expected if the sample is idle and you navigate.\r\n\r\nI think it should be fine to merge how it is now. @nmetulev "}], "satisfaction_conditions": ["A solution that eliminates or significantly reduces memory leaks in the Semantic Kernel sample", "A more generic implementation approach that works with multiple models/services", "Proper dependency management that doesn't cause other issues", "Acceptable performance characteristics for the sample"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:00:41"}, "dockerfile": "FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build\n\n# Install necessary tools and dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wine64 \\\n    mono-complete \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /source\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/microsoft/ai-dev-gallery.git . && \\\n    git checkout b581325fc3d0717d4284142330e0b08016c0dabf\n\n# Enable Windows targeting for cross-platform builds\nENV EnableWindowsTargeting=true\nENV DOTNET_CLI_TELEMETRY_OPTOUT=1\n\n# Configure Wine for Windows executables\nRUN mkdir -p /root/.wine && \\\n    winecfg\n\n# Build only specific projects that don't require Windows-specific components\nRUN dotnet build AIDevGallery.Utils/AIDevGallery.Utils.csproj --configuration Release\nRUN dotnet build AIDevGallery.SourceGenerator/AIDevGallery.SourceGenerator.csproj --configuration Release\n\n# Create a smaller final image\nFROM mcr.microsoft.com/dotnet/sdk:9.0-alpine\n\nWORKDIR /app\n\n# Copy built artifacts from the build stage\nCOPY --from=build /source/AIDevGallery.Utils/bin/Release /app/AIDevGallery.Utils/bin/Release\nCOPY --from=build /source/AIDevGallery.SourceGenerator/bin/Release /app/AIDevGallery.SourceGenerator/bin/Release\nCOPY --from=build /source/AIDevGallery /app/AIDevGallery\n\n# Set the entry point to a shell so the container stays running\nCMD [\"/bin/sh\"]", "language": "c#"}
{"number": 66, "title": "Open sln by vs and press F5, but got the error report", "created_at": "2025-01-02T06:41:02Z", "closed_at": "2025-01-06T03:41:44Z", "commit_id": "86837c7a77b83c66f918136b2cfb5431711370dc", "labels": [], "url": "https://github.com/microsoft/ai-dev-gallery/issues/66", "body": "Cloned the source code, opened sln by vs, but got the error as below after press F5:\n\nThe project does not know how to run the configuration file named \"AIDevGallery (Package)\" with the command \"MsixPackage\".\n\nPlease guide me how to continue, thank you!", "comments_url": "https://api.github.com/repos/microsoft/ai-dev-gallery/issues/66/comments", "author": "kinghighland", "comments": [{"user": "nmetulev", "created_at": "2025-01-02T16:14:54Z", "body": "Hi, please make sure you are on the latest version of Visual Studio and it's fully updated. Also make sure you have the \"Windows application development\" workload installed. \n\nIf this doesn't work, please provide more info about your version of OS, VS, and what workloads you have installed."}, {"user": "kinghighland", "created_at": "2025-01-06T03:41:44Z", "body": "thank you, it works after reinstalled the Windows Application Development."}], "satisfaction_conditions": ["Instructions for resolving the Visual Studio configuration error when running the project", "Guidance on required Visual Studio components or workloads needed to run the project", "Troubleshooting steps for Visual Studio project configuration issues"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:47"}, "dockerfile": null, "language": "c#"}
{"number": 57, "title": "[a11y BUG] Narrator is not announcing the generating output information after invoking the Generate button in the Generate button.", "created_at": "2024-12-13T17:44:58Z", "closed_at": "2025-01-22T23:06:44Z", "commit_id": "3eae6d33d0bc9c264634c06bd222fa718c0bdd35", "labels": ["🐛bug"], "url": "https://github.com/microsoft/ai-dev-gallery/issues/57", "body": "Repro Steps:\nLaunch AI Dev Gallery App\nNavigate to samples tab and invoke it.\nNavigate to Text Drop down and invoke it\nNavigate to Generate and invoke it\nNow observe the behavior\nActual Result:\nNarrator is not announcing the generating output information after invoking the Generate button in the Generate button.\nObservation: Narrator is kept remains silent, upon invoking the generate button\nNote: This issue is observed throughout the App for All Models in the samples tab.\n\nExpected Result:\nNarrator should announce the generating output information after invoking the Generate button in the Generate button.\n\nUser Impact: '\nUsers with low vision who rely on screen reader will be impacted if Narrator is not announcing the generating output information after invoking the Generate button in the Generate button.\n\nFix: Have the narrator read the generated text", "comments_url": "https://api.github.com/repos/microsoft/ai-dev-gallery/issues/57/comments", "author": "gregwoo-microsoft", "comments": [{"user": "Jaylyn-Barbee", "created_at": "2025-01-22T16:21:24Z", "body": "Our current experience is\n1. Use invokes one of our generative text samples\n2. Narrator: \"Generating content please wait\"\n3. Narrator: \"Content has started generating\"\n4. Narrator: \"Content has finished generating\" \n5. We automatically focus the text block\n6. At this point the user should enter scan mode to have the Narrator read the text\n\nAutomatically having Narrator read the text creates a situation where we can't stop the text from being read out. Users have complete control of this in scan mode."}, {"user": "gregwoo-microsoft", "created_at": "2025-01-22T23:06:44Z", "body": "Closing this bug as the current experience detailed above by @Jaylyn-Barbee is the best outcome."}], "satisfaction_conditions": ["Appropriate screen reader feedback during content generation process", "Accessible way for screen reader users to consume generated content", "User control over screen reader behavior", "Clear explanation of the existing accessibility workflow"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:51"}, "dockerfile": null, "language": "c#"}
{"number": 34, "title": "'System.TypeLoadException' in DevWinUI.Controls.dll launching app with Win SDK 1.7 Preview 1", "created_at": "2025-02-12T21:43:30Z", "closed_at": "2025-02-13T15:15:12Z", "commit_id": "b35e9f62f27c644564393f620aeda72ab4160681", "labels": [], "url": "https://github.com/ghost1372/DevWinUI/issues/34", "body": "Hi!  I am getting 'System.TypeLoadException' in DevWinUI.Controls.dll launching app with Win SDK 1.7 Preview 1, app won't launch.  Known issue?  Thanks!", "comments_url": "https://api.github.com/repos/ghost1372/DevWinUI/issues/34/comments", "author": "MPITech", "comments": [{"user": "ghost1372", "created_at": "2025-02-12T22:23:24Z", "body": "Hi, are you using v8.0.0-preview6 ? Or are you using the source code directly?\nI'll look into this further tomorrow, I have to sleep now it is 2-AM. (sorry 😅)\nI didn't see any problems while testing some new code today. (I used it directly.)"}, {"user": "MPITech", "created_at": "2025-02-12T22:27:47Z", "body": "Hi, goodnight!  Sorry I didn't mention, yes, I am using 8.0.0 preview 6 and I see a dozen of those same errors in the Immediate Window in VS and the app crashes without showing the main window.  When I reverted back to 1.7 exp 3, it was fine again.  Thanks!  (I am not using the source code directly, just the nuget packages)"}, {"user": "MPITech", "created_at": "2025-02-12T22:36:19Z", "body": "Upon further examination, I am seeing \"Exception thrown: 'System.TypeLoadException' in DevWinUI.Controls.dll\" a dozen times in the immediate window even with SDK 1.7 exp 3 and the application still loads okay.\n\nI checked the event viewer and the crash in my app with SDK 1.7 preview 1 is actually happening in Microsoft.UI.Xaml.dll, version: 3.1.7.0,  So it is likely not your issue, I just never noticed those DevWinUI.Controls.dll entries before when using SDK 1.7 exp 3 but I guess they have always been there.  Is there a way I can get more info for you on that error short of sending source code, which I am not permitted to do?  Thanks!"}, {"user": "ghost1372", "created_at": "2025-02-12T22:43:15Z", "body": "I will be releasing a new version soon based on Preview 1. I am finalizing some things and if all goes well I will release it tomorrow."}, {"user": "MPITech", "created_at": "2025-02-12T22:46:16Z", "body": "Ok thanks!"}, {"user": "ghost1372", "created_at": "2025-02-13T15:15:12Z", "body": "Hi @MPITech \nUsing both WSSDK v1.7- Preview and Experimental at the same time in the same project causes this problem, v8.0.0-Preview 7 is based on WASDK v1.7-Preview 1. And there is no problem.\n\nUnfortunately, at the last minute, while testing the new codes, I realized that StartupHelper class was not working properly for packaged applications, but as I promised you, I uploaded the new version.\nAnd the StartupHelper problem will be fixed in the next version."}, {"user": "MPITech", "created_at": "2025-02-13T15:23:01Z", "body": "Hi, I wasn't using 1.7 preview and experimental at the same time, but DevWinUI Preview 7 and SDK 1.7 Preview 1 together are now fine and my app no longer crashes on startup, thanks a lot!\n\nI do see one error on launch in the immediate window on startup now:  Exception thrown: 'System.InvalidCastException' in DevWinUI.dll that wasn't there before.  Is there anything I can do to get you more information for that?  I don't see any functionality problems yet."}, {"user": "ghost1372", "created_at": "2025-02-13T15:27:52Z", "body": "> Hi, I wasn't using 1.7 preview and experimental at the same time, but DevWinUI Preview 7 and SDK 1.7 Preview 1 together are now fine and my app no longer crashes on startup, thanks a lot!\n> \n> I do see one error on launch in the immediate window on startup now: Exception thrown: 'System.InvalidCastException' in DevWinUI.dll that wasn't there before. Is there anything I can do to get you more information for that? I don't see any functionality problems yet.\n\nThis problem was caused because DevWinUI was using the experimental version and you were using the preview version.\n It's good that the problem has been fixed.\n\nIt might be because of the HomeLandingPage, I made some changes to it to remove the dependency on the communityToolkit package. \n\nYou can check if the error still occurs by disabling the HomeLandingPage, but it is a bit difficult to check because the crash does not occur."}, {"user": "ghost1372", "created_at": "2025-02-13T16:23:16Z", "body": "@MPITech i tested DevWinUI.GalleryApp and i did not see any  **InvalidCastException**, can you provide a sample repro project?"}, {"user": "MPITech", "created_at": "2025-02-13T17:35:14Z", "body": "@ghost1372 I will do my best, I am at work right now but I will see if I can replicate in a small project when I can.  TY"}], "satisfaction_conditions": ["A solution that resolves the application crash when using Win SDK 1.7 Preview 1", "Compatibility between DevWinUI and the latest Windows SDK versions", "Clear identification of the source of the error", "Timely updates to the library that address compatibility issues"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:36"}, "dockerfile": null, "language": "c#"}
{"number": 10, "title": "[Question] Best practice for ThemeService in multi-window application?", "created_at": "2024-12-07T02:06:40Z", "closed_at": "2024-12-07T12:14:24Z", "commit_id": "c90a62eac0148a09b0ea67c9c0aa569d1c4a7310", "labels": ["enhancement"], "url": "https://github.com/ghost1372/DevWinUI/issues/10", "body": "Hi,\r\n\r\nWhen using the ThemeService, it must be initialized with the window as a parameter.  In a multi-window app, should a new ThemeService be created for each window and its settings set to the main application window's settings?  Or should the main window's ThemeService just call .Initialize() on the newly created window?\r\n\r\nAlso, what is the best method for applying changes to the settings (tint color, backdrop) to all open windows?  If the above answer is to create a separate TheemService for each window, should I just apply the settings to each window's ThemeService in a loop?\r\n\r\nThanks!", "comments_url": "https://api.github.com/repos/ghost1372/DevWinUI/issues/10/comments", "author": "MPITech", "comments": [{"user": "ghost1372", "created_at": "2024-12-07T04:57:30Z", "body": "It is not good to create multiple instances of a class for each window.\nYou can get backdrop and ElementTheme from ThemeService and manually set it in window loaded/ctor method.\nI am considering adding multi-window support to the ThemeService.\nBut this requires you to track the windows that are created.\n\nFor example:\n```\nWindow newWindow = new Window();\nWindowHelper.TrackWindow(newWindow);\n```\n\nThe TrackWindow method adds the windows to a dictionary, and finally the ThemeService can access the windows and apply theme/backdrop..."}, {"user": "ghost1372", "created_at": "2024-12-07T12:14:24Z", "body": "Hi @MPITech \r\ni added Multi-Window support for ThemeService.\r\nwhen you created a window, you have to use `TrackWindow `like this:\r\n\r\n`WindowHelper.TrackWindow(myWindow);`\r\nthen ThemeService will use a foreach in a list and apply all backdrop/elementTheme/Tint,....\r\n"}, {"user": "MPITech", "created_at": "2024-12-07T12:43:02Z", "body": "Awesome, thanks.  I was already tracking windows so this will be easy to integrate.  Thanks again."}], "satisfaction_conditions": ["A solution for managing themes across multiple windows without creating multiple ThemeService instances", "A mechanism to apply theme changes (tint color, backdrop) to all open windows simultaneously", "Integration with existing window tracking functionality", "Clear guidance on the architectural approach for theme management in multi-window applications"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:40"}, "dockerfile": null, "language": "c#"}
{"number": 24, "title": "[BUG] WithArgumentParser not called", "created_at": "2025-04-09T09:11:53Z", "closed_at": "2025-04-09T13:51:10Z", "commit_id": "b26897e304402f582511ad1751ba9afce0fb1d4f", "labels": ["bug"], "url": "https://github.com/peteraritchie/ConsoleApplicationBuilder/issues/24", "body": "## Source area of bug\n\n- [ ] Console Application Builder\n- [X] System.CommandLine Extensions\n\n## Description of the bug\n\nI have a Command with two required options - each having a `WithArgumentParser` attached. But only one the last `WithArgumentParser` is called\n\n**To Reproduce**\nExample code that produces the issue:\n\n```csharp\n            var builder = ConsoleApplication.CreateBuilder(args);\n            builder.Services.AddCommand()\n                .WithDescription(\"Update a WxS file with contents from a folder\")\n                .WithRequiredOption<FileInfo>(\"--file\", \"The input WxS file to update\")\n                    .WithArgumentParser((result) =>\n                    {\n                        var fileInfo = new FileInfo(result.Tokens[0].Value);\n                        if (!fileInfo.Exists)\n                        {\n                            throw new FileNotFoundException($\"The file '{fileInfo.FullName}' does not exist.\");\n                        }\n                        return fileInfo;\n                    })\n                .WithRequiredOption<DirectoryInfo>(\"--source-folder\", \"The directory containing the files to include\")\n                    .WithArgumentParser((result) =>\n                    {\n                        var dirInfo = new DirectoryInfo(result.Tokens[0].Value);\n                        if (!dirInfo.Exists)\n                        {\n                            throw new DirectoryNotFoundException($\"The directory '{dirInfo.FullName}' does not exist.\");\n                        }\n                        return dirInfo;\n                    })\n                .WithHandler((wxsFile, sourceFolder) =>\n                {\n                    // Read the content of the input file\n                    string content = File.ReadAllText(wxsFile.FullName);\n                    // Replace the placeholder with the new value\n                    string updatedContent = content.Replace(\"PLACEHOLDER\", \"NEW_VALUE\");\n                    // Write the updated content to the output file\n                    File.WriteAllText(wxsFile.FullName, updatedContent);\n                });\n\n            builder.Build<RootCommand>().Invoke/*Async*/(args);\n```\n- Set a breakpoint in all three lambda expressions and run.\n- Supply an existing folder to the `--source-folder` parameter and a non-existing file to the `--file` parameter.\n- Run.\n- Notice that the `ParseArgument<DirectoryInfo>()` lambda is hit and returns the `DirectoryInfo` instance.\n- Notice that the `ParseArgument<FileInfo>()` lambda is **not** hit.\n- Notice that the handler is hit with the `wxsFile` pointing to a non-existing file.\n\n**Expected behavior**\nMy expectation is that **both** `ParseArgument<FileInfo>()` **and** `ParseArgument<DirectoryInfo>` lambdas are hit in order to parse and validate both options.\n\n**Success Criteria**\nHaving both `ParseArgument<T>()` lambdas hit (as long as the already called does not throw exceptions).\n\n**Desktop (please complete the following information):**\n\n- OS: [Windows 11 x64]\n- Version [23H2 (22631.5039)]\n\n", "comments_url": "https://api.github.com/repos/peteraritchie/ConsoleApplicationBuilder/issues/24/comments", "author": "bstordrup", "comments": [{"user": "bstordrup", "created_at": "2025-04-09T09:20:01Z", "body": "I think the issue is that the `TwoParameterCommandBuilder.BuildCommand` does not add a value to `argumentParser` parameter to `AddParameter<TParam1>()` method when building the command."}, {"user": "peteraritchie", "created_at": "2025-04-09T12:48:26Z", "body": "Thanks, I'll have a look."}, {"user": "peteraritchie", "created_at": "2025-04-09T14:51:07Z", "body": "Thanks for noticing that. Fixed and the latest Nugget (1.0.4) fixes this "}, {"user": "bstordrup", "created_at": "2025-04-09T19:47:11Z", "body": "Nice 👍\n\nWill get new version tomorrow (and update my fork).\n\nThank you! "}, {"user": "bstordrup", "created_at": "2025-04-09T19:48:53Z", "body": "And cool project btw. Makes a cleaner approach to System.CommandLine."}], "satisfaction_conditions": ["A fix that ensures both WithArgumentParser methods are called during command execution", "A solution available through an official package update", "Proper validation of both required command options"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:21"}, "dockerfile": null, "language": "c#"}
{"number": 13, "title": "Some tools return error messages in Claude Desktop", "created_at": "2025-04-03T16:18:59Z", "closed_at": "2025-04-04T15:15:10Z", "commit_id": "6e6bd61195efcc568cdf0f6b584381b5c3ec68a8", "labels": ["bug"], "url": "https://github.com/CoderGamester/mcp-unity/issues/13", "body": "I used mcp-unity with Claude Desktop. When Claude used the select_gameobject or execute_menu_item tools, they were executed correctly in the Unity editor, but the message returned to Claude Desktop as a result of using the tools was \"Unsupported content type: undefined\". Claude judges that this tool is not working properly. On the other hand, the notify_message tool returns the message \"Message displayed:\" correctly. I looked at the source a little, and noticed that in the Unity extension, notify_message returns a json containing \"type\", while select_gameobject and execute_menu_item do not contain a \"type\". And I think the error is occurring because the websocket server code is trying to access a non-existent \"type\". Sorry if I'm mistaken.", "comments_url": "https://api.github.com/repos/CoderGamester/mcp-unity/issues/13/comments", "author": "umiyuki", "comments": [{"user": "CoderGamester", "created_at": "2025-04-03T21:13:45Z", "body": "thank you for the report @umiyuki \n\nwill investigate this as soon as I fix the current resources."}, {"user": "CoderGamester", "created_at": "2025-04-04T00:12:54Z", "body": "@umiyuki I fixed the issue. You were correct that the output was missing the text field to work properly. Should work fine now\nPlease let me know if you have any further issues"}, {"user": "umiyuki", "created_at": "2025-04-04T15:15:10Z", "body": "Thank you for your quick response. I have confirmed that it has been fixed to return a normal response. I will close the issue."}], "satisfaction_conditions": ["Fix for tools returning proper responses in Claude Desktop", "Resolution of the 'Unsupported content type: undefined' error message", "Proper formatting of tool responses to include necessary fields", "Consistent behavior across different tools in the Unity extension"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:53"}, "dockerfile": null, "language": "c#"}
{"number": 11, "title": "README Suggestion: Unity *needs to be* in focus or tools may time out", "created_at": "2025-04-01T18:17:03Z", "closed_at": "2025-04-02T23:35:18Z", "commit_id": "3acfb232f564ae2ef10282469c22359be035961d", "labels": ["bug"], "url": "https://github.com/CoderGamester/mcp-unity/issues/11", "body": "According to my testing (Mac / Apple Silicon), essentially all of the tools rely on the Unity window being in focus to execute, potentially because its main thread heavily throttles function calls if the application is not in focus. In other words, you might see the tool requests time out UNLESS you switch back to Unity to let them execute.\n\nMarking \"Run in Background\" seems to only affect builds, as far as I can tell, and doesn't help.  \n\nThere may be a way around this, but for now, everyone using this should know this limitation.", "comments_url": "https://api.github.com/repos/CoderGamester/mcp-unity/issues/11/comments", "author": "dsarno", "comments": [{"user": "alexander-andrianov", "created_at": "2025-04-02T08:24:54Z", "body": "+1, can confirm this behavior too. Based on how Unity handles thread prioritization, it’s likely some intentional optimization to throttle background processes (at least on Apple Silicon)\n@dsarno did you test it on windows / intel?"}, {"user": "dsarno", "created_at": "2025-04-02T13:36:04Z", "body": "Alexander I didn’t test on PC but would definitely be interested if it made\r\na difference.\r\n"}, {"user": "CoderGamester", "created_at": "2025-04-02T20:00:33Z", "body": "This was a problem indeed. Thank you for reporting\nI pushed a new fix today @dsarno @alexander-andrianov \n\nIt is a messy issue with Unity only allowing to run Editor code on the mainThread, but thankfully it was solvable \n\nCan you try again?\nShould be fine by now"}, {"user": "dsarno", "created_at": "2025-04-02T23:32:23Z", "body": "This was fixed for me @CoderGamester !  Well done!"}], "satisfaction_conditions": ["A solution that allows tools to execute properly without requiring Unity to be in focus", "A fix that addresses the thread prioritization issue on Apple Silicon Macs", "A solution that prevents tool request timeouts"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:59"}, "dockerfile": null, "language": "c#"}
{"number": 5, "title": "Missing ConsoleWindowUtility on Unity 2022.3", "created_at": "2025-03-20T09:48:31Z", "closed_at": "2025-03-24T06:53:20Z", "commit_id": "7d2a2dab10bf34ea671ef569842924d3ed842a39", "labels": ["bug"], "url": "https://github.com/CoderGamester/mcp-unity/issues/5", "body": "```\nLibrary/PackageCache/com.gamelovers.mcp-unity@7d2a2dab10/Editor/Resources/GetConsoleLogsResource.cs(96,13): error CS0103: The name 'ConsoleWindowUtility' does not exist in the current context\n```", "comments_url": "https://api.github.com/repos/CoderGamester/mcp-unity/issues/5/comments", "author": "trungdlp-wolffun", "comments": [{"user": "CoderGamester", "created_at": "2025-03-21T21:50:39Z", "body": "Good report\nlooking now into that"}, {"user": "CoderGamester", "created_at": "2025-03-22T14:10:19Z", "body": "@trungdlp-wolffun apologies for the delay\n\nI pushed a fix for the issue.\nLet me know if you still have problems"}, {"user": "trungdlp-wolffun", "created_at": "2025-03-24T06:53:20Z", "body": "It works well, thanks a lot @CoderGamester "}], "satisfaction_conditions": ["A fix for the missing ConsoleWindowUtility error in Unity 2022.3", "Compatibility with the user's Unity 2022.3 environment", "Resolution that allows the package to compile without errors"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:03"}, "dockerfile": null, "language": "c#"}
{"number": 34, "title": "能实现对docker 里面的日志起作用吗？(Can it work on the logs in docker?)", "created_at": "2025-03-17T11:04:45Z", "closed_at": "2025-03-20T01:59:19Z", "commit_id": "fff46a21f7b8582d47f92fb02802533220152be1", "labels": ["question"], "url": "https://github.com/xiaomi7732/PrettyLogging/issues/34", "body": null, "comments_url": "https://api.github.com/repos/xiaomi7732/PrettyLogging/issues/34/comments", "author": "maikebing", "comments": [{"user": "xiaomi7732", "created_at": "2025-03-17T17:59:54Z", "body": "Thanks for the question.\n\nIn docker, it depends on which console logging provider that you are using.\n\nFor example, if you are using the systemD logger provider, which is recommended inside systemd environment, `PrettyLogging` is **not** going to interfere with it:\n\n```csharp\nloggingBuilder.AddSystemdConsole().PrettyIt(); // Pretty it won't interfere with SystemdConsole logger.\n```\n\nBut if you are using the SimpleConsole logging provider, then, **yes**, it will work\n\n```csharp\nloggingBuilder.AddSimpleConsole().PrettyIt(); // Pretty will work inside a container.\n```\n\nNotes: because of the ANSI color code, you will see those \"strange text\" when using the simple console logging provider inside the container.\n\nTo mitigate that side effect, yuou might want to disable the color behavior like this:\n\n```csharp\nloggingBuilder.AddSimpleConsole().PrettyIt(opt=> opt.ColorBehavior = \n    Microsoft.Extensions.Logging.Console.LoggerColorBehavior.Disabled\n);\n```"}, {"user": "maikebing", "created_at": "2025-03-20T01:59:19Z", "body": "感谢。。非常感谢。 "}], "satisfaction_conditions": ["Clear guidance on Docker compatibility with the logging library", "Specific configuration examples for different Docker logging scenarios", "Information about potential issues and their workarounds in Docker environments"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:48"}, "dockerfile": null, "language": "c#"}
{"number": 49, "title": "other players", "created_at": "2025-03-30T03:19:29Z", "closed_at": "2025-03-30T17:24:43Z", "commit_id": "f906cb01019e06dea65bd62009cc695d27d50da9", "labels": [], "url": "https://github.com/azuradara/neo-act-plugin/issues/49", "body": "is there a way to see party member dps ? i can only see myself, but i saw people tracking other people too. what do i have to do for that to show ? ", "comments_url": "https://api.github.com/repos/azuradara/neo-act-plugin/issues/49/comments", "author": "kixxn", "comments": [{"user": "azuradara", "created_at": "2025-03-30T13:21:54Z", "body": "Hello, you have to turn on their damage in the combat chat settings."}, {"user": "kixxn", "created_at": "2025-03-30T15:09:25Z", "body": "ah thank you\n"}], "satisfaction_conditions": ["Instructions on how to view other players' DPS metrics", "Information about where to find relevant settings", "A simple, direct solution that doesn't require complex setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:57"}, "dockerfile": null, "language": "c#"}
{"number": 227, "title": "Json serialization / initialization error when used with NativeAOT", "created_at": "2025-04-07T10:24:59Z", "closed_at": "2025-04-11T11:26:48Z", "commit_id": "faf12b6a9496111f21fd474cd9173071673a8c8d", "labels": ["bug"], "url": "https://github.com/modelcontextprotocol/csharp-sdk/issues/227", "body": "**Describe the bug**\nWhen compiling with NativeAOT, I'm getting runtime errors due to some methods not being code-generated.\nIs there a way to configure the library so that it'll use source generators for System.Text.Json so that this will work properly?\n\n\n**To Reproduce**\nI'm creating a McpClient with stdio transport like so:\n```\nDictionary<string, string> options = new()\n        {\n            [\"command\"] = command,\n            [\"arguments\"] = arguments,\n        };\n\n        // Add environment variables, prefixed with \"env:\" to options\n        if (environmentVariables != null)\n        {\n            foreach (var kvp in environmentVariables)\n            {\n                options[$\"env:{kvp.Key}\"] = kvp.Value;\n            }\n        }\n\n        ILoggerFactory loggerFactory = LoggerFactory.Create(builder => builder.AddConsole());\n\n        var client = await McpClientFactory.CreateAsync(new McpServerConfig()\n        {\n            Id = id,\n            Name = id,\n            TransportType = TransportTypes.StdIo,\n            TransportOptions = options,\n        }, loggerFactory: loggerFactory, cancellationToken: cancellationToken);\n```\n\n\n**Expected behavior**\nThis connects correctly when running with CoreCLR, but fails when compiled with NativeAOT due to code not being generated for a specific type.\nI'd expect the library to work on NativeAOT and not throw the exception.\n\n**Logs**\n```\n07.04.2025 12:12:51.27 <info> [Backend]: fail: ModelContextProtocol.Client.McpClient[403959396]\n      Client server Client (db6cee23-4a25-44e2-9cd7-3dc6d44625d2: db6cee23-4a25-44e2-9cd7-3dc6d44625d2) initialization error\n      ModelContextProtocol.Protocol.Transport.McpTransportException: Failed to send message\n       ---> System.MissingMethodException: Method not found: 'Void System.Text.Json.Serialization.Metadata.JsonObjectInfoValues`1<ModelContextProtocol.Protocol.Messages.JsonRpcRequest>.set_ConstructorAttributeProviderFactory(System.Func`1<System.Reflection.ICustomAttributeProvider>)'.\n         at Internal.Runtime.TypeLoaderExceptionHelper.CreateMissingMethodException(ExceptionStringID, String) + 0x4c\n         at Internal.Runtime.CompilerHelpers.ThrowHelpers.ThrowMissingMethodException(ExceptionStringID, String) + 0xc\n         at ModelContextProtocol.Utils.Json.McpJsonUtilities.JsonContext.Create_JsonRpcRequest(JsonSerializerOptions) + 0x18\n         at System.Text.Json.Serialization.Metadata.JsonTypeInfoResolverChain.GetTypeInfo(Type, JsonSerializerOptions) + 0x44\n         at System.Text.Json.JsonSerializerOptions.GetTypeInfoNoCaching(Type) + 0x58\n         at System.Text.Json.JsonSerializerOptions.CachingContext.CreateCacheEntry(Type type, JsonSerializerOptions.CachingContext context) + 0x20\n      --- End of stack trace from previous location ---\n         at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw() + 0x24\n         at System.Text.Json.JsonSerializerOptions.CachingContext.CacheEntry.GetResult() + 0x24\n         at System.Text.Json.JsonSerializerOptions.GetTypeInfoInternal(Type, Boolean, Nullable`1, Boolean, Boolean) + 0x50\n         at System.Text.Json.JsonSerializerOptions.GetTypeInfo(Type) + 0x4c\n         at ModelContextProtocol.Utils.Json.McpJsonUtilities.GetTypeInfo[T](JsonSerializerOptions) + 0x30\n         at ModelContextProtocol.Utils.Json.JsonRpcMessageConverter.Write(Utf8JsonWriter, IJsonRpcMessage, JsonSerializerOptions) + 0xe4\n         at System.Text.Json.Serialization.JsonConverter`1.TryWrite(Utf8JsonWriter writer, T& value, JsonSerializerOptions options, WriteStack& state) + 0xb8\n         at System.Text.Json.Serialization.JsonConverter`1.WriteCore(Utf8JsonWriter writer, T& value, JsonSerializerOptions options, WriteStack& state) + 0x20\n         at System.Text.Json.Serialization.Metadata.JsonTypeInfo`1.Serialize(Utf8JsonWriter, T&, Object) + 0x120\n         at System.Text.Json.JsonSerializer.WriteString[TValue](TValue&, JsonTypeInfo`1) + 0x3c\n         at System.Text.Json.JsonSerializer.Serialize[TValue](TValue, JsonTypeInfo`1) + 0x40\n         at ModelContextProtocol.Protocol.Transport.StdioClientTransport.<SendMessageAsync>d__12.MoveNext() + 0x134\n         --- End of inner exception stack trace ---\n         at ModelContextProtocol.Protocol.Transport.StdioClientTransport.<SendMessageAsync>d__12.MoveNext() + 0x3dc\n      --- End of stack trace from previous location ---\n         at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw() + 0x24\n         at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task) + 0x100\n         at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task, ConfigureAwaitOptions) + 0x68\n         at ModelContextProtocol.Shared.McpJsonRpcEndpoint.<SendRequestAsync>d__22`1.MoveNext() + 0x2a4\n      --- End of stack trace from previous location ---\n         at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw() + 0x24\n         at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task) + 0x100\n         at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task, ConfigureAwaitOptions) + 0x68\n         at ModelContextProtocol.Client.McpClient.<InitializeAsync>d__20.MoveNext() + 0x1f4\n      --- End of stack trace from previous location ---\n         at System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw() + 0x24\n         at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task) + 0x100\n         at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task, ConfigureAwaitOptions) + 0x68\n         at ModelContextProtocol.Client.McpClient.<ConnectAsync>d__19.MoveNext() + 0x28c\n```\n\nILC-specific configuration properties in the csproj for this project:\n```\n    <RootAllApplicationAssemblies>true</RootAllApplicationAssemblies>\n    <IlcGenerateCompleteTypeMetadata>true</IlcGenerateCompleteTypeMetadata>\n    <IlcGenerateStackTraceData>false</IlcGenerateStackTraceData>\n```\n", "comments_url": "https://api.github.com/repos/modelcontextprotocol/csharp-sdk/issues/227/comments", "author": "christianscheuer", "comments": [{"user": "stephentoub", "created_at": "2025-04-07T11:09:36Z", "body": "@christianscheuer, what version of the library are you using? Can you share a standalone repro please? "}, {"user": "christianscheuer", "created_at": "2025-04-08T09:47:11Z", "body": "@stephentoub thank you so much for the quick reply!\n\nEmbarrassingly, I was stuck on version 0.1.0-preview.2 and hadn't noticed the newer updates. My apologies! I can confirm the issue was fixed sometime in between preview 2 and 0.1.0-preview.6."}, {"user": "stephentoub", "created_at": "2025-04-08T10:21:38Z", "body": "Great! Glad it's addressed. "}, {"user": "christianscheuer", "created_at": "2025-04-09T15:59:30Z", "body": "Yes! Most normal queries now run fine - however, I just found that some tool calls have problems. The following error is reported:\n\n```\nJsonTypeInfo metadata for type 'System.Collections.Generic.List`1[System.Object]' was not provided by TypeInfoResolver of type '[ModelContextProtocol.Utils.Json.McpJsonUtilities+JsonContext,Microsoft.Extensions.AI.AIJsonUtilities+JsonContext]'.\nIf using source generation, ensure that all root types passed to the serializer have been annotated with 'JsonSerializableAttribute', along with any types that might be serialized polymorphically. Path: $.\n```\n\nThis appears to happen with tools that report back arrays of objects in their responses and/or receive it.\n\nIs there anything obvious here that stands out, or would you need a repro case for it? Since it depends on MCP servers and specific queries, I'm not sure how easy it'll be - but perhaps the error message illustrates the problem?"}, {"user": "eiriktsarpalis", "created_at": "2025-04-09T16:04:04Z", "body": "Could you share a repro? I suspect what is happening here is you're defining a tool that accepts or returns a `List<object>`. In AOT you would need to explicitly source generate that type and pass the relevant `JsonSerializerOptions` to the tool calling method."}, {"user": "christianscheuer", "created_at": "2025-04-11T11:26:48Z", "body": "Hi @eiriktsarpalis.\n\nYou were right. I wasn't defining a tool myself (this is a MCP client, so the definition is by the server), but I was passing a List<object> as one of the arguments in the Dictionary<string, object>. Made everything JsonElements now so it serializes correctly.\nThanks again for the quick responses.\n\nWe're generally more used to using JObject from Newtonsoft which always works in NativeAOT re. serialization, so I guess it's the Dictionary<string, object> that tricked me into believing I could pass anything in there.\n\nPerhaps an overload which only accepts System.Text.Json JsonElements would be interesting, to make it easier to catch potential NativeAOT errors ahead of time for consumers of the library? Or maybe that's overengineering it.\n\nAnyway, problem solved for us thanks to your quick answers - much appreciated."}, {"user": "eiriktsarpalis", "created_at": "2025-04-11T12:57:06Z", "body": "> We're generally more used to using JObject from Newtonsoft which always works in NativeAOT re. serialization\n\nAre you sure that's the case? I doubt this would work with this library unless you passed a custom STJ converter for the type then apply a source generator."}], "satisfaction_conditions": ["A solution that enables the library to work with NativeAOT compilation", "Guidance on proper serialization approaches for complex types in NativeAOT environments", "Clear explanation of why certain serialization patterns fail in NativeAOT", "Timely and responsive support"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:01:28"}, "dockerfile": "FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build\n\n# Install dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/modelcontextprotocol/csharp-sdk.git . && \\\n    git checkout faf12b6a9496111f21fd474cd9173071673a8c8d\n\n# Modify global.json to use SDK version 9.0.100-preview.5.24251.5 (compatible with our base image)\nRUN sed -i 's/\"version\": \"9.0.100\"/\"version\": \"9.0.100-preview.5.24251.5\"/g' global.json\n\n# Restore NuGet packages\nRUN dotnet restore\n\n# Build the project\nRUN dotnet build --configuration Release\n\n# Create a test project to verify the NativeAOT issue\nWORKDIR /app/NativeAotTest\nRUN dotnet new console\nRUN dotnet add reference /app/src/ModelContextProtocol/ModelContextProtocol.csproj\n\n# Update the project file for NativeAOT support\nRUN echo '<Project Sdk=\"Microsoft.NET.Sdk\"><PropertyGroup><OutputType>Exe</OutputType><TargetFramework>net8.0</TargetFramework><PublishAot>true</PublishAot><RootAllApplicationAssemblies>true</RootAllApplicationAssemblies><IlcGenerateCompleteTypeMetadata>true</IlcGenerateCompleteTypeMetadata><IlcGenerateStackTraceData>false</IlcGenerateStackTraceData></PropertyGroup></Project>' > NativeAotTest.csproj\n\n# Create a test program that reproduces the issue\nRUN echo 'using System; using System.Collections.Generic; using System.Threading; using System.Threading.Tasks; using Microsoft.Extensions.Logging; using ModelContextProtocol.Client; namespace NativeAotTest { class Program { static async Task Main() { var loggerFactory = LoggerFactory.Create(builder => builder.AddConsole()); try { var client = await McpClientFactory.CreateAsync(new McpServerConfig { Id = \"test\", Name = \"test\", TransportType = TransportTypes.StdIo, TransportOptions = new Dictionary<string, string> { [\"command\"] = \"echo\", [\"arguments\"] = \"test\" } }, loggerFactory: loggerFactory); } catch (Exception ex) { Console.WriteLine($\"Error: {ex}\"); } } } }' > Program.cs\n\n# Add System.Text.Json source generator to help with NativeAOT\nRUN dotnet add package Microsoft.Extensions.Logging.Console\nRUN dotnet add package System.Text.Json\n\n# Return to the main directory\nWORKDIR /app", "language": "c#"}
{"number": 127, "title": "How do I return file in current implementation?", "created_at": "2025-03-28T06:55:56Z", "closed_at": "2025-03-28T12:13:30Z", "commit_id": "9330774795e0544940e6ad25721da7732b52fd73", "labels": [], "url": "https://github.com/modelcontextprotocol/csharp-sdk/issues/127", "body": "Hello! Just checked the docs and tests and did not find any sample on how I can return file as tool answer, for example. Could anyone shed some light on it?", "comments_url": "https://api.github.com/repos/modelcontextprotocol/csharp-sdk/issues/127/comments", "author": "vshapenko", "comments": [{"user": "stephentoub", "created_at": "2025-03-28T12:11:01Z", "body": "There are a variety of ways, but probably the easiest is to just return a `Microsoft.Extensions.AI.DataContent`, e.g.\n```C#\n[McpServerTool]\npublic static DataContent GetMyImage()\n{\n    byte[] bytes = File.ReadAllBytes(\"path/to/my/image.png\");\n    return new DataContent(bytes, \"image/png\");\n}\n```"}, {"user": "vshapenko", "created_at": "2025-03-28T12:12:32Z", "body": "Thanks a lot, will try"}], "satisfaction_conditions": ["A code example showing how to return a file as a tool answer", "Information about the appropriate class or method to use for file returns", "A complete, executable code snippet that demonstrates the file return process"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:32"}, "dockerfile": null, "language": "c#"}
{"number": 13, "title": "Latest update does not include the Forms.DLL", "created_at": "2025-01-10T21:01:46Z", "closed_at": "2025-01-10T21:15:54Z", "commit_id": "81581626e9550a9d993eb023d3b1854d6a4027b0", "labels": [], "url": "https://github.com/YusufOzmen01/desktopmate-custom-avatar-loader/issues/13", "body": null, "comments_url": "https://api.github.com/repos/YusufOzmen01/desktopmate-custom-avatar-loader/issues/13/comments", "author": "Oroborius", "comments": [{"user": "aemisigna", "created_at": "2025-01-10T21:08:06Z", "body": "Same issue here, the mod is not working due to System.Windows.Forms not being in the package"}, {"user": "aemisigna", "created_at": "2025-01-10T21:12:24Z", "body": "> Same issue here, the mod is not working due to System.Windows.Forms not being in the package\r\n\r\nNevermind, I just downloaded and installed it again and it worked, weird."}, {"user": "gotolouco", "created_at": "2025-01-10T21:14:27Z", "body": "Possibly your Windows defender excludes it by giving a false positive in the dll."}, {"user": "YusufOzmen01", "created_at": "2025-01-10T21:15:54Z", "body": "I have forgot to add the DLL file. I added it a bit ago so that's why it worked :3"}, {"user": "Oroborius", "created_at": "2025-01-11T00:01:38Z", "body": "> Possibly your Windows defender excludes it by giving a false positive in the dll.\r\n\r\nI don't have Defender. I have it removed from the OS. Was just forgot to be included."}], "satisfaction_conditions": ["Inclusion of the missing Forms.DLL file in the package", "A working mod that properly loads System.Windows.Forms", "A complete installation package with all required dependencies"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:16"}, "dockerfile": null, "language": "c#"}
{"number": 24, "title": "Add SortAttribute, .gitignore and update documentation 🔨", "created_at": "2025-03-20T02:09:52Z", "closed_at": "2025-03-28T11:56:35Z", "commit_id": "2de3e83635f89f4da2f486114a4622feef6121ba", "labels": [], "url": "https://github.com/AbZorbaGames/artificetoolkit/pull/24", "body": "Hey @ZackPer,  \r\n\r\n**What's new:**  \r\n- Added `SortAttribute` to order Inspector fields   \r\n- Updated docs with sword example ⚔️  \r\n- Added `.gitignore` for IDE folders 🚫  \r\n\r\n**Quick question:**  \r\n\r\nIn `ArtificeInspector`, this line (12), gets uncommented by Unity when I clone the repo 😅. \r\n```csharp  \r\n// [CustomEditor(typeof(Object), true), CanEditMultipleObjects]  \r\n```\r\n\r\nMeant to be a `///` comment?\r\n", "comments_url": "https://api.github.com/repos/AbZorbaGames/artificetoolkit/issues/24/comments", "author": "exejutable", "comments": [{"user": "ZackPer", "created_at": "2025-03-20T14:10:23Z", "body": "Regarding this line,\r\n\r\n```c#\r\n// [CustomEditor(typeof(Object), true), CanEditMultipleObjects]  \r\n```\r\n\r\nThe intended use for Artifice is to add it to your project through git. That means the source code is inside unity's the Library folder which is always under gitignore. \r\n\r\nSo we tried to utilize this to be able to turn on and off the ArtificeInspector completely, by removing the attribute which applies it to the inspector. If someone wants to download and add it manually, he should gitignore this file specifically to be able to turn on/off the inspector.\r\n\r\nSo when working on artifice, the developer should completely ignore this script 🐵 \r\n\r\nThis is not documented, so I will put a task for it to update it."}, {"user": "ZackPer", "created_at": "2025-03-20T14:11:41Z", "body": "Also great job @exejutable ! Your PR was well structured and documented. After resolving the truly minor comments I added, we can pull and merge!"}, {"user": "exejutable", "created_at": "2025-03-20T21:18:07Z", "body": "Hi @ZackPer ,\r\n\r\nThanks for the explanation! \r\n\r\n**About the `//` behavior:**\r\nWhen adding Artifice as a local package (from disk), Unity removes them automatically. Switching to `///` fixes this issue, as Unity preserves triple-slash comments.\r\n\r\n**About the \"minor comments\":**\r\nYou mentioned resolving \"truly minor comments,\" but I don’t see them in the PR. Did you mean:\r\n\r\n1. You’ll handle them?\r\n2. Or should I address them?\r\n\r\nLet me know so I can help out! "}, {"user": "ZackPer", "created_at": "2025-03-21T08:52:47Z", "body": "Good morning @exejutable \r\n\r\nI see what you mean know with the `//`. It feels weird that I havent really noticed it by now but I confirmed it now haha. I will add it on a later patch so dont bother with it for now. \r\n\r\nIt seems I never submitted my review, I apologize. New to the Github UI, I have mostly used GitLab until now. You should be able to see them now.\r\n\r\nOne last comment, you should also progress the package.json version to 1.3.2\r\n\r\nThanks again!"}, {"user": "exejutable", "created_at": "2025-03-24T20:48:26Z", "body": "Hi @ZackPer I removed the `.gitignore` added the `///` to the `ArtificeInspector` also updated the package to `1.3.2`"}, {"user": "ZackPer", "created_at": "2025-03-28T11:39:19Z", "body": "Hello @exejutable !\r\n\r\nSorry for the delay but I was on my day-offs 🫣\r\n\r\nSo I made some changes after reviewing the branch.\r\n\r\nFirst of all, I reverted the '//' instead of the '///' because it does not have to do with Unity. Probably, the Artifice_Utilities class was simply removing the '//' because it detected that previously you had enabled the ArtificeToolkit. So long story sort, the Artifice_Utilities which also offers the MenuItem options, enforces the '//' based on whether you want to use Artifice or not. This way, your preference is kept even when you update the ArtificeToolkit version.\r\n\r\nSecondly, I made a small optimization to the sorting utility method. I added a boolean to skip the OrderBy call if no sorting is needed, so we keep the O(n) time complexity for all cases that we dont have any sorting. I know the OrderBy has the best case of O(n) eitherway, but it feels better to enforce it so we stay agnostic of the sorting algorithm.\r\n\r\nLastly, I changed the default sorting order value (if no sort attribute was used at a specific property, but we need sorting because of another attribute), to be 0. This way, if you want to make a property which appears first in your script, appear last in the inspector, you dont need to put [Sort] to every other property. This is also the way Odin does it!\r\n\r\nWith this changes, I will probably squash and merge soon. Let me know your thoughts when you see this.\r\n"}, {"user": "exejutable", "created_at": "2025-04-08T23:57:01Z", "body": "Hi @ZackPer ,\r\n\r\nNo worries at all about the delay—hope you had a great time on your day off!\r\n\r\nThanks for the detailed explanation and the changes. Everything looks good to me! The only thing that caught my eye was the shift from explicit typing to var, but that’s just me being a typing psychopath lol."}, {"user": "ZackPer", "created_at": "2025-04-09T06:22:26Z", "body": "Hello @exejutable \r\n\r\nThe whole project uses 'var' instead of explicitly defining types. Probably there are some cases where this is impossible if no value can be assigned to it yet, until some logic actually does it. \r\n\r\nBeing a typing freak is a great quality for a programmer :) "}], "satisfaction_conditions": ["Clear explanation of the comment behavior in ArtificeInspector", "Guidance on how to properly handle the PR review process", "Information about version numbering requirements for the package", "Feedback on their code contribution quality", "Technical rationale for maintainer's code changes"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:36"}, "dockerfile": null, "language": "c#"}
{"number": 9, "title": "Shell integration", "created_at": "2024-12-06T13:21:58Z", "closed_at": "2025-02-16T03:16:43Z", "commit_id": "b9a3e7167453b8ca04071f079cd03a6f56cffabf", "labels": ["bug"], "url": "https://github.com/hfiref0x/WinDepends/issues/9", "body": "RemoveAssoc is never called - easy fix.\r\n\r\nSetAssoc, use a quoted path, for when people have spaces in their file path:\r\n\r\n```\r\ntry\r\n        {\r\n            using (var regKey = Registry.ClassesRoot.CreateSubKey(extKeyName, true))\r\n            {\r\n                if (regKey != null)\r\n                {\r\n                    // Set command value.\r\n                    using (var subKey = regKey.CreateSubKey(\"command\"))\r\n                    {\r\n                        subKey?.SetValue(\"\", $\"\\\"{Application.ExecutablePath}\\\" \\\"%1\\\"\", RegistryValueKind.String);\r\n                    }\r\n\r\n                    // Set icon value.\r\n                    regKey.SetValue(\"Icon\", $\"{Application.ExecutablePath}, 0\", RegistryValueKind.String);\r\n                }\r\n            }\r\n        }\r\n\r\n\r\n```", "comments_url": "https://api.github.com/repos/hfiref0x/WinDepends/issues/9/comments", "author": "AlexW-13", "comments": [{"user": "i486", "created_at": "2025-02-15T00:48:22Z", "body": "@hfiref0x \nIt seems like you forgot to include quotes around the `%1` placeholder. The latest snapshot build is still creating `\"C:\\WinDepends\\bin\\WinDepends.exe\" %1` for the context menu, which doesn't work for files with spaces in their paths.\n\nBTW: Working great on Windows 7. Thanks for this amazing tool."}, {"user": "hfiref0x", "created_at": "2025-02-15T03:40:32Z", "body": "@i486 \nYou are right, thanks. This should be fixed now in the above mentioned commit."}], "satisfaction_conditions": ["Proper handling of file paths with spaces in the shell integration", "Compatibility with Windows 7"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:31"}, "dockerfile": null, "language": "c#"}
{"number": 52, "title": "chat commands not working", "created_at": "2025-01-26T23:31:33Z", "closed_at": "2025-01-27T00:42:27Z", "commit_id": "2b562dbab6531ef4ea3f5a6285783d9428879550", "labels": [], "url": "https://github.com/DrMeepso/WebFishingCove/issues/52", "body": "Have an issue where none of the built in commands like spawn, kick, ban, users work at all. it only says the command isnt found even though the command file has them in it. any fix?", "comments_url": "https://api.github.com/repos/DrMeepso/WebFishingCove/issues/52/comments", "author": "JBork1", "comments": [{"user": "Ech0klang", "created_at": "2025-01-26T23:32:00Z", "body": "Enable plugins in the server config"}, {"user": "JBork1", "created_at": "2025-01-26T23:32:44Z", "body": "Ah, thank you. was having an issue banning someone earlier but now i can deal with them."}], "satisfaction_conditions": ["A solution that enables the built-in chat commands to function", "A simple configuration adjustment that doesn't require complex troubleshooting", "Information about where in the server configuration the relevant setting needs to be changed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:11"}, "dockerfile": null, "language": "c#"}
{"number": 11, "title": "libsteam_api64: No such file or directory", "created_at": "2024-11-12T04:55:19Z", "closed_at": "2024-11-12T05:40:49Z", "commit_id": "ff4748aa5fb3f05b0d39e573ec75e24277170679", "labels": [], "url": "https://github.com/DrMeepso/WebFishingCove/issues/11", "body": "Getting an error after server setup when attempting to start:\r\n\r\nUnable to load shared library 'steam_api64' or one of its dependencies. In order to help diagnose loading problems, consider setting the LD_DEBUG environment variable: libsteam_api64: cannot open shared object file: No such file or directory\r\n\r\nsorry if I'm just dumb", "comments_url": "https://api.github.com/repos/DrMeepso/WebFishingCove/issues/11/comments", "author": "JackOtsig", "comments": [{"user": "DrMeepso", "created_at": "2024-11-12T05:00:09Z", "body": "You just have to rename libsteam_api.so to libsteam_api64.so. Thats on me, I'll update the build action to automatically do that! "}, {"user": "JackOtsig", "created_at": "2024-11-12T05:12:59Z", "body": "Ah, that makes sense, now facing the same issue as the other guy, but if they fixed it, so can I.\r\nThank you so much <3 you're amazing"}, {"user": "DrMeepso", "created_at": "2024-11-12T05:40:49Z", "body": "anytime <3"}], "satisfaction_conditions": ["A clear explanation of the file naming issue causing the library loading error", "A simple, actionable fix for the 'libsteam_api64' loading error", "Confirmation that their issue is a known/common problem"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:27"}, "dockerfile": null, "language": "c#"}
{"number": 87, "title": "Panel", "created_at": "2025-03-28T20:14:43Z", "closed_at": "2025-03-29T01:59:01Z", "commit_id": "1bb61fd354d435de1c26ff98106a7a091789b64e", "labels": [], "url": "https://github.com/Quasar-Continuation/Pulsar/issues/87", "body": "So, I build my rat in builder, I run it to test it, I don't show up on my panel what can I do?\n", "comments_url": "https://api.github.com/repos/Quasar-Continuation/Pulsar/issues/87/comments", "author": "JCrobotss1234alt", "comments": [{"user": "JCrobotss1234alt", "created_at": "2025-03-28T20:25:19Z", "body": "remind you im super stupid too"}, {"user": "Body-Alhoha", "created_at": "2025-03-28T22:47:37Z", "body": "Please make sure the IP & Port you provided is valid and you are currently listening 🙏 "}, {"user": "JCrobotss1234alt", "created_at": "2025-03-29T00:48:18Z", "body": "> Please make sure the IP & Port you provided is valid and you are currently listening 🙏\n\nty it worked, also why does it disconnect at random times?"}], "satisfaction_conditions": ["Instructions for ensuring proper connection configuration between the rat and panel", "Guidance for troubleshooting basic connectivity issues with the panel", "Information presented in simple, accessible terms"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:24"}, "dockerfile": null, "language": "c#"}
{"number": 28, "title": "fix: GetDCのリソース開放を追加", "created_at": "2025-01-15T14:42:04Z", "closed_at": "2025-01-15T15:05:31Z", "commit_id": "d9bb12739a49ccdda846a0b2cfb86c5493bebc1c", "labels": [], "url": "https://github.com/MidraLab/uDesktopMascot/pull/28", "body": "DPIスケールを取得時のGetDC()にて、ReleaseDC()をしていなかったので修正しました\r\n以下問題が軽減するかもしれません\r\n- 動作が重い\r\n- 終了に時間がかかる\r\n- エディタ実行時に停止ができない\r\n\r\nプルリク初めてなので、やりかたおかしかったら教えて下さい", "comments_url": "https://api.github.com/repos/MidraLab/uDesktopMascot/issues/28/comments", "author": "hirona98", "comments": [{"user": "ayutaz", "created_at": "2025-01-15T15:04:58Z", "body": "ありがとうございます！問題ないと思います！"}, {"user": "hirona98", "created_at": "2025-01-15T15:20:37Z", "body": "了解です！ありがとうございます！\n（ブランチ名スペルミスしてた…）"}], "satisfaction_conditions": ["Acceptance of the proposed fix for the resource leak in GetDC()", "Confirmation that the pull request process was handled correctly", "Acknowledgment that the fix might address the reported performance issues"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:11"}, "dockerfile": null, "language": "c#"}
{"number": 153, "title": "v1.0.544.0  on \"Latest\" tag for Raspi Docker", "created_at": "2025-03-19T19:09:31Z", "closed_at": "2025-03-19T19:21:15Z", "commit_id": "49099083e65227c090c9b8041e25977c341d581c", "labels": [], "url": "https://github.com/Webreaper/SolisAgileManager/issues/153", "body": "I've just ran a pull on my images and got v1.0.544.0 on the \"latest\" tag but expected 1.0.553 according to the releases. \nDid something fail to build? (Not even sure if that's the right terminology, i'm just trying to sound like I know what I'm on about 😜)", "comments_url": "https://api.github.com/repos/Webreaper/SolisAgileManager/issues/153/comments", "author": "tabannis", "comments": [{"user": "tabannis", "created_at": "2025-03-19T19:10:19Z", "body": "PS I got v1.0.554.0 on DEV tag."}, {"user": "Webreaper", "created_at": "2025-03-19T19:16:00Z", "body": "Yeah, there's a race condition with the container build, so sometimes it fails. I just clicked retry. :)"}, {"user": "Webreaper", "created_at": "2025-03-19T19:21:15Z", "body": "Should be there now. "}, {"user": "tabannis", "created_at": "2025-03-19T19:42:44Z", "body": "Yup! TYVM"}], "satisfaction_conditions": ["Confirmation that the latest Docker image has been updated to the expected version", "Explanation of why the version discrepancy occurred", "Resolution of the version discrepancy between the 'latest' tag and the expected release version"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:38"}, "dockerfile": null, "language": "c#"}
{"number": 140, "title": "Schedule Action set once to Inverter and retained", "created_at": "2025-03-11T09:22:31Z", "closed_at": "2025-03-12T13:18:08Z", "commit_id": "4a5edcb97079a59e1e8d8c66a54d790ff40fca36", "labels": [], "url": "https://github.com/Webreaper/SolisAgileManager/issues/140", "body": "Was wondering about the Scheduled Actions specifically for charging actions say 23.30-05.30 in my example where I want to guarantee an overnight charge no matter what.   Is it possible to have this set once to the inverter without it being reset?  So, in the case Solis Agile Manager webserver has an outage (host failure for example) I can be sure an overnight charge will always happen.\n\nI had a look at setting this directly at the Inverter using the secondary or third charging periods (leaving the first period free for Solis Agile Manager to utilise) but this causes a time overlap conflict if the Solis Agile Manager tries to apply charging periods it sees a cheap periods.\n\nThanks for the amazing work on this project and will buy you coffees for such brilliant and simple to use solution.", "comments_url": "https://api.github.com/repos/Webreaper/SolisAgileManager/issues/140/comments", "author": "cs95dtt", "comments": [{"user": "cs95dtt", "created_at": "2025-03-11T09:52:32Z", "body": "I just had a thought the amp value is reset to 0 so any sort of permanent override wouldn't work anyway for 23:30-05:30 charge period I want to set permanently.\n\nI'm over thinking and complicating this.\n\nGreat work nevertheless from you!\n\n"}, {"user": "Webreaper", "created_at": "2025-03-12T13:18:08Z", "body": "Yeah, mixing manual SolisCloud updates with the app becomes complicated because of the potential for conflicts, which then stop the app working correctly. The app pretty much blats over the entire charging setup when it applies the charge state for a new slot, to avoid this. \n\nI think if you just set up a bunch of 'Charge' scheduled actions it should do what you want, though, right? The fact that the 23:30-05:30 charge will be written to the inverter once, at 23:30, each day isn't really a biggie."}], "satisfaction_conditions": ["A way to ensure overnight charging happens reliably even if the Solis Agile Manager webserver experiences an outage", "A solution that avoids conflicts between scheduled actions and Solis Agile Manager's dynamic charging periods", "Clarification on how scheduled actions persist or reset on the inverter", "A simple approach that doesn't overcomplicate the charging schedule setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:45"}, "dockerfile": null, "language": "c#"}
{"number": 139, "title": "Further issue with detecting Ohme charging", "created_at": "2025-03-11T07:55:41Z", "closed_at": "2025-03-12T13:18:37Z", "commit_id": "4a5edcb97079a59e1e8d8c66a54d790ff40fca36", "labels": [], "url": "https://github.com/Webreaper/SolisAgileManager/issues/139", "body": "The. Updated Ohme software does not let you avoid dynamic charging now so your car may charge randomly overnight if it is plugged in. I don’t think that any notification is sent when this happens so you could see your house battery being used to charge the car overnight. I can’t think of a way round this with software so I think it will have to raised as a bug/problem with Ohme.\n\n", "comments_url": "https://api.github.com/repos/Webreaper/SolisAgileManager/issues/139/comments", "author": "dqj999", "comments": [{"user": "dqj999", "created_at": "2025-03-11T08:12:56Z", "body": "Having thought about this you could use the scheduled action facility to reduce the effect of this. If you set a low power scheduled charge for say the first 4 hours of the cheap period then a high power charge in the last two hours that would correct any clash between the two charging systems and would ensure that the battery was at the desired charge level in the morning, although it might have had a few random charge/discharge cycles."}, {"user": "Webreaper", "created_at": "2025-03-11T08:34:03Z", "body": "Yeah, I was going to suggest that if you set a scheduled action to charge the battery all the way through the cheap overnight period (which you'd probably want anyway) then you could prevent the battery discharging to charge the car. Have you seen the latest dev build allows you to specify amps for each scheduled action?"}, {"user": "dqj999", "created_at": "2025-03-11T10:12:29Z", "body": "Yes thanks,  I spotted that just after I made the first comment. Good feature!\n\nDoes that depend on the latest release of the Inverter software?"}, {"user": "Webreaper", "created_at": "2025-03-12T13:18:37Z", "body": "No, it'll work for any version of the firmware. "}], "satisfaction_conditions": ["A workaround solution to prevent house battery depletion when Ohme charger activates randomly", "Confirmation about the ability to set charging power levels for scheduled actions", "Clarification about software compatibility requirements"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:49"}, "dockerfile": null, "language": "c#"}
{"number": 138, "title": "Running in Docker on macOS", "created_at": "2025-03-10T21:54:51Z", "closed_at": "2025-03-11T09:59:41Z", "commit_id": "4a5edcb97079a59e1e8d8c66a54d790ff40fca36", "labels": [], "url": "https://github.com/Webreaper/SolisAgileManager/issues/138", "body": "Has anyone got this running successfully in Docker on macOS?\nI have followed the guidance provided in the release and the docker-compose file, but am seeing these errors when I try to run the container:\n\n```\nsolismanager exited with code 255\nsolismanager  | exec /app/SolisManager: no such file or directory\n```\nAny ideas?  Thanks", "comments_url": "https://api.github.com/repos/Webreaper/SolisAgileManager/issues/138/comments", "author": "0rangutan", "comments": [{"user": "Webreaper", "created_at": "2025-03-10T22:02:46Z", "body": "Have a look at the last 3 comments in #124. The alpha tag, with the environment variable in the docker-compose snippet, should work. "}, {"user": "Webreaper", "created_at": "2025-03-11T09:59:41Z", "body": "Closing this - the latest image should work (you won't need the env var). Please let me know if it does/doesn't."}, {"user": "0rangutan", "created_at": "2025-03-11T10:01:24Z", "body": "Thanks - I have the app running on macOS now!\nWorks with the Alpha and currently with 1.0.522 and the Env variable.\nI'll try it without now..."}, {"user": "0rangutan", "created_at": "2025-03-11T10:05:01Z", "body": "Yes, works without the environment variable, thanks!"}], "satisfaction_conditions": ["A working Docker configuration for running SolisManager on macOS", "Clear instructions on which Docker image version to use", "Information about environment variable requirements", "A solution that resolves the 'no such file or directory' error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:57"}, "dockerfile": null, "language": "c#"}
{"number": 87, "title": "Help information on Pricing & Charging Plan Screen cannot be displayed on touch screen device", "created_at": "2025-02-01T15:30:42Z", "closed_at": "2025-02-04T08:35:20Z", "commit_id": "b2758915724489d3b38bbda3ecad2a5257788155", "labels": [], "url": "https://github.com/Webreaper/SolisAgileManager/issues/87", "body": "If you hover the mouse cursor over any of the labels at the top of the screen, extra information is displayed e.g. \"Current load being consumed by the house\".\n\nThis doesn't happen on a touch screen device, and currently there is no way to see this info. On the Config Screen there are specific Help Icons that you can click to get more information.  Is this a possibility on this screen?\n\n", "comments_url": "https://api.github.com/repos/Webreaper/SolisAgileManager/issues/87/comments", "author": "dqj999", "comments": [{"user": "Webreaper", "created_at": "2025-02-01T15:36:15Z", "body": "Tool tips don't generally work on touch screen device. Hopefully people will learn what the icons mean so won't need them on a phone.\n\nOne thought I had is to duplicate the colours and icons of the Soliscloud app so people will be familiar. "}, {"user": "Webreaper", "created_at": "2025-02-01T15:38:43Z", "body": "One thing I could do is make them tappable or clickable, and display a popup with the description, bit like the `?` icons in the config screen. "}, {"user": "dqj999", "created_at": "2025-02-01T15:43:43Z", "body": "Making them clickable would work :-)\n\nThe pair I find confusing are the \"Total Solar PV Generation Today\" and the \"Current Solar PV Generation\".  Maybe putting a Sigma in front of it would help the mathematically oriented amongst us!"}, {"user": "Webreaper", "created_at": "2025-02-01T15:52:10Z", "body": "Doesn't the inclusion of the units make it absolutely clear? "}, {"user": "dqj999", "created_at": "2025-02-01T17:06:51Z", "body": "Yes it does if I think about it! \n\nDoh!"}, {"user": "Webreaper", "created_at": "2025-02-04T08:35:20Z", "body": "Fixed in most recent release. "}], "satisfaction_conditions": ["A way to access help information on touch screen devices", "Clear visual distinction between current values and cumulative totals"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:06"}, "dockerfile": null, "language": "c#"}
{"number": 34, "title": "Change QMeshPolygonNode polygon radius in code", "created_at": "2025-02-02T18:27:52Z", "closed_at": "2025-02-19T14:23:40Z", "commit_id": "7afa037049f660257df8e9e9399ad74740596663", "labels": [], "url": "https://github.com/erayzesen/godot-quarkphysics/issues/34", "body": "I am trying to change the polygon radius in code for a softbody. Is seems that the function: set_polygon_radius(value) does not work. I tested it like this:\n```\nextends QSoftBodyNode\n\nvar expanded_size = 100\nvar normal_size= 40\n\nfunc _physics_process(delta: float) -> void:            \n\tif Input.is_action_pressed(\"expand\"):\n\t\t$Mesh.set_polygon_radius(expanded_size)  #$Mesh is a QMeshPolygonNode\n\telse:\n\t\t$Mesh.set_polygon_radius(normal_size)\n\n```\nThis does not work too\n```\nextends QSoftBodyNode\n\nvar expanded_size = 100\nvar normal_size = 40\n\nfunc _physics_process(delta: float) -> void:            \n\tif Input.is_action_pressed(\"expand\"):\n\t\t$Mesh.polygon_radius = expanded_size\n\telse:\n\t\t$Mesh.polygon_radius = normal_size\n\n```\n\nI know the code isnt efficent, its just for demonstration", "comments_url": "https://api.github.com/repos/erayzesen/godot-quarkphysics/issues/34/comments", "author": "WilleIshere", "comments": [{"user": "erayzesen", "created_at": "2025-02-02T22:44:05Z", "body": "Hi @WilleIshere. Objects like QMeshCircleNode, QMeshPolygonNode, and QMeshRectNode are nodes that generate the target mesh when added to the scene based on your settings. We do something similar with QMeshAdvancedNode using an editor plugin.\n\nIf a QMeshNode object is under a QSoftBodyNode, its particles move individually.  Therefore, if you want to control the particles during the simulation, you need to use methods related to the particles directly. In the example you provided, you would need to modify both the local,global positions of the particles and the spring properties that enforce distance constraints between them."}, {"user": "WilleIshere", "created_at": "2025-02-03T09:38:33Z", "body": "Thanks, Can you give an example how this can be done?"}, {"user": "erayzesen", "created_at": "2025-02-03T16:58:38Z", "body": "Of course. \n\n```\nfunc _process(delta: float) -> void:\n\tvar mesh:QMeshNode=$QMeshPolygonNode\n\tif(Input.is_action_pressed(\"ui_up\")) :\n\t\tfor i in range(mesh.get_particle_count()) :\n\t\t\tvar p:QParticleObject=mesh.get_particle_at(i)\n\t\t\tp.set_position(p.get_position()+ p.get_position().normalized() )\n\t\tfor i in range(mesh.get_spring_count()) :\n\t\t\tvar s:QSpringObject=mesh.get_spring_at(i)\n\t\t\tvar current_local_distance=s.get_particle_b().get_position()-s.get_particle_a().get_position()\n\t\t\tvar new_length=current_local_distance.length()\n\t\t\ts.set_length(new_length)\n```\n\nThis code should position the particles 1 unit outward from the center each time the up arrow key is pressed and update the springs accordingly."}, {"user": "WilleIshere", "created_at": "2025-02-03T18:33:53Z", "body": "Thank you so much!"}], "satisfaction_conditions": ["A working code example showing how to dynamically resize a QMeshPolygonNode in a QSoftBodyNode", "An explanation of why the original approach didn't work", "Guidance on the correct approach to manipulate soft body meshes during runtime"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:26"}, "dockerfile": null, "language": "c++"}
{"number": 8, "title": "Unable to install zipfs extension. Getting \"Failed to download extension zipfs \" : Duckdb version 1.1.0", "created_at": "2025-02-04T07:04:47Z", "closed_at": "2025-02-04T13:31:03Z", "commit_id": "1998be89bf7f2a464161121661f016e0c8fe1302", "labels": [], "url": "https://github.com/isaacbrodsky/duckdb-zipfs/issues/8", "body": "Hi, I am unable to execute the following zipfs install command on Duckdb 1.1.0\n```sql\nINSTALL zipfs FROM community\n```\nI tried forcefully enabling the community extensions by executing following statement.\n```sql\nSET allow_community_extensions = true;\n```\nBut still, I am getting the following error message.\n\n```\nHTTP Error: Failed to download extension \"zipfs\" at URL \"http:// community-extensions. duckdb. org/ v1.1.0/ osx_arm64/ zipfs. duckdb_extension. gz\" (HTTP 403)  Candidate extensions: \"httpfs\", \"tpcds\", \"https\", \"postgres\", \"icu\"\n```", "comments_url": "https://api.github.com/repos/isaacbrodsky/duckdb-zipfs/issues/8/comments", "author": "aby-kuruvilla-clear", "comments": [{"user": "isaacbrodsky", "created_at": "2025-02-04T07:11:03Z", "body": "Hi, this extension was developed quite recently and only supports recent versions of DuckDB (1.1.3 or the forthcoming 1.2.0) If you update your DuckDB version I expect it will find a compatible build to install."}, {"user": "aby-kuruvilla-clear", "created_at": "2025-02-04T13:28:32Z", "body": "Thank you so much for the response."}], "satisfaction_conditions": ["Information about version compatibility requirements for the zipfs extension", "A clear explanation for why the installation command was failing", "A solution path to resolve the extension installation problem"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:56"}, "dockerfile": null, "language": "c++"}
{"number": 43, "title": "Mode FM under Band CB not selectable", "created_at": "2025-04-09T06:29:21Z", "closed_at": "2025-04-09T07:14:15Z", "commit_id": "392ce79c18f2cccdb2f9c24985557a47efe2bb5f", "labels": [], "url": "https://github.com/esp32-si4732/ats-mini/issues/43", "body": "Hello,\n\nI got another issue.\n\nUnder Band CB I'm not able to select Mode FM on my device.\n\nI'd installed V1.09 with rotated display.", "comments_url": "https://api.github.com/repos/esp32-si4732/ats-mini/issues/43/comments", "author": "BrightCGN", "comments": [{"user": "jimjackii", "created_at": "2025-04-09T07:01:42Z", "body": "That is correct. The SI4732 is not capable of FM in the CB band.\n\nRegards, Steffen"}, {"user": "BrightCGN", "created_at": "2025-04-09T07:06:09Z", "body": "> That is correct. The SI4732 is not capable of FM in the CB band.\n> \n> Regards, Steffen\n\nThanks for the infos :-)"}], "satisfaction_conditions": ["A clear explanation of whether the feature is possible or not", "Technical reasoning for why a feature limitation exists"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:51"}, "dockerfile": null, "language": "c++"}
{"number": 13, "title": "Simple lock-free framebuffer read & write", "created_at": "2024-12-11T03:49:13Z", "closed_at": "2024-12-14T15:28:17Z", "commit_id": "91b0ee8d69d522a4c5e3868ea3b493c41c66618b", "labels": [], "url": "https://github.com/xuwd1/wemeet-wayland-screenshare/pull/13", "body": "不太清楚 README 中所述功率是如何测得的，因此我并未测试实际的性能提升。\r\n\r\n~无锁的双缓存区应该会带来一些提升？~ 我很乐意补充我本地的性能测试结果，如果你可以告诉我如何评估性能的话。\r\n\r\n---\r\n\r\n此外，我并未仔细检查上下文的注释是否清理/修改妥当，如有不妥，敬请指正。", "comments_url": "https://api.github.com/repos/xuwd1/wemeet-wayland-screenshare/issues/13/comments", "author": "Coekjan", "comments": [{"user": "xuwd1", "created_at": "2024-12-11T07:22:24Z", "body": "谢谢你的工作！不过，我认为现在的改动还存在一些问题. 我想首先说明一下为何现在的设计中采用了锁：\r\n- 如你所见，我们的代码中framebuffer的参数是跟随pw的参数变化的\r\n- 在整个从payload写入，到hook读出的过程中，从始至终我们只利用fb中的一块内存作为中转\r\n- fb的参数更新可能会使得fb的内存发生重分配. 更具体地，导致重分配的参数更新主要是pw报告的分辨率发生了变化，有两种主要情形：a. fb的初始参数和开始录制后的实际参数不一致，比如用户使用的是一块2160p屏幕. b. 用户录制的是一个窗口，录制开始后用户改变了窗口的形状\r\n- 因此，在存在这种重分配的情况下，我们必须要设法保证，hook在读取fb时，fb的内存块**不会被重分配**. 否则我们一定会遇到segfault.\r\n\r\n正是基于如上的原因，我最后就采用了加锁的方案. 我可以理解本PR的改动中尝试将锁换为具有一致性保证的原子变量的动机，并且双缓冲的确也拉长了依赖距离，但是我认为现在的设计的一个显著问题是其无法保证双缓冲不存在**翻转**. 我们设想一个场景：\r\n\r\n1. hook开始读fb A，但由于某种原因它读的比较慢，这个过程一直到最后都没能结束\r\n2. payload写完了另一块fb B\r\n3. payload又打算开始写fb A，并且用户恰好最大化了窗口\r\n4. fb A改变参数，程序崩溃\r\n\r\n因此，我认为要取消锁，仅靠改为原子变量是不够的. 我们真正需要的其实是改变这套依赖于重分配的逻辑，或者在尽量避免重分配的同时围绕重分配实现一套安全逻辑. \r\n\r\n最后还是谢谢你的工作，但我认为我们目前的确需要更多努力才能将锁取消.\r\n\r\n最后，关于功耗监测的方法，windows上我主要使用hwinfo64观察功耗，而linux上我用的主要是amdgpu_top. intel平台上应当也有相似的工具（我记得有一个依赖于内核模块的，非常详细的工具，甚至能观察一些hw counter，但我忘记叫什么了.）"}, {"user": "xuwd1", "created_at": "2024-12-11T08:25:21Z", "body": "我还想额外补充一点想法：\r\n1. 我想也许我们可以实现某种“huge buffer”. 比如，我们让这个huge buffer足够的大（say, like maybe 8192x8192），这样应该就能使得绝大多数（或者可以说所有）情况下所需要的buffer size都比这个huge buffer小，那么我们就可以保证至少我们始终都在安全的地址空间里. 然后，我们认为所有超出这个buffer size的都是非法情况，那么我们就永远都不需要重分配了. 不过这样也是个很糟糕的方案，光是这个huge buffer我们就需要消耗我们256M的内存. 不过感觉这个方案也可以进一步细化，比如我们先借助XRR看一下用户的显示器分辨率（代码中已经有这种功能），然后根据这个分辨率决定一个“更大，但不太大 (like 1.5 times of the largest screen size, or simply the whole X framebuffer size)”的huge buffer size.\r\n2. 或者我们可以实现一种\"multi-buffer pyramids\"，也就是说我们事先准备一系列小的，不同尺寸的buffer，在参数发生变化时，我们只在这些buffer的范围内选择合适的buffer. 那么即使参数发生变化，我们只需要调整指针指向的实际buffer."}, {"user": "Coekjan", "created_at": "2024-12-11T08:31:05Z", "body": "感谢你的指正。针对“避免重分配”的问题，是否可以考虑仅向 kernel 申请 huge buffer 那么大的虚拟内存空间，真正用到时，kernel 理应会自动（按页/按大页）分配物理内存？"}, {"user": "xuwd1", "created_at": "2024-12-11T08:34:23Z", "body": "@Coekjan 感觉这个应该就是basically how VM works. 但是我感觉我们应该至少需要首先对这块buffer全部填0，那么最后还是需要消耗这么多内存. 不过刚才想了下感觉根据屏幕尺寸选择尺寸应该是个比较合理的方案. （毕竟，compositor本身也是要消耗这么多内存的）"}, {"user": "Coekjan", "created_at": "2024-12-11T08:36:59Z", "body": "> 但是我感觉我们应该至少需要首先对这块buffer全部填0，那么最后还是需要消耗这么多内存.\r\n\r\n不可以另外维护“长度”么，这样子 reader 和 writer 在 `data_size` 不变化时只访问“长度”内的区域，当需要访问“长度”外的区域时，再进行初始化（填 0）。"}, {"user": "xuwd1", "created_at": "2024-12-11T08:39:46Z", "body": "@Coekjan 啊，这也就是说在参数更新时如果需要再去填0，那我觉得的确是个还不错的主意. 而且无论如何感觉最多256M的消耗应该也还算是一个可以接受的范围.😉"}, {"user": "Coekjan", "created_at": "2024-12-11T09:33:34Z", "body": "> 但是我感觉我们应该至少需要首先对这块buffer全部填0\r\n\r\n填 0 真的是必要的么（主分支上的代码似乎也没有将 buffer 初始化为 0），我现在尝试了一下，似乎不填 0 也不会出大问题？reader 尽管在一开始读到了未初始化的数据，但由于 writer 产生数据的速度足够快，所以用户大概不会感知到这个“不可预知”的 frame。"}, {"user": "xuwd1", "created_at": "2024-12-11T09:47:28Z", "body": "@Coekjan 刚才看了一下，是我记错了，我们目前是对ximage填0的：\r\n```\r\n  CvMat ximage_cvmat;\r\n  OpencvDLFCNSingleton::cvInitMatHeader(\r\n    &ximage_cvmat, ximage_height, ximage_width,\r\n    CV_8UC4, image.data, ximage_bytes_per_line\r\n  );\r\n  OpencvDLFCNSingleton::cvSetZero(&ximage_cvmat);\r\n```\r\n\r\n这样的话你说的是对的，我们的确可以不去管buffer的内容.😃"}, {"user": "Coekjan", "created_at": "2024-12-11T10:01:40Z", "body": "请查收最新 push 的版本：\r\n1. 使用了 huge buffer 的思路，不需要初始化值，因此仅占据虚拟内存，并不会导致物理内存开销变大；\r\n2. 完全移除了对应的 mutex。"}, {"user": "xuwd1", "created_at": "2024-12-11T10:06:00Z", "body": "谢谢你的努力，目前的版本看起来还不错，我想这样的话我们的hook的效率会有提升。我晚些时候会再检查和测试一下，如果没问题的话我会着手合并，并且可能会立即做一个commit做一些微小的调整，谢谢！"}, {"user": "DerryAlex", "created_at": "2024-12-11T12:48:19Z", "body": "真的有双缓存区吗？怎么看着只是把锁去掉了，并把 buffer 改成了固定大小\r\n\r\n真实现 ring buffer 的话，也不能完全去掉 lock, 不过只锁 `buffer[read_index]` 应该能降低冲突的概率，还是需要实验来测试一下。\r\n\r\n另外可以对每帧都存一下格式吧，这样也不需要 huge buffer 的 workaround\r\n\r\n "}, {"user": "Coekjan", "created_at": "2024-12-11T12:51:57Z", "body": "> 真的有双缓存区吗？怎么看着只是把锁去掉了，并把 buffer 改成了固定大小\r\n\r\n1. 没有双缓冲区。\r\n2. buffer 改成固定大小（足够大），使得不需要所谓的“重分配”，因此不需要锁来保证 read 过程中 framebuffer 不被“重分配”。同时，这足够大的 buffer 并不会带来令人担忧的内存开销，因为只有真正用到对应虚拟页面，kernel 才会分配物理内存。\r\n\r\n---\r\n\r\n补充：我理解原代码中锁只是为了避免“重分配”使得正读取 buffer 的线程访问无效内存。"}, {"user": "DerryAlex", "created_at": "2024-12-11T13:12:07Z", "body": "> 补充：我理解原代码中锁只是为了避免“重分配”使得正读取 buffer 的线程访问无效内存。\r\n\r\n一部分数据是上一帧的，另外一部分数据是下一帧的也不行吧"}, {"user": "Coekjan", "created_at": "2024-12-11T13:23:15Z", "body": "> 一部分数据是上一帧的，另外一部分数据是下一帧的也不行吧\r\n\r\n如果用目前 PR 的代码，理论上是可能会出现这种情况的。由于我本地测试时，几乎感知不到这种现象，这种现象也许并不那么“错误”，所以还是完全移除了锁。当然，若维护者认为应当极力避免这种竞态，我认为可以考虑改为双缓冲区实现。"}, {"user": "xuwd1", "created_at": "2024-12-11T14:01:56Z", "body": "我觉得如果测试下来几乎不会有可察觉到的副作用的话，去除锁对于改善效率和功耗应当是有正面作用的。不过既然另一位贡献者有些疑虑，那我就多进行些测试. 不过由于近日仓库突然感觉热度增加了不少，本PR的处理速度可能也要稍微放缓一点（由于我也有本职工作要做），预计可能最晚会到本周末处理，还希望@Coekjan 理解.🥲"}, {"user": "xuwd1", "created_at": "2024-12-14T15:26:53Z", "body": "经过测试可以验证至少在um5606wa (HX370) 的性能模式下，hugebuffer的方法可以降低2W的封装功耗(which is huge imho)，但在安静模式下（封装功耗同样大约为4.7W）功耗几乎没有区别. 但我认为这说明我们值得用这个简单的手段完全移除mutex，感谢@Coekjan 的工作！但不过由于我感觉代码中有多处实现需要略微调整，暂时先合入`hugebuffer` branch做进一步修改，随后并入`master`."}, {"user": "xuwd1", "created_at": "2024-12-14T15:34:35Z", "body": "待我修改完毕并将`hugebuffer`合入`master`后，我会对`README.md`进行相应的调整以修正其中的说明并增加相应的credits，谢谢！"}], "satisfaction_conditions": ["A solution that eliminates mutex locks while maintaining memory safety", "A performance improvement that reduces power consumption", "A solution that prevents memory reallocation issues during framebuffer parameter changes", "An implementation that doesn't require excessive physical memory consumption", "Information about how to measure power consumption for performance testing"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:27"}, "dockerfile": null, "language": "c++"}
{"number": 1, "title": "关于 OpenCV", "created_at": "2024-11-21T08:09:57Z", "closed_at": "2024-11-25T19:52:35Z", "commit_id": "c3d9465e4777e018e3d797965b7321d77bd9fbf2", "labels": [], "url": "https://github.com/xuwd1/wemeet-wayland-screenshare/issues/1", "body": "你试过用 dlopen（`RTLD_LOCAL`）来加载 opencv 库吗？虽然没看到崩溃细节，但我怀疑是符号冲突，那么 RTLD_LOCAL 应当能避免。", "comments_url": "https://api.github.com/repos/xuwd1/wemeet-wayland-screenshare/issues/1/comments", "author": "lilydjwg", "comments": [{"user": "xuwd1", "created_at": "2024-11-21T08:18:37Z", "body": "好主意，感谢建议，我近日会试一下. 之前使用的方法都是在cmake中链接opencv."}, {"user": "xuwd1", "created_at": "2024-11-25T19:52:35Z", "body": "本项目现在已经根据提议实现了对opencv库的动态加载. 得益于此，现在本项目中的图像缩放可以确保aspect ratio正确了，感谢提议！"}], "satisfaction_conditions": ["A solution that resolves symbol conflicts when loading OpenCV libraries", "A method that ensures correct aspect ratio in image scaling operations", "An alternative approach to the traditional CMake linking method for OpenCV"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:40"}, "dockerfile": null, "language": "c++"}
{"number": 8, "title": "llm-audio ignores playVolume setting from clients", "created_at": "2025-02-03T12:44:46Z", "closed_at": "2025-02-08T01:20:30Z", "commit_id": "fe01d735cae761a7b3db1a12d52a8dbd35d5aaa4", "labels": [], "url": "https://github.com/m5stack/StackFlow/issues/8", "body": "Environment: StackFlow v1.4.0 and M5Module-LLM dev branch\n\nThe playVolume setting is not available from M5Module-LLM library on arduino.\nThe below code makes no effect.\n```\n    /* Setup Audio module */\n    M5.Display.printf(\">> Setup audio..\\n\");\n    m5_module_llm::ApiAudioSetupConfig_t audio_config;\n    audio_config.playVolume = 0.01;  \n    module_llm.audio.setup(audio_config);\n```\nWhen I changed the value of \"volume\" of \"play_param\" in /opt/m5stack/share/audio.json,  the volume got quietter as expected. So I doubt that the volume setting from json might not be implemented in v1.4.0.\n", "comments_url": "https://api.github.com/repos/m5stack/StackFlow/issues/8/comments", "author": "nyasu3w", "comments": [{"user": "Abandon-ht", "created_at": "2025-02-06T07:12:55Z", "body": "The playVolume parameter is obsolete in StackFlow 1.3 and later versions. Use json for configuration instead."}, {"user": "nyasu3w", "created_at": "2025-02-06T14:47:04Z", "body": "Oh, it is obsolete. How do I change volumes of awake_wav(kws) and tts?"}, {"user": "Abandon-ht", "created_at": "2025-02-07T02:25:28Z", "body": "Modify the value of volume in the play_param item in the /opt/m5stack/share/audio.json file."}, {"user": "dianjixz", "created_at": "2025-02-07T06:42:28Z", "body": "Before calling the audio unit, you can use the following:\n```\n{\n    \"request_id\": \"1\",\n    \"work_id\": \"audio\",\n    \"action\": \"setup\",\n    \"object\": \"audio.play\",\n    \"data\": {\n        \"volume\": 0.5\n        }\n}\n```\nInitialize the audio module to dynamically configure the volume.\n"}, {"user": "nyasu3w", "created_at": "2025-02-07T11:53:34Z", "body": "Thanks for good information.\nMy understanding is that \"playVolume\" is renamed to \"volume\", and it is not imeplemented yet in M5Module-LLM library.\n(And it seems that more configurations are supported in llm_audio by CONFIG_AUTO_SET macro.)"}], "satisfaction_conditions": ["Information about how to properly configure audio volume in StackFlow v1.4.0", "Clarification on why the original approach (using playVolume parameter) wasn't working", "Specific methods to dynamically control audio volume programmatically", "Understanding of the relationship between configuration options in different versions/libraries"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:02:23"}, "dockerfile": "FROM ubuntu:20.04\n\n# Avoid interactive prompts during installation\nENV DEBIAN_FRONTEND=noninteractive\n\n# Set up timezone information\nRUN apt-get update && apt-get install -y tzdata && \\\n    ln -fs /usr/share/zoneinfo/UTC /etc/localtime && \\\n    dpkg-reconfigure -f noninteractive tzdata\n\n# Install build dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    cmake \\\n    python3 \\\n    python3-pip \\\n    python3-dev \\\n    scons \\\n    wget \\\n    unzip \\\n    pkg-config \\\n    libssl-dev \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nRUN pip3 install --no-cache-dir numpy protobuf\n\n# Create working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/m5stack/StackFlow.git && \\\n    cd StackFlow && \\\n    git checkout fe01d735cae761a7b3db1a12d52a8dbd35d5aaa4\n\n# Set working directory to the repository\nWORKDIR /app/StackFlow\n\n# Build the project\n# This is a generic build command as the specific build instructions are not provided\n# The user will need to run the appropriate build command for their specific needs\nRUN cd projects/llm_framework && \\\n    if [ -f ./setup.sh ]; then chmod +x ./setup.sh && ./setup.sh; fi && \\\n    if [ -f ./build.sh ]; then chmod +x ./build.sh && ./build.sh; fi\n\n# Set the default command to show help\nCMD [\"echo\", \"StackFlow environment is ready. Navigate to /app/StackFlow to work with the project.\"]", "language": "c++"}
{"number": 7, "title": "llm_llm suddenly causes error in handling multi-byte utf8 string", "created_at": "2025-01-27T13:34:38Z", "closed_at": "2025-02-08T01:19:19Z", "commit_id": "fe01d735cae761a7b3db1a12d52a8dbd35d5aaa4", "labels": [], "url": "https://github.com/m5stack/StackFlow/issues/7", "body": "Environment: StackFlow v1.4.0 and M5Module-LLM dev branch\n\nThe output string of llm_llm is sent separetedly in json format, but the separation point can be at wrong point inside of multi-byte character. When this wrong separation happens, maybe the json output is corrupted to make some error.\n\nIf llm_llm gets \"ガンダムについて語ってください\" (in ja language) as input for inference, it will stop by the below error.\n[W][inference][ 199]: lLaMa_->Run have error!\n\nThe result for the input is always \"(snip) 作品は、1960年に発売された(snip)\", and separated at \"発\"character\n\"作品は、\", \"196\", \"0年にXX\", \"Y売された\"\n(発 is 3 bytes char 0xe799ba: XX=e799 Y=ba )\n\nIf json output is stopped, no error seems to happen.\nExtended log is the following. Ignore 6066d1, it is my logging mistake.\n\n[I][task_output][ 249]: send:作品は、\n[I][task_output][ 251]: datalen:12\n[I][task_output][ 253]: data:e4,bd,9c,e5,93,81,e3,81\n[I][task_output][ 255]: data:af,6066d1\n[I][task_output][ 273]: send stream\n[I][task_output][ 249]: send:196\n[I][task_output][ 251]: datalen:3\n[I][task_output][ 273]: send stream\n[I][task_output][ 249]: send:0年に��\n[I][task_output][ 251]: datalen:9\n[I][task_output][ 253]: data:30,e5,b9,b4,e3,81,ab,e7\n[I][task_output][ 255]: data:99,6066d1\n// if json is output, the error is here.\n[I][task_output][ 249]: send:�売された\n[I][task_output][ 251]: datalen:13\n[I][task_output][ 253]: data:ba,e5,a3,b2,e3,81,95,e3\n[I][task_output][ 255]: data:82,6066d1\n[I][task_output][ 273]: send stream\n\nThe logging code is like this in llm_llm::task_output()\n```\n        SLOGI(\"send:%s\", data.c_str());   // this is the original logging \n        const char* cstr = data.c_str();\n        SLOGI(\"datalen:%d\",data.length());\n        if(data.length() > 8)\n            SLOGI(\"data:%x,%x,%x,%x,%x,%x,%x,%x\",cstr[0],cstr[1],cstr[2],cstr[3],cstr[4],cstr[5],cstr[6],cstr[7]);\n        if(data.length() > 8)  SLOGI(\"data:%x, _%x_ \",cstr[8]);  // mistake\n```", "comments_url": "https://api.github.com/repos/m5stack/StackFlow/issues/7/comments", "author": "nyasu3w", "comments": [{"user": "Abandon-ht", "created_at": "2025-02-06T08:43:16Z", "body": "Thanks for your feedback. The cached token content is incorrectly truncated when output. I will fix it.\n\n```cpp\nif (cached_token.size() >= 3)\n{\n\tfloat t_cost_ms = t_cost.cost();\n\tfloat token_per_sec = token_ids.size() / (t_cost_ms / 1000);\n\tauto tmp_out = tokenizer->Decode(cached_token);\n\tprintf(\"tmp_out: %s\\n\", tmp_out.c_str());\n\t_attr.runing_callback(cached_token.data(), cached_token.size(), tmp_out.c_str(), token_per_sec, _attr.reserve);\n\tcached_token.clear();\n}\n```\n\nThis problem can be avoided by changing \"if (cached_token.size() >= 3)\" to \"if (cached_token.size() >= 5)\"."}, {"user": "nyasu3w", "created_at": "2025-02-06T14:43:55Z", "body": "Thanks for the information. I can enjoy LLM(s) in Japanese with the code even before it is released."}], "satisfaction_conditions": ["A fix for the UTF-8 character truncation issue in the LLM output", "Support for properly displaying Japanese language content", "A solution that works with their existing setup (StackFlow v1.4.0 and M5Module-LLM)", "A timely solution they could implement before an official release"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:02:33"}, "dockerfile": "FROM ubuntu:20.04\n\n# Avoid interactive prompts during installation\nENV DEBIAN_FRONTEND=noninteractive\n\n# Set up timezone information\nRUN apt-get update && apt-get install -y tzdata && \\\n    ln -fs /usr/share/zoneinfo/UTC /etc/localtime && \\\n    dpkg-reconfigure -f noninteractive tzdata\n\n# Install build dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    cmake \\\n    python3 \\\n    python3-pip \\\n    python3-dev \\\n    scons \\\n    wget \\\n    unzip \\\n    pkg-config \\\n    libssl-dev \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nRUN pip3 install --no-cache-dir numpy protobuf\n\n# Create working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/m5stack/StackFlow.git && \\\n    cd StackFlow && \\\n    git checkout fe01d735cae761a7b3db1a12d52a8dbd35d5aaa4\n\n# Set working directory to the repository\nWORKDIR /app/StackFlow\n\n# Set up environment for building the LLM framework\nRUN cd projects/llm_framework && \\\n    if [ -f ./setup.sh ]; then chmod +x ./setup.sh && ./setup.sh; fi\n\n# Build the project\nRUN cd projects/llm_framework && \\\n    if [ -f ./build.sh ]; then chmod +x ./build.sh && ./build.sh; fi\n\n# Set the default command\nCMD [\"echo\", \"Environment is ready to work with StackFlow and fix the UTF-8 multi-byte string issue in llm_llm. Navigate to /app/StackFlow to work with the project.\"]", "language": "c++"}
{"number": 22, "title": "Does it support the GCC compiler with a custom instruction set?", "created_at": "2025-03-15T03:42:19Z", "closed_at": "2025-03-25T06:17:28Z", "commit_id": "14f74db85eb3694f6617f569a2e0e4530fcb451b", "labels": ["question"], "url": "https://github.com/lynx-family/primjs/issues/22", "body": "We are an embedded development team from China. Our devices use self-developed SoCs. Can it support custom instruction sets?", "comments_url": "https://api.github.com/repos/lynx-family/primjs/issues/22/comments", "author": "chenzd123456", "comments": [{"user": "viekai", "created_at": "2025-03-17T07:15:01Z", "body": "Since the core code of our template interpreter is in the .S file, which is generated by an internal assembler, it might be difficult to achieve this until we open-source our assembler. However, you can turn off the template interpreter and use the C interpreter version."}, {"user": "chenzd123456", "created_at": "2025-03-25T06:17:08Z", "body": "THX. I will try it."}], "satisfaction_conditions": ["A viable workaround for using the system with a custom instruction set", "Clear explanation of the technical limitations preventing direct support", "A practical solution that can be implemented by the embedded development team"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:59"}, "dockerfile": null, "language": "c++"}
{"number": 6, "title": "How does TOS caching and GC roots interact?", "created_at": "2025-03-05T15:27:48Z", "closed_at": "2025-03-06T03:52:35Z", "commit_id": "b1fd84b0ef125cd82595d9bcb4b0589f9b8f4e0a", "labels": [], "url": "https://github.com/lynx-family/primjs/issues/6", "body": "Hi PrimJS developers. First of all, great work on the runtime!\n\nGC docs say that one of the roots is the `Interpreter Execution Stack`. However, the interpreter docs also say primjs does TOS caching. Once values are TOS cached into register x0/x1, don't they become invisible to the GC? In that case, how are the objects kept alive?\n\nThis is just a curious question, as I work on CPython, and we're planning to do TOS caching too.\n", "comments_url": "https://api.github.com/repos/lynx-family/primjs/issues/6/comments", "author": "Fidget-Spinner", "comments": [{"user": "sunzhipengbd", "created_at": "2025-03-06T03:48:18Z", "body": "Before the interpreter jumps to runtime, x0 and x1 are pushed onto the stack.\nThen the gc mark stage will scan the stack"}, {"user": "Fidget-Spinner", "created_at": "2025-03-06T03:52:25Z", "body": "Makes sense. Thanks!"}], "satisfaction_conditions": ["An explanation of how TOS cached values remain visible to the garbage collector", "A clear description of the mechanism that prevents memory leaks when using TOS caching", "Information that is transferable to other runtime implementations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:04"}, "dockerfile": null, "language": "c++"}
{"number": 61, "title": "blocking_send_recv_example.py seems incorrect", "created_at": "2025-03-19T05:27:33Z", "closed_at": "2025-03-20T00:20:25Z", "commit_id": "4b073797578685afa65755e0893952eecb41a067", "labels": [], "url": "https://github.com/ai-dynamo/nixl/issues/61", "body": "Hello! Firstly, this library looks extremely promising, would solve a very big issue I was dealing with!\n\nI had a look at the `blocking_send_recv_example.py` to see how I could potentially send over a tensor.\nThis test doesn't seem to work, it misuses `zmq`, as both procs are doing `connect`, while the correct usage is for the target proc to use `.bind` and the initiator proc to use `.connect`. The string literal for the addr in `.connect` is also wrong as there are a few extra spaces.\n\nAfter fixing a few of these issues myself, I am getting the following error from the initiator:\n```\nTraceback (most recent call last):\n  File \"/mnt/large_shared/libs/nixl/test/python/blocking_send_recv_example.py\", line 93, in <module>\n    xfer_handle = agent.initialize_xfer(\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/large_shared/users/federico/env_nightly/lib/python3.11/site-packages/nixl/_api.py\", line 299, in initialize_xfer\n    handle = self.agent.createXferReq(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: createXferReq(): incompatible function arguments. The following argument types are supported:\n    1. (self: nixl._bindings.nixlAgent, operation: nixl._bindings.nixl_xfer_op_t, local_descs: nixl._bindings.nixlXferDList, remote_descs: nixl._bindings.nixlXferDList, remote_agent: str, notif_msg: str = '', backend: int = 0) -> int\n\nInvoked with: <nixl._bindings.nixlAgent object at 0x7f17915f8370>, <nixl_xfer_op_t.NIXL_READ: 0>, <nixl._bindings.nixlRegDList object at 0x7f1793d44fb0>, <nixl._bindings.nixlRegDList object at 0x7f179173abf0>, b'b', 'UUID'\n[1742361716.638792] [g001:2525325:0]          rcache.c:643  UCX  WARN  mlx5_0: destroying inuse region 0x5566884a1010 [0x5566875a20c0..0x5566875a20f0] g- rw ref 1 lkey 0x1f2aea rkey 0x1f2aea atomic_rkey 0x21d268\n[g001:2525325:0:2525325]      rcache.c:383  Assertion `region->refcount == 0' failed: region 0x5566884a1010 0x5566875a20c0..0x5566875a20f0 of mlx5_0\n```\n\nWould be great if there was a functional example on how to send over a tensor 🙏 ", "comments_url": "https://api.github.com/repos/ai-dynamo/nixl/issues/61/comments", "author": "cassanof", "comments": [{"user": "mkhazraee", "created_at": "2025-03-19T22:01:30Z", "body": "Hello and thanks for pointing this out. We have fixed it in PR #65 and it's already merged. We further added data checks and some clean ups to the code.\n \nOne point to consider is that two sided and blocking is not our targeted mode of operation, this was an example to give an idea of how it's possible to replicate 2-sided with 1-sided. (Since it was an example to demonstrate the idea, it was supposed to be in our examples directory, and we plan to add CIs for examples directory very soon to avoid these issues). You can still pass tensors to 1-sided operations too, it's the same API."}, {"user": "cassanof", "created_at": "2025-03-20T00:20:25Z", "body": "thank you! \n\ni got it to work last night. been a big unblocker for me. thanks for releasing this library!"}], "satisfaction_conditions": ["A fix for the issues in the blocking_send_recv_example.py example code", "A working example of how to transfer tensors using the library", "Clarification on the intended usage patterns of the library"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 00:59:52"}, "dockerfile": "FROM nvcr.io/nvidia/pytorch:25.02-py3\n\n# Set timezone\nENV TZ=America\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n\n# Install required dependencies\nRUN apt-get update -y && apt-get install -y \\\n    git \\\n    build-essential \\\n    cmake \\\n    pkg-config \\\n    libnuma-dev \\\n    numactl \\\n    wget \\\n    autotools-dev \\\n    automake \\\n    libtool \\\n    libz-dev \\\n    libiberty-dev \\\n    flex \\\n    libibverbs-dev \\\n    libgoogle-glog-dev \\\n    libgtest-dev \\\n    libjsoncpp-dev \\\n    libpython3-dev \\\n    libboost-all-dev \\\n    libssl-dev \\\n    libgrpc-dev \\\n    libgrpc++-dev \\\n    libprotobuf-dev \\\n    protobuf-compiler-grpc \\\n    pybind11-dev \\\n    python3-full \\\n    python3-pip \\\n    python3-numpy \\\n    meson \\\n    ninja-build \\\n    uuid-dev \\\n    pciutils \\\n    libpci-dev \\\n    ibverbs-utils \\\n    libibmad-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nRUN pip3 install --no-cache-dir meson ninja pybind11\n\n# Install UCX 1.18.0 with optimized build flags\nWORKDIR /tmp\nRUN wget https://github.com/openucx/ucx/releases/download/v1.18.0/ucx-1.18.0.tar.gz && \\\n    tar xzf ucx-1.18.0.tar.gz && \\\n    cd ucx-1.18.0 && \\\n    ./contrib/configure-release \\\n    --prefix=/usr/local \\\n    --enable-optimizations \\\n    --enable-cma \\\n    --enable-mt \\\n    --with-cuda=/usr/local/cuda && \\\n    make -j$(nproc) && \\\n    make install && \\\n    ldconfig && \\\n    cd .. && \\\n    rm -rf ucx-1.18.0 ucx-1.18.0.tar.gz\n\n# Clone the repository and checkout the specific commit\nWORKDIR /app\nRUN git clone https://github.com/ai-dynamo/nixl.git && \\\n    cd nixl && \\\n    git checkout 4b073797578685afa65755e0893952eecb41a067\n\n# Set environment variables\nENV LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\nENV PATH=/usr/local/bin:$PATH\n\n# Build the project\nWORKDIR /app/nixl\nRUN meson setup build && \\\n    cd build && \\\n    ninja\n\n# Install the Python package\nWORKDIR /app/nixl\nRUN pip install --no-cache-dir .\n\n# Set working directory to the repository root\nWORKDIR /app/nixl", "language": "c++"}
{"number": 103, "title": "blocking_send_recv_example seems not working", "created_at": "2025-03-31T22:14:43Z", "closed_at": "2025-04-02T05:51:02Z", "commit_id": "c6b871cd912921cd431fe6f87b17cc37c2440c66", "labels": [], "url": "https://github.com/ai-dynamo/nixl/issues/103", "body": "r```\noot@ad-h100-80gb-sxm-ib-8x-research-01:/workspace/nixl/examples/python# python3 blocking_send_recv_example.py --name wei_test --zmq_ip 172.16.121.7 \nLoaded plugin UCX_MO\nLoaded plugin UCX\nInitialized NIXL agent: wei_test\ninitiator Tensors: [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\nTraceback (most recent call last):\n  File \"/workspace/nixl/examples/python/blocking_send_recv_example.py\", line 79, in <module>\n    peer_name = agent.add_remote_agent(remote_meta)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/nixl/_api.py\", line 335, in add_remote_agent\n    agent_name = self.agent.loadRemoteMD(metadata)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnixl._bindings.nixlInvalidParamError: NIXL_ERR_INVALID_PARAM\n```", "comments_url": "https://api.github.com/repos/ai-dynamo/nixl/issues/103/comments", "author": "gongwei-130", "comments": [{"user": "tstamler", "created_at": "2025-04-01T13:36:00Z", "body": "Hi @gongwei-130 , can you share how you are running the target application?"}, {"user": "gongwei-130", "created_at": "2025-04-01T18:04:35Z", "body": "@tstamler \n`python3 blocking_send_recv_example.py --name test --zmq_ip NCCL_SOCKET_IFNAME_IP_ADDRESS --zmq_port 8080 --mode target`"}, {"user": "tstamler", "created_at": "2025-04-01T20:45:29Z", "body": "Just using these commands I'm not able to reproduce because the sockets aren't able to connect. The default port used in the original command is 5555, but in the target command you are specifying port 8080. Can you double check that these are the exact matching command line arguments to reproduce this issue? \n\nI suspect that you may have specified the same name for both ends of the test, which would give this exact error."}, {"user": "donglinz", "created_at": "2025-04-02T03:49:54Z", "body": "@tstamler I have exactly the same issue. I build the nixl container with ```./contrib/build-container.sh``` and launch the target & the initiator.\n\nThe initiator hang and the target failed.\n\n```\npython blocking_send_recv_example.py  --name test --zmq_ip localhost --mode target\nLoaded plugin UCX\nLoaded plugin UCX_MO\nInitialized NIXL agent: test\ntarget Tensors: [tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n```\n\n```\npython blocking_send_recv_example.py  --name test --zmq_ip localhost\nLoaded plugin UCX\nLoaded plugin UCX_MO\nInitialized NIXL agent: test\ninitiator Tensors: [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\nTraceback (most recent call last):\n  File \"/data/donglin/nixl/examples/python/blocking_send_recv_example.py\", line 79, in <module>\n    peer_name = agent.add_remote_agent(remote_meta)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/nixl/_api.py\", line 335, in add_remote_agent\n    agent_name = self.agent.loadRemoteMD(metadata)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnixl._bindings.nixlInvalidParamError: NIXL_ERR_INVALID_PARAM\n```\n\nI am testing with a h100 dgx node.\n\nLet me know if I made any mistakes in running the example or you need more information from me to reproduce."}, {"user": "mkhazraee", "created_at": "2025-04-02T03:56:56Z", "body": "Clarifying what Tim mentioned, --name values should not be the same, they're agent names. So you can do something like this:\npython blocking_send_recv_example.py  --name target_007 --zmq_ip localhost --mode target\npython blocking_send_recv_example.py  --name james_bond --zmq_ip localhost\n\nAlso sometimes using localhost causes issues in some systems, better to do 127.0.0.1.\n\nLet us know if that fixes the problem."}, {"user": "gongwei-130", "created_at": "2025-04-02T04:44:33Z", "body": "yes, I think it is the name issue. The document should clarify that to avoid confusion."}, {"user": "mkhazraee", "created_at": "2025-04-02T05:51:02Z", "body": "Agreed, we're doing some improvements to the test environment, including more documentation, for sure will include this."}], "satisfaction_conditions": ["Clarification that different agent names must be used for initiator and target in the blocking_send_recv_example", "Documentation improvements that prevent confusion about parameter requirements", "Working configuration guidance for the blocking_send_recv_example", "Explanation of error messages related to agent configuration"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:30"}, "dockerfile": null, "language": "c++"}
{"number": 68, "title": "Question: what does `sorted` mean in  `nixlRegDList`?", "created_at": "2025-03-20T02:37:53Z", "closed_at": "2025-03-20T03:44:51Z", "commit_id": "06d8c69712227fdb09f81a6c26b400d2a8f1b9ee", "labels": [], "url": "https://github.com/ai-dynamo/nixl/issues/68", "body": "PR title.\nWondering when I should set `sorted` to true or false.\n\nThanks!", "comments_url": "https://api.github.com/repos/ai-dynamo/nixl/issues/68/comments", "author": "cassanof", "comments": [{"user": "mkhazraee", "created_at": "2025-03-20T03:03:25Z", "body": "If nixlXferDlist is sorted, it can benefit from some perf optimization when creating a handle. We kept it for nixlRegDlist, just as they have the same base class and we wanted to be consistent, but it doesn't provide any perf boost. We were debating to remove it from the python interface, just kept it for consistency reasons for now. Because even the slight possible perf improvement is not that useful, it's just during initialization and for a few memory regions. What you see in the test was just to make sure passing of optional arguments and so are working properly. We might update the test a little bit not to be misleading.\n\nAside from that, when submitting a transfer, if the nixlXferDlist is sorted, since the internal registered memory lists are also sorted, we can find the relevant information for each descriptor in linear time instead of NlogM, N being number of elements in the XferDList and M being number of registered memories for that specific backend and that memory type."}, {"user": "cassanof", "created_at": "2025-03-20T03:44:51Z", "body": "I see thank you!\n\n"}], "satisfaction_conditions": ["An explanation of the purpose and effect of the 'sorted' parameter in nixlRegDList", "Information about the practical impact of using the 'sorted' parameter", "Context about why the parameter exists in the API", "Guidance on when to use each option (true/false)"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:44"}, "dockerfile": null, "language": "c++"}
{"number": 2, "title": "Fix the folder distribution!", "created_at": "2025-01-17T21:49:03Z", "closed_at": "2025-01-17T22:17:09Z", "commit_id": "39541a4bbc4ab1251f2020d6c438a312ddbcbcca", "labels": [], "url": "https://github.com/iiMidknightii/PathMesh3D/issues/2", "body": "i tried downloading this addon since its description is exactly what i need, when i was downloading it, i noticed it have a lot of folders outside of the addons folder, this messes with anyone working directory, where should be only the users folders.\ni'm going to test it in a separated project.", "comments_url": "https://api.github.com/repos/iiMidknightii/PathMesh3D/issues/2/comments", "author": "lucasthomaz97", "comments": [{"user": "lucasthomaz97", "created_at": "2025-01-17T22:17:09Z", "body": "I just tested it in another project, i noticed that i only needed the content inside the addons folder."}, {"user": "iiMidknightii", "created_at": "2025-01-17T22:46:21Z", "body": "Yes, for the addon to work you only need the addons folder.  The others are necessary if you want to build the plugin yourself for other platforms (like Mac or Android).  You could also edit the source code files to better suit your needs.  When you download from the Godot AssetLib tab, it lets you choose which folders are downloaded and installed into your project.  There you could just select the addons folder only if that's all you need."}], "satisfaction_conditions": ["Clarification about which folders are essential for the addon to function", "Explanation of why additional folders exist in the distribution", "Information about how to obtain only the necessary files when downloading"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:08"}, "dockerfile": null, "language": "c++"}
{"number": 42, "title": "Fix compatability with IMGUI_DISABLE_OBSOLETE_FUNCTIONS", "created_at": "2025-01-02T12:29:24Z", "closed_at": "2025-01-03T03:25:03Z", "commit_id": "09c9458293adc8a63001f68e541e79f97fbe49dc", "labels": ["type:chore", "prio:medium", "status:done"], "url": "https://github.com/brenocq/implot3d/pull/42", "body": "Hello!\r\nI noticed that ImPlot3D currently does not compile when configured with ```IMGUI_DISABLE_OBSOLETE_FUNCTIONS```. In particular, the ```IM_FLOOR``` and ```IM_OFFSETOF``` macros are no longer available in this case. This pull request changes those calls to ```ImFloor``` and C++11's ```offsetof``` respectively.\r\n", "comments_url": "https://api.github.com/repos/brenocq/implot3d/issues/42/comments", "author": "bratpilz", "comments": [{"user": "bratpilz", "created_at": "2025-01-03T10:42:34Z", "body": "No problem, thanks for merging it so quickly! Where do you see these macros used in ImPlot exactly though? I can't seem to find any usage of IM_FLOOR or IM_OFFSETOF in the master branch. Are you talking about something else?"}, {"user": "brenocq", "created_at": "2025-01-03T19:30:42Z", "body": "> Where do you see these macros used in ImPlot exactly though? I can't seem to find any usage of IM_FLOOR or IM_OFFSETOF in the master branch. Are you talking about something else?\r\n\r\nOoooh I was testing with the latest release (`v0.16`), but I just checked the `master` branch and it is indeed already fixed there. I'll talk with @epezent about creating a new release!\r\n\r\nThank you again @bratpilz!"}], "satisfaction_conditions": ["A solution that allows ImPlot3D to compile when IMGUI_DISABLE_OBSOLETE_FUNCTIONS is defined", "Replacement of deprecated ImGui macros with their modern equivalents", "Clarification on where these macros are used in the codebase", "Information about version differences between release and master branches"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:00"}, "dockerfile": null, "language": "c++"}
{"number": 8, "title": "local libraries", "created_at": "2025-02-24T05:18:47Z", "closed_at": "2025-02-24T08:21:02Z", "commit_id": "15af1c8b15dd381cfddd97fa62178c1a35be3e49", "labels": [], "url": "https://github.com/livebook-dev/pythonx/issues/8", "body": "I'm having troubles using my own libraries.\nConsider this setup: in the root folder of the app, I have a folder called `plibs` which at the moment, for test purposes, it contains one very simple library generated with `uv`. Running `uv build` does what it needs to do, building the `.tar.gz` and the `.whl` . For testing purposes, I have a `ptest` folder where I import the above library as a dependency like this:\n```\ndependencies = [\n    \"firstlib\"\n  ]\n[tool.uv.sources]\n  firstlib = { path = \"../plibs/firstlib/dist/firstlib-0.1.0-py3-none-any.whl\" }\n```\nRunning it via `uv run` works as expected.\nNow, I want to use the same library in Pythonx. So, I'm configuring it like this:\n```\nconfig :pythonx, :uv_init,\n  pyproject_toml: \"\"\"\n  [project]\n  name = \"nwsite\"\n  version = \"0.0.0\"\n  requires-python = \"==3.12.*\"\n  dependencies = [\n    \"firstlib\"\n  ]\n  [tool.uv.sources]\n  firstlib = { path = \"plibs/firstlib/dist/firstlib-0.1.0-py3-none-any.whl\" }\n  \"\"\"\n```\n\nThe result is this:\n\n```\n==> pythonx\nCompiling 10 files (.ex)\nUsing CPython 3.12.8\nCreating virtual environment at: .venv\nerror: Distribution not found at: file:///...../_build/dev/lib/pythonx/priv/uv/project/plibs/firstlib/dist/firstlib-0.1.0-py3-none-any.whl\n\n== Compilation error in file lib/pythonx/application.ex ==\n** (RuntimeError) fetching Python and dependencies failed, see standard output for details\n    lib/pythonx/uv.ex:36: Pythonx.Uv.fetch/3\n    lib/pythonx/application.ex:24: (module)\ncould not compile dependency :pythonx, \"mix compile\" failed. Errors may have been logged above. You can recompile this dependency with \"mix deps.compile pythonx --force\", update it with \"mix deps.update pythonx\" or clean it with \"mix deps.clean pythonx\"\n```\nIt tries to load relatively to the _build directory, not the root directory of the project.\n\nSo, my question is: how can I configure a local uv-built library? Hardcoding the absolute path to the library (it works, tested it, but it doesn't feels very flexible to me) ? \nAny other options/ideas I could explore?\n\nThank you!\n ", "comments_url": "https://api.github.com/repos/livebook-dev/pythonx/issues/8/comments", "author": "dantodor", "comments": [{"user": "jonatanklosko", "created_at": "2025-02-24T06:01:02Z", "body": "Hey @dantodor! Since the config is an Elixir string, you can actually \"generate\" it to a reasonable extent, so what about this?\n\n```elixir\nconfig :pythonx, :uv_init,\n  pyproject_toml: \"\"\"\n  [project]\n  name = \"nwsite\"\n  version = \"0.0.0\"\n  requires-python = \"==3.12.*\"\n  dependencies = [\n    \"firstlib\"\n  ]\n  [tool.uv.sources]\n  firstlib = { path = \"#{File.cwd!()}/plibs/firstlib/dist/firstlib-0.1.0-py3-none-any.whl\" }\n  \"\"\"\n```"}, {"user": "dantodor", "created_at": "2025-02-24T08:21:01Z", "body": "That did it, thanks for the great suggestion!\n"}], "satisfaction_conditions": ["A way to reference local Python libraries in Pythonx that works with relative paths", "A solution that maintains flexibility in project configuration", "A method to dynamically resolve paths in the Pythonx configuration", "A solution that integrates with the existing uv build workflow"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:45"}, "dockerfile": null, "language": "c++"}
{"number": 43, "title": "Add Custom Edge Sorting with Predicate Function added edgeRetrieval.cpp", "created_at": "2025-01-15T14:37:41Z", "closed_at": "2025-01-18T14:58:18Z", "commit_id": "b60fe8c2f7a4dcd67abdbde03827dcd37ff4e999", "labels": ["enhancement", "ADVANCED", "SWOC"], "url": "https://github.com/SharonIV0x86/Appledore/pull/43", "body": "closes: #34 \r\n\r\nTested and works", "comments_url": "https://api.github.com/repos/SharonIV0x86/Appledore/issues/43/comments", "author": "ash01825", "comments": [{"user": "SharonIV0x86", "created_at": "2025-01-16T10:00:26Z", "body": "@ash01825 I am not able to edit these files in the PR, maybe you have disabled the option that allows maintainers to edit the code. \r\n\r\nThere are a few changes need to be done, the first one is to include ``<functional>`` header in ``GraphMatrix.h`` without which ``std::function`` wont work.\r\nSecond one is to add a check in your ``getAllEdges`` function, if the graph is unweighted you cannot really return anything, so throw an exception at that point.\r\n\r\nIf you can enable the option that allows me to edit the code in the PR then it will be good, as i have to do some changes in example and the function also."}, {"user": "ash01825", "created_at": "2025-01-16T10:12:18Z", "body": "shouldn't getAllEdges return all existing Edges for Unweighted Graphs too?"}, {"user": "SharonIV0x86", "created_at": "2025-01-16T10:26:41Z", "body": "> shouldn't getAllEdges return all existing Edges for Unweighted Graphs too?\r\n\r\nInteresting, well yes it can, but in the returned tuple \r\n```cpp\r\nstd::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n```\r\nthe ``EdgeType`` will be ``UnweightedG`` and user cannot actually use ``UnweightedG`` anywhere, maybe there is a way we can return \r\nthis for weighted graphs\r\n```cpp\r\nstd::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n```\r\nand this for unweighted graphs?\r\n```cpp\r\nstd::vector<std::tuple<VertexType, VertexType>\r\n```"}, {"user": "SharonIV0x86", "created_at": "2025-01-16T10:46:18Z", "body": "@ash01825 Possibly we can utilize ``std::variant`` but will require more code. although this is not that important as of now.\r\n\r\n> > shouldn't getAllEdges return all existing Edges for Unweighted Graphs too?\r\n> \r\n> Interesting, well yes it can, but in the returned tuple\r\n> \r\n> ```c++\r\n> std::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n> ```\r\n> \r\n> the `EdgeType` will be `UnweightedG` and user cannot actually use `UnweightedG` anywhere, maybe there is a way we can return this for weighted graphs\r\n> \r\n> ```c++\r\n> std::vector<std::tuple<VertexType, VertexType, EdgeType>\r\n> ```\r\n> \r\n> and this for unweighted graphs?\r\n> \r\n> ```c++\r\n> std::vector<std::tuple<VertexType, VertexType>\r\n> ```\r\n\r\n"}, {"user": "SharonIV0x86", "created_at": "2025-01-17T05:22:48Z", "body": "@ash01825 any update?"}, {"user": "ash01825", "created_at": "2025-01-17T08:12:16Z", "body": "yeah sorry was out yesterday yeah I've made the changes"}, {"user": "SharonIV0x86", "created_at": "2025-01-17T08:23:22Z", "body": "> yeah sorry was out yesterday yeah I've made the changes\r\n\r\nIts fine no issues, the thing i am concerned about is that i want to make some changes to your current example file in this PR, but i dont have the permission to do so as you must have unchecked the ``allow maintainers to edit files`` while making this PR due to which i am not able to edit the files. \r\n\r\nSo either you give me permission to edit the code or i'll have to make those changes after merging the PR, your call."}, {"user": "ash01825", "created_at": "2025-01-17T11:22:39Z", "body": "Yeah I've turned on the allow edit my maintainers👍"}, {"user": "SharonIV0x86", "created_at": "2025-01-17T12:39:04Z", "body": "@ash01825 I have approved the changes and PR will be merged in sometime. Till then you are free to work on some other issue."}, {"user": "SharonIV0x86", "created_at": "2025-01-18T14:58:53Z", "body": "@ash01825 The PR is merged, and points are assigned to you. Thank you for contributing, kindly star ⭐ the repository as it shows appreciation to repository maintainers for their work."}], "satisfaction_conditions": ["Inclusion of necessary header files for the implementation", "Proper handling of edge retrieval for both weighted and unweighted graphs", "Enabling maintainer edit permissions on the PR", "Functional implementation that passes testing", "Addressing all feedback from code review"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:00:21"}, "dockerfile": "FROM ubuntu:22.04\n\n# Set noninteractive installation to avoid prompts\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    cmake \\\n    g++ \\\n    make \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/SharonIV0X86/Appledore.git && \\\n    cd Appledore && \\\n    git checkout b60fe8c2f7a4dcd67abdbde03827dcd37ff4e999\n\n# Set up a build directory\nWORKDIR /app/Appledore/build\n\n# Generate build system with CMake if there's a CMakeLists.txt, otherwise prepare for manual build\nRUN if [ -f ../CMakeLists.txt ]; then \\\n        cmake ..; \\\n    else \\\n        echo \"No CMakeLists.txt found. The project may require manual build.\"; \\\n        mkdir -p include examples; \\\n    fi\n\n# Build the project if it has a CMakeLists.txt\nRUN if [ -f ../CMakeLists.txt ]; then \\\n        make; \\\n    else \\\n        echo \"Project ready for manual compilation.\"; \\\n    fi\n\n# Set the working directory back to the project root\nWORKDIR /app/Appledore\n\n# The container is now ready with the project built or prepared for building\n# Users can compile examples or work with the library headers as needed\nCMD [\"/bin/bash\"]", "language": "c++"}
{"number": 19, "title": "Regex.hpp:35:31: error: unknown type name 'uint32_t'", "created_at": "2025-02-21T19:37:37Z", "closed_at": "2025-02-22T00:00:30Z", "commit_id": "1be2a22fd57adf71509fd1d5150d56ea146a0d16", "labels": [], "url": "https://github.com/msqr1/importizer/issues/19", "body": "**Issue:** Doesn't compile with error from above.\n\n**Test system**\n```\nSystem:\n  Kernel: 6.12.13_1 arch: x86_64 bits: 64 compiler: gcc v: 13.2.0 clocksource: tsc\n  Distro: Void Linux\nCPU:\n  Info: quad core model: 11th Gen Intel Core i7-1165G7 bits: 64 type: MT MCP smt: enabled\n    arch: Tiger Lake rev: 1 cache: L1: 320 KiB L2: 5 MiB L3: 12 MiB\n  Speed (MHz): avg: 1389 min/max: 400/4700 cores: 1: 1389 2: 1389 3: 1389 4: 1389 5: 1389 6: 1389\n    7: 1389 8: 1389 bogomips: 44851\n  Flags: avx avx2 ht lm nx pae sse sse2 sse3 sse4_1 sse4_2 ssse3 vmx\n  Compilers: clang: 19.1.4 gcc: 13.2.0 Client: Cinnamon v: 6.4.2 inxi: 3.3.37\n```\n**Commands run**\n\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Release > CMakeSetup.Clang19.1.4.log 2>&1 && cmake --build build --config Release -j $(cmake -P nproc.cmake) > CMakeBuild.Clang19.1.4.log 2>&1\ncmake -B build -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_COMPILER=/usr/local/llvm/bin/clang -DCMAKE_CXX_COMPILER=/usr/local/llvm/bin/clang++ > CMakeSetup.CLang21.0.0git.log 2>&1 && cmake --build build --config Release -j $(cmake -P nproc.cmake) > CMakeBuild.CLang21.0.0git.log 2>&1\n```\n\n**Output (clang 19)**\n```\n$ cat CMakeSetup.Clang19.1.4.log\n-- The C compiler identification is Clang 19.1.4\n-- The CXX compiler identification is Clang 19.1.4\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/clang - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/clang++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.48.1\")\n-- Performing Test OFP\n-- Performing Test OFP - Success\n-- Performing Test UBSAN\n-- Performing Test UBSAN - Success\n-- Performing Test noRTTI\n-- Performing Test noRTTI - Success\n-- Performing Test LTO\n-- Performing Test LTO - Success\nSingle-config generator detected\n-- {fmt} version: 11.1.3\n-- Build type: Release\n-- Performing Test HAS_NULLPTR_WARNING\n-- Performing Test HAS_NULLPTR_WARNING - Success\n-- Found BZip2: /usr/lib/libbz2.so (found version \"1.0.8\")\n-- Looking for BZ2_bzCompressInit\n-- Looking for BZ2_bzCompressInit - found\n-- Found ZLIB: /usr/lib/libz.so (found version \"1.3.1\")\n-- Could NOT find Readline (missing: READLINE_INCLUDE_DIR READLINE_LIBRARY) \n-- Found Editline: /usr/include/editline\n-- Looking for assert.h\n-- Looking for assert.h - found\n-- Looking for dirent.h\n-- Looking for dirent.h - found\n-- Looking for sys/stat.h\n-- Looking for sys/stat.h - found\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for unistd.h\n-- Looking for unistd.h - found\n-- Looking for windows.h\n-- Looking for windows.h - not found\n-- Looking for bcopy\n-- Looking for bcopy - found\n-- Looking for memfd_create\n-- Looking for memfd_create - not found\n-- Looking for memmove\n-- Looking for memmove - found\n-- Looking for secure_getenv\n-- Looking for secure_getenv - not found\n-- Looking for strerror\n-- Looking for strerror - found\n-- Performing Test HAVE_REALPATH\n-- Performing Test HAVE_REALPATH - Success\n-- Performing Test HAVE_ATTRIBUTE_UNINITIALIZED\n-- Performing Test HAVE_ATTRIBUTE_UNINITIALIZED - Success\n-- Performing Test HAVE_VISIBILITY\n-- Performing Test HAVE_VISIBILITY - Success\n-- Performing Test HAVE_BUILTIN_ASSUME\n-- Performing Test HAVE_BUILTIN_ASSUME - Failed\n-- Performing Test HAVE_BUILTIN_MUL_OVERFLOW\n-- Performing Test HAVE_BUILTIN_MUL_OVERFLOW - Success\n-- Performing Test HAVE_BUILTIN_UNREACHABLE\n-- Performing Test HAVE_BUILTIN_UNREACHABLE - Success\n-- Performing Test INTEL_CET_ENABLED\n-- Performing Test INTEL_CET_ENABLED - Failed\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- \n-- \n-- PCRE2-10.45 configuration summary:\n-- \n--   Install prefix .................... : /usr/local\n--   C compiler ........................ : /usr/bin/clang\n--   C compiler flags (Debug) .......... : -g\n--   C compiler flags (Release) ........ : -O3 -DNDEBUG\n-- \n--   Build configurations .............. : Debug;Release\n--   Build 8 bit PCRE2 library ......... : ON\n--   Build 16 bit PCRE2 library ........ : OFF\n--   Build 32 bit PCRE2 library ........ : OFF\n--   Include debugging code ............ : IfDebugBuild\n--   Enable JIT compiling support ...... : on\n--   Use SELinux allocator in JIT ...... : OFF\n--   Enable Unicode support ............ : ON\n--   Newline char/sequence ............. : LF\n--   \\R matches only ANYCRLF ........... : OFF\n--   \\C is disabled .................... : OFF\n--   EBCDIC coding ..................... : OFF\n--   EBCDIC coding with NL=0x25 ........ : OFF\n--   Rebuild char tables ............... : OFF\n--   Internal link size ................ : 2\n--   Maximum variable lookbehind ....... : 255\n--   Parentheses nest limit ............ : 250\n--   Heap limit ........................ : 20000000\n--   Match limit ....................... : 10000000\n--   Match depth limit ................. : MATCH_LIMIT\n--   Build shared libs ................. : OFF\n--   Build static libs ................. : ON\n--      with PIC enabled ............... : OFF\n--   Build pcre2grep ................... : off\n--   Enable JIT in pcre2grep ........... : ON\n--   Enable callouts in pcre2grep ...... : ON\n--   Enable callout fork in pcre2grep .. : ON\n--   Buffer size for pcre2grep ......... : 20480\n--   Build tests (implies pcre2test .... : off\n--                and pcre2grep)\n--   Link pcre2grep with libz .......... : ON\n--   Link pcre2grep with libbz2 ........ : ON\n--   Link pcre2test with libeditline ... : OFF\n--   Link pcre2test with libreadline ... : Library not found\n--   Support Valgrind .................. : OFF\n--   Use %zu and %td ................... : AUTO\n-- \n-- Configuring done (3.5s)\n-- Generating done (0.0s)\n-- Build files have been written to: /home/programming/importizer/build\n$ cat CMakeBuild.Clang19.1.4.log \n[  2%] Building CXX object fmt/CMakeFiles/fmt.dir/src/format.cc.o\n[  4%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_auto_possess.c.o\n[  8%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/pcre2_chartables.c.o\n[  8%] Building CXX object fmt/CMakeFiles/fmt.dir/src/os.cc.o\n[ 11%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_chkdint.c.o\n[ 13%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_compile.c.o\n[ 15%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_compile_class.c.o\n[ 17%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_config.c.o\n[ 20%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_context.c.o\n[ 22%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_convert.c.o\n[ 24%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_dfa_match.c.o\n[ 26%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_error.c.o\n[ 28%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_extuni.c.o\n[ 31%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_find_bracket.c.o\n[ 33%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_jit_compile.c.o\n[ 35%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_maketables.c.o\n[ 37%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_match.c.o\n[ 40%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_match_data.c.o\n[ 42%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_newline.c.o\n[ 44%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_ord2utf.c.o\n[ 46%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_pattern_info.c.o\n[ 48%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_script_run.c.o\n[ 51%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_serialize.c.o\n[ 53%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_string_utils.c.o\n[ 55%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_study.c.o\n[ 57%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_substitute.c.o\n[ 60%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_substring.c.o\n[ 62%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_tables.c.o\n[ 64%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_ucd.c.o\n[ 66%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_valid_utf.c.o\n[ 68%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_xclass.c.o\n[ 71%] Linking CXX static library libfmt.a\n[ 71%] Built target fmt\n[ 73%] Linking C static library libpcre2-8.a\n[ 73%] Built target pcre2-8-static\n[ 75%] Building C object pcre2/CMakeFiles/pcre2-posix-static.dir/src/pcre2posix.c.o\n[ 77%] Building CXX object src/CMakeFiles/importizer.dir/Base.cc.o\n[ 80%] Building CXX object src/CMakeFiles/importizer.dir/Regex.cc.o\n[ 82%] Building CXX object src/CMakeFiles/importizer.dir/OptProcessor.cc.o\n[ 84%] Linking C static library libpcre2-posix.a\n[ 84%] Built target pcre2-posix-static\n[ 86%] Building CXX object src/CMakeFiles/importizer.dir/FileOp.cc.o\nIn file included from /home/programming/importizer/src/OptProcessor.cc:1:\nIn file included from /home/programming/importizer/src/OptProcessor.hpp:2:\n/home/programming/importizer/src/Regex.hpp:35:31: error: unknown type name 'uint32_t'\n   35 |   Regex(std::string_view pat, uint32_t opts = 0);\n      |                               ^\n/home/programming/importizer/src/Regex.hpp:36:38: error: unknown type name 'uint32_t'\n   36 |   Regex& reset(std::string_view pat, uint32_t opts = 0);\n      |                                      ^\nIn file included from /home/programming/importizer/src/Regex.cc:1:\n/home/programming/importizer/src/Regex.hpp:35:31: error: unknown type name 'uint32_t'\n   35 |   Regex(std::string_view pat, uint32_t opts = 0);\n      |                               ^\n/home/programming/importizer/src/Regex.hpp:36:38: error: unknown type name 'uint32_t'\n   36 |   Regex& reset(std::string_view pat, uint32_t opts = 0);\n      |                                      ^\n/home/programming/importizer/src/Regex.cc:39:8: error: out-of-line definition of 'Regex' does not match any declaration in 'Regex'\n   39 | Regex::Regex(std::string_view pat, uint32_t opts) {\n      |        ^~~~~\n/home/programming/importizer/src/Regex.cc:42:15: error: out-of-line definition of 'reset' does not match any declaration in 'Regex'\n   42 | Regex& Regex::reset(std::string_view pat, uint32_t opts) {\n      |               ^~~~~\n[ 88%] Building CXX object src/CMakeFiles/importizer.dir/Main.cc.o\n4 errors generated.\nmake[2]: *** [src/CMakeFiles/importizer.dir/build.make:90: src/CMakeFiles/importizer.dir/Regex.cc.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\n2 errors generated.\nmake[2]: *** [src/CMakeFiles/importizer.dir/build.make:104: src/CMakeFiles/importizer.dir/OptProcessor.cc.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:154: src/CMakeFiles/importizer.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\n```\n\n**Output (Clang 21)**\n```\n$ cat CMakeSetup.CLang21.0.0git.log \n-- The C compiler identification is Clang 21.0.0\n-- The CXX compiler identification is Clang 21.0.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/local/llvm/bin/clang - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/local/llvm/bin/clang++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.48.1\")\n-- Performing Test OFP\n-- Performing Test OFP - Success\n-- Performing Test UBSAN\n-- Performing Test UBSAN - Failed\n-- Performing Test noRTTI\n-- Performing Test noRTTI - Success\n-- Performing Test LTO\n-- Performing Test LTO - Failed\nSingle-config generator detected\n-- {fmt} version: 11.1.3\n-- Build type: Release\n-- Performing Test HAS_NULLPTR_WARNING\n-- Performing Test HAS_NULLPTR_WARNING - Success\n-- Found BZip2: /usr/lib/libbz2.so (found version \"1.0.8\")\n-- Looking for BZ2_bzCompressInit\n-- Looking for BZ2_bzCompressInit - found\n-- Found ZLIB: /usr/lib/libz.so (found version \"1.3.1\")\n-- Could NOT find Readline (missing: READLINE_INCLUDE_DIR READLINE_LIBRARY) \n-- Found Editline: /usr/include/editline\n-- Looking for assert.h\n-- Looking for assert.h - found\n-- Looking for dirent.h\n-- Looking for dirent.h - found\n-- Looking for sys/stat.h\n-- Looking for sys/stat.h - found\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for unistd.h\n-- Looking for unistd.h - found\n-- Looking for windows.h\n-- Looking for windows.h - not found\n-- Looking for bcopy\n-- Looking for bcopy - found\n-- Looking for memfd_create\n-- Looking for memfd_create - not found\n-- Looking for memmove\n-- Looking for memmove - found\n-- Looking for secure_getenv\n-- Looking for secure_getenv - not found\n-- Looking for strerror\n-- Looking for strerror - found\n-- Performing Test HAVE_REALPATH\n-- Performing Test HAVE_REALPATH - Success\n-- Performing Test HAVE_ATTRIBUTE_UNINITIALIZED\n-- Performing Test HAVE_ATTRIBUTE_UNINITIALIZED - Success\n-- Performing Test HAVE_VISIBILITY\n-- Performing Test HAVE_VISIBILITY - Success\n-- Performing Test HAVE_BUILTIN_ASSUME\n-- Performing Test HAVE_BUILTIN_ASSUME - Failed\n-- Performing Test HAVE_BUILTIN_MUL_OVERFLOW\n-- Performing Test HAVE_BUILTIN_MUL_OVERFLOW - Success\n-- Performing Test HAVE_BUILTIN_UNREACHABLE\n-- Performing Test HAVE_BUILTIN_UNREACHABLE - Success\n-- Performing Test INTEL_CET_ENABLED\n-- Performing Test INTEL_CET_ENABLED - Failed\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- \n-- \n-- PCRE2-10.45 configuration summary:\n-- \n--   Install prefix .................... : /usr/local\n--   C compiler ........................ : /usr/local/llvm/bin/clang\n--   C compiler flags (Debug) .......... : -g\n--   C compiler flags (Release) ........ : -O3 -DNDEBUG\n-- \n--   Build configurations .............. : Debug;Release\n--   Build 8 bit PCRE2 library ......... : ON\n--   Build 16 bit PCRE2 library ........ : OFF\n--   Build 32 bit PCRE2 library ........ : OFF\n--   Include debugging code ............ : IfDebugBuild\n--   Enable JIT compiling support ...... : on\n--   Use SELinux allocator in JIT ...... : OFF\n--   Enable Unicode support ............ : ON\n--   Newline char/sequence ............. : LF\n--   \\R matches only ANYCRLF ........... : OFF\n--   \\C is disabled .................... : OFF\n--   EBCDIC coding ..................... : OFF\n--   EBCDIC coding with NL=0x25 ........ : OFF\n--   Rebuild char tables ............... : OFF\n--   Internal link size ................ : 2\n--   Maximum variable lookbehind ....... : 255\n--   Parentheses nest limit ............ : 250\n--   Heap limit ........................ : 20000000\n--   Match limit ....................... : 10000000\n--   Match depth limit ................. : MATCH_LIMIT\n--   Build shared libs ................. : OFF\n--   Build static libs ................. : ON\n--      with PIC enabled ............... : OFF\n--   Build pcre2grep ................... : off\n--   Enable JIT in pcre2grep ........... : ON\n--   Enable callouts in pcre2grep ...... : ON\n--   Enable callout fork in pcre2grep .. : ON\n--   Buffer size for pcre2grep ......... : 20480\n--   Build tests (implies pcre2test .... : off\n--                and pcre2grep)\n--   Link pcre2grep with libz .......... : ON\n--   Link pcre2grep with libbz2 ........ : ON\n--   Link pcre2test with libeditline ... : OFF\n--   Link pcre2test with libreadline ... : Library not found\n--   Support Valgrind .................. : OFF\n--   Use %zu and %td ................... : AUTO\n-- \n-- Configuring done (6.0s)\n-- Generating done (0.0s)\n-- Build files have been written to: /home/programming/importizer/build\n$ cat CMakeBuild.CLang21.0.0git.log \n[  2%] Building CXX object fmt/CMakeFiles/fmt.dir/src/os.cc.o\n[  4%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/pcre2_chartables.c.o\n[  6%] Building CXX object fmt/CMakeFiles/fmt.dir/src/format.cc.o\n[  8%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_auto_possess.c.o\n[ 11%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_chkdint.c.o\n[ 13%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_compile.c.o\n[ 15%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_compile_class.c.o\n[ 17%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_config.c.o\n[ 20%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_context.c.o\n[ 22%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_convert.c.o\n[ 24%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_dfa_match.c.o\n[ 26%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_error.c.o\n[ 28%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_extuni.c.o\n[ 31%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_find_bracket.c.o\n[ 33%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_jit_compile.c.o\n[ 35%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_maketables.c.o\n[ 37%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_match.c.o\n[ 40%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_match_data.c.o\n[ 42%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_newline.c.o\n[ 44%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_ord2utf.c.o\n[ 46%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_pattern_info.c.o\n[ 48%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_script_run.c.o\n[ 51%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_serialize.c.o\n[ 53%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_string_utils.c.o\n[ 55%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_study.c.o\n[ 57%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_substitute.c.o\n[ 60%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_substring.c.o\n[ 62%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_tables.c.o\n[ 64%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_ucd.c.o\n[ 66%] Linking CXX static library libfmt.a\n[ 66%] Built target fmt\n[ 68%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_valid_utf.c.o\n[ 71%] Building C object pcre2/CMakeFiles/pcre2-8-static.dir/src/pcre2_xclass.c.o\n[ 73%] Linking C static library libpcre2-8.a\n[ 73%] Built target pcre2-8-static\n[ 75%] Building CXX object src/CMakeFiles/importizer.dir/OptProcessor.cc.o\n[ 77%] Building CXX object src/CMakeFiles/importizer.dir/Base.cc.o\n[ 80%] Building CXX object src/CMakeFiles/importizer.dir/Regex.cc.o\n[ 82%] Building C object pcre2/CMakeFiles/pcre2-posix-static.dir/src/pcre2posix.c.o\n[ 84%] Linking C static library libpcre2-posix.a\n[ 84%] Built target pcre2-posix-static\n[ 86%] Building CXX object src/CMakeFiles/importizer.dir/FileOp.cc.o\nIn file included from /home/programming/importizer/src/Regex.cc:1:\n/home/programming/importizer/src/Regex.hpp:35:31: error: unknown type name 'uint32_t'\n   35 |   Regex(std::string_view pat, uint32_t opts = 0);\n      |                               ^\n/home/programming/importizer/src/Regex.hpp:36:38: error: unknown type name 'uint32_t'\n   36 |   Regex& reset(std::string_view pat, uint32_t opts = 0);\n      |                                      ^\nIn file included from /home/programming/importizer/src/OptProcessor.cc:1:\nIn file included from /home/programming/importizer/src/OptProcessor.hpp:2:\n/home/programming/importizer/src/Regex.hpp:35:31: error: unknown type name 'uint32_t'\n   35 |   Regex(std::string_view pat, uint32_t opts = 0);\n      |                               ^\n/home/programming/importizer/src/Regex.hpp:36:38: error: unknown type name 'uint32_t'\n   36 |   Regex& reset(std::string_view pat, uint32_t opts = 0);\n      |                                      ^\n/home/programming/importizer/src/Regex.cc:39:8: error: out-of-line definition of 'Regex' does not match any declaration in 'Regex'\n   39 | Regex::Regex(std::string_view pat, uint32_t opts) {\n      |        ^~~~~\n/home/programming/importizer/src/Regex.hpp:28:7: note: Regex defined here\n   28 | class Regex {\n      |       ^~~~~\n/home/programming/importizer/src/Regex.cc:42:15: error: out-of-line definition of 'reset' does not match any declaration in 'Regex'\n   42 | Regex& Regex::reset(std::string_view pat, uint32_t opts) {\n      |               ^~~~~\n/home/programming/importizer/src/Regex.hpp:28:7: note: Regex defined here\n   28 | class Regex {\n      |       ^~~~~\n4 errors generated.\nmake[2]: *** [src/CMakeFiles/importizer.dir/build.make:90: src/CMakeFiles/importizer.dir/Regex.cc.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\n2 errors generated.\nmake[2]: *** [src/CMakeFiles/importizer.dir/build.make:104: src/CMakeFiles/importizer.dir/OptProcessor.cc.o] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:154: src/CMakeFiles/importizer.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\n```\n\n**Fix**\nAdd missing include statement to `src/Regex.hpp`. \nPossibly `#include <cstdint>`.", "comments_url": "https://api.github.com/repos/msqr1/importizer/issues/19/comments", "author": "it-sthoffmann", "comments": [{"user": "msqr1", "created_at": "2025-02-21T23:23:14Z", "body": "Yeah, my IDE is warning about that, too, but I didn't notice. Man, I hate the implicit includes... Modules are sooooo much better. Thank you so much!"}], "satisfaction_conditions": ["Identification of the missing header file needed to define uint32_t", "A simple, direct fix that requires minimal code changes", "Explanation of why the error occurs"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:09"}, "dockerfile": null, "language": "c++"}
{"number": 40, "title": "Enabled META and ALT keys to work in Emacs by making them send Escape", "created_at": "2025-03-09T01:03:09Z", "closed_at": "2025-03-13T00:28:38Z", "commit_id": "9b424b5ad6cfc133de24c630b56f8e12ad36a22f", "labels": [], "url": "https://github.com/markeel/gdterm/pull/40", "body": "I tried your addon and it is fantastic. The only issue I had is I wanted to run Emacs within the terminal but the M-<key> combinations were not working, so I have made a small update to make the META and ALT keys send an Escape to resolve that issue.", "comments_url": "https://api.github.com/repos/markeel/gdterm/issues/40/comments", "author": "MichaelBScott", "comments": [{"user": "markeel", "created_at": "2025-03-10T16:02:55Z", "body": "I took a look at the change and I think this is on the right path, but what other terminals do is set a config option for this kind of behavior or react to an escape code being sent.  The libtmt library only did things that matches what an ANSI term type would do, but I have since extended that because it was inadequate for MS windows.  If emacs sends an escape code to have an escape sent for meta, ten I'd prefer to react to that.  Btw were you using emacs on Linux or Windows?"}, {"user": "MichaelBScott", "created_at": "2025-03-10T18:59:05Z", "body": "Thank you for the info, it sounds like it needs a more complete solution. The change I made was based on how to resolve the same issue when using XTerm by setting:\r\n\r\n XTerm.vt100.metaSendsEscape: true\r\n\r\nAdditionally, since pushing that change I have also determined CTRL doesn't quite fully work either. C-/ ended up writing the / character instead of performing the action for the keybinding.\r\n\r\nOh, and I am using Linux."}, {"user": "markeel", "created_at": "2025-03-11T21:01:53Z", "body": "I'm not sure there is really any downside to how you made the change since I'm not sure when or why you might accidentally send an escape by just pressing the alt or meta keys, but since xterm made it configurable I'm leaning to adding that to the GDTerm settings.\r\n\r\nBTW the Ctrl-/ is broken and I'll add an issue.  The range in the Godot key didn't match for the characters '/' and '~' so that's why that's not working.\r\n\r\nI'm not really an emacs user, but I loaded it onto my system and will do a few tests as well."}, {"user": "markeel", "created_at": "2025-03-11T21:41:47Z", "body": "BTW when I tried to use 'emacs -nw' on my Ubuntu system it did not behave well until I changed the TERM environment variable to \"xterm-256color\", so I tested it in my standard Gnome Terminal and it didn't behave well with that TERM either.\r\n\r\nThe library I used (libtmt) doesn't really attempt to do a full TERM=xterm-256color terminal emulation but it is apparently close enough that when using emacs it seemed to behave much better.  Not sure why.  It may be that emacs did some things that weren't quite compatible with a terminal as primitive as TERM=ansi.\r\n\r\n"}, {"user": "markeel", "created_at": "2025-03-11T21:49:04Z", "body": "I merged a change for Ctrl-/ but you will need to merge your updates and compile from source to check them out."}, {"user": "MichaelBScott", "created_at": "2025-03-13T00:28:14Z", "body": "Hi @markeel,\r\nI have built the version using your latest changes from the main branch and as far as I can tell emacs is now fully working with the Send ALT Meta as ESC option selected.\r\n\r\nC-/ is now performing the correct action for the keybinding.\r\nAlso, setting TERM=xterm-256-color seems to work correctly for me.\r\n\r\nThis pull request is no longer required so I am closing it.\r\n\r\nThank you so much for your help.\r\n"}, {"user": "markeel", "created_at": "2025-03-13T04:41:06Z", "body": "> Hi @markeel, I have built the version using your latest changes from the main branch and as far as I can tell emacs is now fully working with the Send ALT Meta as ESC option selected.\r\n> \r\n> C-/ is now performing the correct action for the keybinding. Also, setting TERM=xterm-256-color seems to work correctly for me.\r\n> \r\n> This pull request is no longer required so I am closing it.\r\n> \r\n> Thank you so much for your help.\r\n\r\nThanks for using the plugin and providing feedback!  And I'm glad the changes are working.  \r\n\r\nI'll release a new version when I fix the issue with background color, but that might be toward the end of the month."}], "satisfaction_conditions": ["Support for META and ALT key combinations in Emacs within the terminal", "Proper handling of CTRL key combinations in the terminal", "Configurable option for META/ALT key behavior", "Compatibility with proper terminal environment settings"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:40"}, "dockerfile": null, "language": "c++"}
{"number": 39, "title": "Custom termial program selection", "created_at": "2025-02-25T15:02:59Z", "closed_at": "2025-02-26T01:38:02Z", "commit_id": "9b424b5ad6cfc133de24c630b56f8e12ad36a22f", "labels": [], "url": "https://github.com/markeel/gdterm/issues/39", "body": "Hi! It's an great project! And I tried on my windows PC. It is working as I expected!\nThe only thing that I was using gitbash as my default terminal. So if there has an option to change default terminal in editor setting that could be the last part I am looking forward to.\nThanks!\nBy the way. I tried to change \"cmd\" to \"bash\" in **PtyProxyWin**, But it is not working!", "comments_url": "https://api.github.com/repos/markeel/gdterm/issues/39/comments", "author": "hakuhan", "comments": [{"user": "markeel", "created_at": "2025-02-25T19:52:32Z", "body": "You can put the git bash command in your initial command list in the editor settings for gdterm\n\nIt will then execute that command immediately when the terminal starts"}, {"user": "hakuhan", "created_at": "2025-02-26T01:38:02Z", "body": "Thanks! I use `bash --login -i` as start command line to achieve it."}], "satisfaction_conditions": ["A way to use Git Bash as the default terminal in the editor", "A configuration method that works within the existing editor settings", "A solution that works on Windows"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:49"}, "dockerfile": null, "language": "c++"}
{"number": 157, "title": "如何修改storage_main的监听端口？我修改后还是端口冲突8000", "created_at": "2025-03-12T03:14:17Z", "closed_at": "2025-03-12T03:44:15Z", "commit_id": "3a30c53bf25c23a963aaa547098dcceff2c33baf", "labels": [], "url": "https://github.com/deepseek-ai/3FS/issues/157", "body": "### Issue Report\n### 标题\nstorage_main 进程在初始化时因端口冲突崩溃\n\n### 问题描述\nstorage_main 进程在初始化时崩溃，并生成了coredump文件。崩溃发生在尝试绑定到端口8000时，因为该端口已经被占用。\n\n### 环境信息\n操作系统: Ubuntu\n程序路径: /opt/3fs/bin/storage_main\n### 配置文件路径:\n/opt/3fs/etc/storage_main_launcher.toml\n/opt/3fs/etc/storage_main.toml\n### 崩溃信息\n信号: SIGABRT (Aborted)\n进程ID: 99874\n线程ID: 0x7f7c290f8b80 (LWP 99874)\n### 调用栈信息\n<TEXT>\n@ 00000000011e5aef _ZN5folly10symbolizer12_GLOBAL__N_113signalHandlerEiP9siginfo_tPv /root/3fs/3fs/third_party/folly/folly/experimental/symbolizer/SignalHandler.cpp:449\n@ 000000000004251f (unknown)\n@ 00000000000969fc pthread_kill\n@ 0000000000042475 raise\n@ 00000000000287f2 abort\n@ 000000000124c220 _ZNK5folly11LogCategory12admitMessageERKNS_10LogMessageE /root/3fs/3fs/third_party/folly/folly/logging/LogCategory.cpp:71\n@ 000000000124b6d6 _ZN5folly18LogStreamProcessor6logNowEv /root/3fs/3fs/third_party/folly/folly/logging/LogStreamProcessor.cpp:190\n@ 000000000124b929 _ZN5folly16LogStreamVoidifyILb1EEanERSo /root/3fs/3fs/third_party/folly/folly/logging/LogStreamProcessor.cpp:222\n@ 00000000022a7c48 _ZN5hf3fs19TwoPhaseApplicationINS_7storage13StorageServerEE15initApplicationEv /root/3fs/3fs/src/common/app/TwoPhaseApplication.h:59 -> /root/3fs/3fs/src/storage/storage.cpp\n@ 0000000000f79956 _ZN5hf3fs15ApplicationBase3runEiPPc /root/3fs/3fs/src/common/app/ApplicationBase.cc:65\n@ 00000000022a4a43 main /root/3fs/3fs/src/storage/storage.cpp:7\n@ 0000000000029d8f (unknown)\n@ 0000000000029e3f __libc_start_main\n@ 00000000007eb964 _start\n### 日志信息\n<TEXT>\n[2025-03-12T11:02:58.636580924+08:00 storage_main:99874 Listener.cc:102 ERROR] create socket failed: std::system_error: failed to bind to async server socket: 192.168.223.128:8000: Address already in use\n[2025-03-12T11:02:58.636735964+08:00 storage_main:99874 ServiceGroup.cc:26 ERROR] error: RPC::ListenFailed(2011)\n[2025-03-12T11:02:58.636880029+08:00 storage_main:99874 Server.cc:27 ERROR] Setup group (StorageSerde) failed: RPC::ListenFailed(2011)\n[2025-03-12T11:02:58.637110823+08:00 storage_main:99874 Server.cc:31 ERROR] Server::setup failed: RPC::ListenFailed(2011)\n[2025-03-12T11:02:58.637139597+08:00 storage_main:99874 TwoPhaseApplication.h:59 FATAL] Init server failed: RPC::ListenFailed(2011)\n### 配置信息\nmgmtd_server_addresses = [\"RDMA://192.168.223.128:8000\"] (出现在多个配置文件中)\nlisten_port = 18000 (出现在某些配置文件中)\nlisten_port = 9000 (出现在某些配置文件中)\n### 详细信息\n崩溃点: hf3fs::TwoPhaseApplication<hf3fs::storage::StorageServer>::initApplication 方法中\n### 相关代码路径:\n/root/3fs/3fs/src/common/app/TwoPhaseApplication.h:59\n/root/3fs/3fs/src/storage/storage.cpp\n/root/3fs/3fs/src/common/app/ApplicationBase.cc:65\n/root/3fs/3fs/src/storage/storage.cpp:7\n### 可能的原因\n端口冲突: 端口8000已经被其他进程占用，导致storage_main无法绑定到该端口。\n配置不一致: 配置文件中存在多个不同的listen_port设置，可能导致端口冲突。\n### 重现步骤\n启动 storage_main 进程，使用配置文件 /opt/3fs/etc/storage_main_launcher.toml 和 /opt/3fs/etc/storage_main.toml。\n", "comments_url": "https://api.github.com/repos/deepseek-ai/3FS/issues/157/comments", "author": "xiaoyaoyouyue", "comments": [{"user": "yuchengkang", "created_at": "2025-03-12T03:31:22Z", "body": "修改配置后需要重新执行 /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml  \"set-config --type STORAGE --file /opt/3fs/etc/storage_main.toml\""}, {"user": "xiaoyaoyouyue", "created_at": "2025-03-12T03:44:13Z", "body": "> 修改配置后需要重新执行 /opt/3fs/bin/admin_cli -cfg /opt/3fs/etc/admin_cli.toml \"set-config --type STORAGE --file /opt/3fs/etc/storage_main.toml\"\n\n多谢，"}], "satisfaction_conditions": ["Instructions on how to properly apply configuration changes to the storage_main service", "A process to ensure configuration changes take effect", "A solution that resolves the port conflict issue without requiring detailed debugging"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:02"}, "dockerfile": null, "language": "c++"}
{"number": 84, "title": "Why does create op need to be added in batcheop?", "created_at": "2025-03-04T07:19:28Z", "closed_at": "2025-03-04T11:38:36Z", "commit_id": "fc90de11107ad37512e2e0f3885be077341cb46f", "labels": [], "url": "https://github.com/deepseek-ai/3FS/issues/84", "body": "In my view, batch could be useful in occasion of concurrent setattr, to avoid foundation 'lock'. But how could create relative to this?", "comments_url": "https://api.github.com/repos/deepseek-ai/3FS/issues/84/comments", "author": "z47z", "comments": [{"user": "yiyuanliu", "created_at": "2025-03-04T11:09:40Z", "body": "For each file, 3FS selects chains from the chain table in a round-robin manner to store the file's data. By default, the counter used for round-robin selection is maintained by the meta server.\n\nThere is an optional feature (disabled by default and rarely used) where each directory maintains its own separate counter, which can be beneficial in certain scenarios. For example, when you plan to store a large amount of data in one directory and want the data to be distributed across all storage servers as evenly as possible, maintaining a separate round-robin counter for this directory might achieve better distribution than using the global counter. To reduce transaction conflicts caused by counter modification operations, we have chosen to include the create operation in the batchOp as well."}, {"user": "z47z", "created_at": "2025-03-04T11:38:36Z", "body": "> For each file, 3FS selects chains from the chain table in a round-robin manner to store the file's data. By default, the counter used for round-robin selection is maintained by the meta server.\n> \n> There is an optional feature (disabled by default and rarely used) where each directory maintains its own separate counter, which can be beneficial in certain scenarios. For example, when you plan to store a large amount of data in one directory and want the data to be distributed across all storage servers as evenly as possible, maintaining a separate round-robin counter for this directory might achieve better distribution than using the global counter. To reduce transaction conflicts caused by counter modification operations, we have chosen to include the create operation in the batchOp as well.\n\nComprehend,thx."}], "satisfaction_conditions": ["An explanation of the technical rationale for including create operations in batchOp", "Context about how the file system's chain selection mechanism works", "Clarification of the relationship between concurrent operations and the batchOp functionality"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:07"}, "dockerfile": null, "language": "c++"}
{"number": 53, "title": "question about the peak reading throughput of KV Cache", "created_at": "2025-03-03T06:26:56Z", "closed_at": "2025-03-03T07:13:56Z", "commit_id": "c69d46e7914b07dae9a14e79895da4f848d8a045", "labels": [], "url": "https://github.com/deepseek-ai/3FS/issues/53", "body": "The compute node just has a 1x200 Gbps NIC，so how the peak reading throughput of KV Cache can reach 40 GiB/s？\n\nDoes the KV cache storage system use the DRAM to save some hot KV Cache in the compute node？\n\nHope for your answer！Thank you！", "comments_url": "https://api.github.com/repos/deepseek-ai/3FS/issues/53/comments", "author": "DoubleEspresso-7", "comments": [{"user": "SF-Zhou", "created_at": "2025-03-03T07:05:42Z", "body": "The compute node for KVCache use a 1x400Gbps NIC."}, {"user": "DoubleEspresso-7", "created_at": "2025-03-03T07:13:53Z", "body": "Thank you！Maybe you can add this information in the KV Cache part to avoid misunderstanding.\n"}, {"user": "SF-Zhou", "created_at": "2025-03-03T08:21:14Z", "body": "> Thank you！Maybe you can add this information in the KV Cache part to avoid misunderstanding.\n\nDone!"}], "satisfaction_conditions": ["Clarification about the network interface card (NIC) specifications for the KV Cache compute node", "Documentation update to include accurate hardware specifications"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:14"}, "dockerfile": null, "language": "c++"}
{"number": 247, "title": "LFO (without sync) changes speed based on song tempo", "created_at": "2025-02-13T13:10:16Z", "closed_at": "2025-02-13T13:38:07Z", "commit_id": "fa45ec23037802a2e9254101f6d9c706f26e775b", "labels": [], "url": "https://github.com/baconpaul/six-sines/issues/247", "body": "DAW tempo controls or offsets LFO RATE even with SYNC OFF. That can't be right. \n", "comments_url": "https://api.github.com/repos/baconpaul/six-sines/issues/247/comments", "author": "Taronium", "comments": [{"user": "baconpaul", "created_at": "2025-02-13T13:24:39Z", "body": "Oh no really? "}, {"user": "Taronium", "created_at": "2025-02-13T13:28:33Z", "body": "Yes, haha. Really!\nKind of a little shocker, because I love playing with LFO as oscillator. With Key Tracking to 25% it tracks perfectly. But then... Bamm! "}, {"user": "baconpaul", "created_at": "2025-02-13T13:31:17Z", "body": "That s embarrassing but i have fixed it in #248\n\ngood catch and wow very sorry about that"}, {"user": "Taronium", "created_at": "2025-02-13T13:40:07Z", "body": "Awesome, Paul, no worries! You're doing a bang up job! 😎👍"}, {"user": "baconpaul", "created_at": "2025-02-13T14:10:17Z", "body": "I tested tempo sync extensively when I added it\nJust not in the off position \nlol "}], "satisfaction_conditions": ["Fix for LFO rate being affected by DAW tempo when sync is off", "Maintaining LFO's ability to function as an oscillator with key tracking", "Prompt acknowledgment and resolution of the issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:15"}, "dockerfile": null, "language": "c++"}
{"number": 173, "title": "Bass/Brand New Bass & Substrata are not tuned in the same way as the other basslines", "created_at": "2025-01-27T13:54:55Z", "closed_at": "2025-03-17T21:43:11Z", "commit_id": "897c131fb6419ce101649b5ca3ab22fe541b30a7", "labels": [], "url": "https://github.com/baconpaul/six-sines/issues/173", "body": "i have to play it at G-4 and G-5 to hear what the other basslines sound like at C-4 C-5.\nany chance the tuning could match?\n", "comments_url": "https://api.github.com/repos/baconpaul/six-sines/issues/173/comments", "author": "esaruoho", "comments": [{"user": "baconpaul", "created_at": "2025-01-27T14:18:59Z", "body": "Sure we can take a peek for 1.1 thanks!"}, {"user": "baconpaul", "created_at": "2025-01-27T14:40:04Z", "body": "One fix for this patch (which indeed has its fundemntal as 1.0/6.0) would be to implement #119 and then tune the entire patch up 7 semis in the main. Just linking that though there."}, {"user": "esaruoho", "created_at": "2025-01-27T14:45:29Z", "body": "same with Pads/Daughter of FloatChime\nPads/OST Pad\n..\nsorry, got sidetracked having to pick up my son so didn't finish the post before you replied"}, {"user": "esaruoho", "created_at": "2025-01-27T14:46:01Z", "body": "i can keep going through the presets to find the ones that don't play C at C-note, if it's of use. lmk @baconpaul "}, {"user": "baconpaul", "created_at": "2025-01-27T20:48:28Z", "body": "Yeah! I tagged @kdulcet who wrote a lot of these also to see if there's a reason for it too. (Like is there a modulator which brings them back in tune she used when using them musically or some such).\n\nThanks esa!"}, {"user": "esaruoho", "created_at": "2025-01-27T21:55:04Z", "body": "cool, i'll hit it\n```\nBass/Brand New Bass\nBass/Polite Discourse\nBass/Rehab for Edgelords\nBass/Scream Queen\nBass/Silversmith\nBass/Substrata\nBass/You Got Nothing On This\nKeys/Arpeggiator Food\nKeys/Eat Your Greens\nKeys/Iconoclasm\nKeys/Stack Operator\nLeads/Airlock\nLeads/Asteroid Destroyed\nPads/OST Pad\nPads/Daughter of FloatChime\nPads/OST Pad\n```\nthere are a few more that are kinda \"not sure\" but these stand out a bit to me.\nanyway, would be nice to know\n"}, {"user": "baconpaul", "created_at": "2025-02-07T03:14:04Z", "body": "I've added the coarse tuning shift so it really is now just a matter of me spending 30 minutes and adding a shift 7 to all of these then testing."}, {"user": "esaruoho", "created_at": "2025-02-07T06:55:46Z", "body": "Neat! I trust when this ticket is closed there is no need for me to check the presets but if u mention me here Upon closing i can recheck :)"}, {"user": "baconpaul", "created_at": "2025-02-07T13:57:01Z", "body": "Ahh not all of these are off by 7 semis. OK i'll do this over the weekend. Some are in tune in different parts of the keyboard too.\n"}, {"user": "esaruoho", "created_at": "2025-03-17T21:44:14Z", "body": "thank you!"}, {"user": "baconpaul", "created_at": "2025-03-17T23:27:14Z", "body": "No problem. Some of them were really ambiguous or non-tonal and I left them a lone, but the clearly mistuned ones I fixed up!"}], "satisfaction_conditions": ["Consistent tuning across all basslines and other instruments", "Correction of the specific presets identified as being out of tune", "Verification that the tuning issues have been resolved"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:21"}, "dockerfile": null, "language": "c++"}
{"number": 19, "title": "[BUG] Cava config as template doesn't parse correctly and no errors are shown.", "created_at": "2025-04-10T13:25:24Z", "closed_at": "2025-04-10T15:13:45Z", "commit_id": "44c8c1c4e650ea8b76f4be633b9bfc56e23c80e0", "labels": ["bug", "question"], "url": "https://github.com/danihek/hellwal/issues/19", "body": "The cava config does not parse correctly and results in a incorrect output file.\n\n\nTEMPLATE:\n```\n## Configuration file for CAVA. Default values are commented out. Use either ';' or '#' for commenting.\n\n\n[general]\n\n# Smoothing mode. Can be 'normal', 'scientific' or 'waves'. DEPRECATED as of 0.6.0\n; mode = normal\n\n# Accepts only non-negative values.\nframerate = 60\n\n# 'autosens' will attempt to decrease sensitivity if the bars peak. 1 = on, 0 = off\n# new as of 0.6.0 autosens of low values (dynamic range)\n# 'overshoot' allows bars to overshoot (in % of terminal height) without initiating autosens. DEPRECATED as of 0.6.0\n; autosens = 1\n; overshoot = 20\n\n# Manual sensitivity in %. If autosens is enabled, this will only be the initial value.\n# 200 means double height. Accepts only non-negative values.\n; sensitivity = 100\n\n# The number of bars (0-200). 0 sets it to auto (fill up console).\n# Bars' width and space between bars in number of characters.\n; bars = 0\n; bar_width = 2\n; bar_spacing = 1\n\n# For SDL width and space between bars is in pixels, defaults are:\n; bar_width = 20\n; bar_spacing = 5\n\n\n# Lower and higher cutoff frequencies for lowest and highest bars\n# the bandwidth of the visualizer.\n# Note: there is a minimum total bandwidth of 43Mhz x number of bars.\n# Cava will automatically increase the higher cutoff if a too low band is specified.\n; lower_cutoff_freq = 50\n; higher_cutoff_freq = 10000\n\n\n# Seconds with no input before cava goes to sleep mode. Cava will not perform FFT or drawing and\n# only check for input once per second. Cava will wake up once input is detected. 0 = disable.\n; sleep_timer = 0\n\n\n[input]\n\n# Audio capturing method. Possible methods are: 'pulse', 'alsa', 'fifo', 'sndio' or 'shmem'\n# Defaults to 'pulse', 'alsa' or 'fifo', in that order, dependent on what support cava was built with.\n#\n# All input methods uses the same config variable 'source'\n# to define where it should get the audio.\n#\n# For pulseaudio 'source' will be the source. Default: 'auto', which uses the monitor source of the default sink\n# (all pulseaudio sinks(outputs) have 'monitor' sources(inputs) associated with them).\n#\n# For alsa 'source' will be the capture device.\n# For fifo 'source' will be the path to fifo-file.\n# For shmem 'source' will be /squeezelite-AA:BB:CC:DD:EE:FF where 'AA:BB:CC:DD:EE:FF' will be squeezelite's MAC address\nmethod = pulse\nsource = auto\n\n; method = alsa\n; source = hw:Loopback,1\n\n; method = fifo\n; source = /tmp/mpd.fifo\n; sample_rate = 44100\n; sample_bits = 16\n\n; method = shmem\n; source = /squeezelite-AA:BB:CC:DD:EE:FF\n\n; method = portaudio\n; source = auto\n\n\n[output]\n\n# Output method. Can be 'ncurses', 'noncurses', 'raw' or 'sdl'.\n# 'noncurses' uses a custom framebuffer technique and prints only changes\n# from frame to frame in the terminal. 'ncurses' is default if supported.\n#\n# 'raw' is an 8 or 16 bit (configurable via the 'bit_format' option) data\n# stream of the bar heights that can be used to send to other applications.\n# 'raw' defaults to 200 bars, which can be adjusted in the 'bars' option above.\n#\n# 'sdl' uses the Simple DirectMedia Layer to render in a graphical context.\n; method = ncurses\n\n# Visual channels. Can be 'stereo' or 'mono'.\n# 'stereo' mirrors both channels with low frequencies in center.\n# 'mono' outputs left to right lowest to highest frequencies.\n# 'mono_option' set mono to either take input from 'left', 'right' or 'average'.\n; channels = stereo\n; mono_option = average\n\n# Raw output target. A fifo will be created if target does not exist.\n; raw_target = /dev/stdout\n\n# Raw data format. Can be 'binary' or 'ascii'.\n; data_format = binary\n\n# Binary bit format, can be '8bit' (0-255) or '16bit' (0-65530).\n; bit_format = 16bit\n\n# Ascii max value. In 'ascii' mode range will run from 0 to value specified here\n; ascii_max_range = 1000\n\n# Ascii delimiters. In ascii format each bar and frame is separated by a delimiters.\n# Use decimal value in ascii table (i.e. 59 = ';' and 10 = '\\n' (line feed)).\n; bar_delimiter = 59\n; frame_delimiter = 10\n\n# sdl window size and position. -1,-1 is centered.\n; sdl_width = 1000\n; sdl_height = 500\n; sdl_x = -1\n; sdl_y= -1\n\n[color]\n\n# Colors can be one of seven predefined: black, blue, cyan, green, magenta, red, white, yellow.\n# Or defined by hex code '#xxxxxx' (hex code must be within ''). User defined colors requires\n# ncurses output method and a terminal that can change color definitions such as Gnome-terminal or rxvt.\n# if supported, ncurses mode will be forced on if user defined colors are used.\n# default is to keep current terminal color\n; background = default\n; foreground = default\n\n# SDL only support hex code colors, these are the default:\n; background = '#111111'\n; foreground = '#33cccc'\n\n\n# Gradient mode, only hex defined colors (and thereby ncurses mode) are supported,\n# background must also be defined in hex  or remain commented out. 1 = on, 0 = off.\n# You can define as many as 8 different colors. They range from bottom to top of screen\n# In the [color] section\n\n[color]\n\ngradient = 1\n\ngradient_color_1 = '#%%color8.hex%%'\ngradient_color_2 = '#%%color9.hex%%'\ngradient_color_3 = '#%%color3.hex%%'\ngradient_color_4 = '#%%color4.hex%%'\ngradient_color_5 = '#%%color5.hex%%'\ngradient_color_6 = '#%%color6.hex%%'\ngradient_color_7 = '#%%color13.hex%%'\ngradient_color_8 = '#%%color7.hex%%'\n\n\n\n[smoothing]\n\n# Percentage value for integral smoothing. Takes values from 0 - 100.\n# Higher values means smoother, but less precise. 0 to disable.\n; integral = 77\n\n# Disables or enables the so-called \"Monstercat smoothing\" with or without \"waves\". Set to 0 to disable.\n; monstercat = 0\n; waves = 0\n\n# Set gravity percentage for \"drop off\". Higher values means bars will drop faster.\n# Accepts only non-negative values. 50 means half gravity, 200 means double. Set to 0 to disable \"drop off\".\n; gravity = 100\n\n\n# In bar height, bars that would have been lower that this will not be drawn.\n; ignore = 0\n\n\n[eq]\n\n# This one is tricky. You can have as much keys as you want.\n# Remember to uncomment more then one key! More keys = more precision.\n# Look at readme.md on github for further explanations and examples.\n; 1 = 1 # bass\n; 2 = 1\n; 3 = 1 # midtone\n; 4 = 1\n; 5 = 1 # treble\n```\n\n\nResult:\n```\n## Configuration file for CAVA. Default values are commented out. Use either ';' or '#' for commenting.\n\n\n[general]\n\n# Smoothing mode. Can be 'normal', 'scientific' or 'waves'. DEPRECATED as of 0.6.0\n; mode = normal\n\n# Accepts only non-negative values.\nframerate = 60\n\n# 'autosens' will attempt to decrease sensitivity if the bars peak. 1 = on, 0 = off\n# new as of 0.6.0 autosens of low values (dynamic range)\n# 'overshoot' allows bars to overshoot (in 1c1c53'\ngradient_color_2 = '#242461'\ngradient_color_3 = '#52495c'\ngradient_color_4 = '#4c4970'\ngradient_color_5 = '#9e7aa2'\ngradient_color_6 = '#a980a2'\ngradient_color_7 = '#c598ca'\ngradient_color_8 = '#d3c2e8'\n\n\n\n[smoothing]\n\n# Percentage value for integral smoothing. Takes values from 0 - 100.\n# Higher values means smoother, but less precise. 0 to disable.\n; integral = 77\n\n# Disables or enables the so-called \"Monstercat smoothing\" with or without \"waves\". Set to 0 to disable.\n; monstercat = 0\n; waves = 0\n\n# Set gravity percentage for \"drop off\". Higher values means bars will drop faster.\n# Accepts only non-negative values. 50 means half gravity, 200 means double. Set to 0 to disable \"drop off\".\n; gravity = 100\n\n\n# In bar height, bars that would have been lower that this will not be drawn.\n; ignore = 0\n\n\n[eq]\n\n# This one is tricky. You can have as much keys as you want.\n# Remember to uncomment more then one key! More keys = more precision.\n# Look at readme.md on github for further explanations and examples.\n; 1 = 1 # bass\n; 2 = 1\n; 3 = 1 # midtone\n; 4 = 1\n; 5 = 1 # treble\n```", "comments_url": "https://api.github.com/repos/danihek/hellwal/issues/19/comments", "author": "SherLock707", "comments": [{"user": "danihek", "created_at": "2025-04-10T13:59:31Z", "body": "okay there is a bug for some reason - it dont like % sign commented in line 14 and 18, if you remove that it should work perfectly, it worked for me.\n\nanother bug to TODO I guess - thanks for reporting that"}, {"user": "SherLock707", "created_at": "2025-04-10T14:16:21Z", "body": "thanks! your fix worked."}], "satisfaction_conditions": ["A solution that fixes the template parsing issue with the cava configuration file", "An explanation of what causes the parsing error in the cava config template", "A practical workaround that allows the configuration to parse correctly"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:01:34"}, "dockerfile": "FROM ubuntu:20.04\n\n# Set non-interactive mode for apt\nENV DEBIAN_FRONTEND=noninteractive\n\n# Add metadata\nLABEL maintainer=\"Docker Builder\"\nLABEL description=\"Environment for validating the cava config template parsing issue in hellwal\"\nLABEL version=\"1.0\"\n\n# Update system and install dependencies\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    git \\\n    make \\\n    gcc \\\n    libc6-dev \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/danihek/hellwal.git . \\\n    && git checkout 44c8c1c4e650ea8b76f4be633b9bfc56e23c80e0\n\n# Build the project\nRUN make\n\n# Create a directory for user to mount their images\nRUN mkdir -p /images\n\n# Create a volume for persistent cache\nVOLUME /root/.cache/hellwal\n\n# Set the default command to show help\nCMD [\"./hellwal\", \"--help\"]", "language": "c++"}
{"number": 9, "title": "wrong template foot-color.ini at foot version 1.20.1", "created_at": "2025-01-19T16:37:22Z", "closed_at": "2025-01-27T18:05:41Z", "commit_id": "e71cbb9ebb2a475c5e5a453432a12539a2282823", "labels": [], "url": "https://github.com/danihek/hellwal/issues/9", "body": "work line is:\n\n[colors]\nbackground=%%background%%\nforeground=%%foreground%%\n...\n[cursor]\ncolor=%%cursor%% %%border%%\n\ninstead:\n\n[colors]\nbackground=%%background%%\nforeground=%%foreground%%\ncursor=%%cursor%%\nborder=%%border%%\n", "comments_url": "https://api.github.com/repos/danihek/hellwal/issues/9/comments", "author": "AGG29", "comments": [{"user": "danihek", "created_at": "2025-01-27T18:05:41Z", "body": "Oh, I didn't noticed that, thank you! Everything works now - I appreciate it."}], "satisfaction_conditions": ["Correct format for the colors section in the foot-color.ini template", "Clear identification of the configuration error in the template file"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:40"}, "dockerfile": null, "language": "c++"}
{"number": 7, "title": "Compilation Error in Debian Sid", "created_at": "2025-01-01T13:38:24Z", "closed_at": "2025-01-02T16:25:38Z", "commit_id": "b75e671581aaf004650716b3f2666463e635bed6", "labels": [], "url": "https://github.com/danihek/hellwal/issues/7", "body": "When compiling I got this error\r\nDistro: Debian Sid\r\nGCC: gcc (Debian 14.2.0-12) 14.2.0\r\nMake: GNU Make 4.4.1\r\n\r\nHere is the error:\r\n```\r\n[holland@debian hellwal]$ make\r\ncc -Wall -Wextra -O3 -lm hellwal.c -o hellwal\r\nIn function ‘process_theme’,\r\n    inlined from ‘process_themeing’ at hellwal.c:2122:14:\r\nhellwal.c:2110:9: warning: ‘free’ called on unallocated object ‘pal’ [-Wfree-nonheap-object]\r\n 2110 |         free(pal);\r\n      |         ^~~~~~~~~\r\nhellwal.c: In function ‘process_themeing’:\r\nhellwal.c:2118:13: note: declared here\r\n 2118 |     PALETTE pal;\r\n      |             ^~~\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `stbi__load_main':\r\nhellwal.c:(.text+0x19807): undefined reference to `pow'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `stbi__loadf_main':\r\nhellwal.c:(.text+0x1c6b5): undefined reference to `pow'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `saturate_color':\r\nhellwal.c:(.text+0x207ee): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x20801): undefined reference to `fmaxf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `rgb_to_hsl':\r\nhellwal.c:(.text+0x2093e): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x20948): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x20971): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x2097b): undefined reference to `fminf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `adjust_luminance':\r\nhellwal.c:(.text+0x21168): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x21188): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x211a6): undefined reference to `fminf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o:hellwal.c:(.text+0x21221): more undefined references to `fminf' follow\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `apply_grayscale':\r\nhellwal.c:(.text+0x213c8): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x213db): undefined reference to `fmaxf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `apply_offsets':\r\nhellwal.c:(.text+0x214b5): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x214d5): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x214f4): undefined reference to `fminf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `median_cut':\r\nhellwal.c:(.text+0x21c30): undefined reference to `fmax'\r\n/usr/bin/ld: hellwal.c:(.text+0x21c3a): undefined reference to `fmax'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `palette_handle_color_mode':\r\nhellwal.c:(.text+0x2228d): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x222b4): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x222db): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22302): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22329): undefined reference to `fminf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o:hellwal.c:(.text+0x22350): more undefined references to `fminf' follow\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `palette_handle_light_mode':\r\nhellwal.c:(.text+0x225c7): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x225da): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x226a6): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x226ca): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x226ee): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22715): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x2273c): undefined reference to `fminf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o:hellwal.c:(.text+0x22763): more undefined references to `fminf' follow\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `apply_addtional_arguments':\r\nhellwal.c:(.text+0x22cde): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22cf8): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22e31): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22e52): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22e73): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22edf): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22ef3): undefined reference to `fmaxf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22faf): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22fd0): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x22ff1): undefined reference to `fminf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o: in function `gen_palette':\r\nhellwal.c:(.text+0x23e06): undefined reference to `fminf'\r\n/usr/bin/ld: hellwal.c:(.text+0x23e2f): undefined reference to `fminf'\r\n/usr/bin/ld: /tmp/ccU0KXxd.o:hellwal.c:(.text+0x23e5a): more undefined references to `fminf' follow\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [Makefile:9: hellwal] Error 1\r\n[holland@debian hellwal]$\r\n```\r\n\r\n**_Proposed Solution_**\r\nI will submit a pull request to address these issues. The main changes will likely include:\r\n\r\n    Correcting the use of free(pal) in the process_theme function\r\n    Ensuring proper linkage of the math library, possibly by modifying the compilation command or Makefile\r\n\r\nPlease let me know if this approach is acceptable, and I'll proceed with creating the pull request.", "comments_url": "https://api.github.com/repos/danihek/hellwal/issues/7/comments", "author": "MalcolmReed-ent", "comments": [{"user": "MalcolmReed-ent", "created_at": "2025-01-01T14:06:56Z", "body": "I've addressed the issue through a pull request, improving both hellwal.c and the Makefile for smoother compilation. Feel free to close this at your convenience. \r\n\r\nHowever, I have a few questions about the project:\r\n1. Is it expected behavior for only one terminal to change color when running hellwal? I've noticed that in ST, only the terminal where I execute the command changes, while other open terminals and newly launched ones don't adopt the new colorscheme.\r\n2. Should I add a hellwal command to my .xinitrc file to ensure it runs on system startup?\r\n3. What other applications can benefit from hellwal? Would DWM or various X11 programs be compatible with it?"}, {"user": "danihek", "created_at": "2025-01-01T23:34:29Z", "body": "Hi, thank you for creating an issue and PR!\r\n\r\n1. Hellwal should color all **currently** open terminals. If you want to have colored every new opened terminal, you have to create template file with colors and source it in ``.bashrc``. I've done it like this:\r\n\r\n(these two hellwal templates are in repo in ``./templates`` folder)\r\n```sh\r\ndh » pwd\r\n/home/dh/.config/hellwal\r\ndh » tree\r\n.\r\n├── templates\r\n│   ├── terminal.sh\r\n│   └── variables.sh\r\n```\r\n\r\nAnd in the end of your bashrc put this:\r\n\r\n```sh\r\nsource ~/.cache/hellwal/variables.sh\r\nsh ~/.cache/hellwal/terminal.sh\r\n``` \r\n\r\n2. You don't have to, after running hellwal it's saved to ``~/.cache``, you just have to somehow source or load generated files.\r\n3. A lot, probably anything that can be set through some config file. I use hellwal for Firefox, Terminals, Discord, Duckduckgo, Hyprland, HellWM, Waybar.... There is a lot.\r\n\r\nPS: I already merged **PR**, thank you :) \r\n\r\nAnything else would you like to know about Hellwal or how to use it?"}, {"user": "MalcolmReed-ent", "created_at": "2025-01-02T16:25:20Z", "body": "I get it now, it will take you as far as your scripting will, if i make a script specifying the colors for dwm or st or dmenu it will change according, i get it now, i just thought it was somewhat folder rich of scripts like pywal."}, {"user": "MalcolmReed-ent", "created_at": "2025-01-02T17:58:28Z", "body": "also, if it wouldnt be a physical undertaking could you possibly make a dwm and dmenu script so it can change the colorschemes also since ST already work."}], "satisfaction_conditions": ["A solution that fixes the compilation errors related to math functions", "Guidance on how to apply color schemes to multiple terminals", "Information about how to make hellwal run on system startup", "Clarification on which applications can be integrated with hellwal", "Understanding of hellwal's template-based approach to theming", "Request for additional template scripts for specific applications"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:47"}, "dockerfile": null, "language": "c++"}
{"number": 3, "title": "Typo in repo folder structure", "created_at": "2024-12-08T08:36:42Z", "closed_at": "2024-12-08T08:56:43Z", "commit_id": "9e64ed728cd9fe2afeb47afcf06928e56b43f5cb", "labels": ["documentation"], "url": "https://github.com/danihek/hellwal/issues/3", "body": "Hey, I noticed that the assets folder is called assests. Not a big deal but might be worth fixing :)", "comments_url": "https://api.github.com/repos/danihek/hellwal/issues/3/comments", "author": "ficcdaf", "comments": [{"user": "danihek", "created_at": "2024-12-08T08:56:43Z", "body": "Yeah I haven't noticed that, it's fixed now thank you :D\r\n I'm closing the issue."}], "satisfaction_conditions": ["Acknowledgment of the typo in the folder name", "Correction of the misspelled folder name", "Confirmation that the issue has been addressed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:52"}, "dockerfile": null, "language": "c++"}
{"number": 79, "title": "hw/rdc_setup.sh: fix typos, add missing files", "created_at": "2025-02-21T11:02:00Z", "closed_at": "2025-02-25T08:05:52Z", "commit_id": "e01a18770520220ae0e482fc50d6019924c23ad6", "labels": [], "url": "https://github.com/p4lang/open-p4studio/pull/79", "body": "This PR contains the changes that I found necessary for building the project for the Tofino reference platforms.\r\n\r\nApart from some obvious typos and missing directories for source and header files, it appears to me that the `libavago` pre-built binaries need to be copied as well.\r\n", "comments_url": "https://api.github.com/repos/p4lang/open-p4studio/issues/79/comments", "author": "alexandergall", "comments": [{"user": "jafingerhut", "created_at": "2025-02-21T16:51:02Z", "body": "@vgurevich @pkotikal Would one of you be able to try this out?  I do not think Fabian has access to the proprietary Intel software to try this out, and I do not."}, {"user": "vgurevich", "created_at": "2025-02-21T16:59:51Z", "body": "@jafingerhut -- I do not have access to Tofino HW at the moment. But I'd trust @alexandergall :) I was also surprised seeing the `avago` directory missing in the original code, so this makes sense to me for sure."}, {"user": "jafingerhut", "created_at": "2025-02-21T17:03:32Z", "body": "> @jafingerhut -- I do not have access to Tofino HW at the moment. But I'd trust @alexandergall :) I was also surprised seeing the `avago` directory missing in the original code, so this makes sense to me for sure.\r\n\r\nI was mainly thinking of trying out the script with the mix of open-p4studio and proprietary Intel software to see if the script runs without error.  Testing it on hardware afterwards is of course ideal, but verifying that the script actually runs for one more person other than the one who wrote it is a far better check than only the one person who wrote it.\r\n\r\nIf that isn't possible, no worries.  Just hoping for at least a _little bit_ of testing or review on this."}, {"user": "pkotikal", "created_at": "2025-02-21T17:05:02Z", "body": "@ansamalintel, can you please look into this? "}, {"user": "vgurevich", "created_at": "2025-02-21T18:18:42Z", "body": "@jafingerhut -- I decided to give it a try, but one thing I can see right away is that the instructions in the `README.md` are kinda incomplete and the script will require further enhancements for usability. Let me try to build the way I think it **should** work and we'll see what will happen. "}, {"user": "vgurevich", "created_at": "2025-02-21T18:53:48Z", "body": "I followed the instructions the best I could, specifically:\r\n\r\n1. Extracted the contents of the SDE package `bf-drivers-9.13.3` into `/tmp` (that's the step that seems to be missing in the` README.md`)\r\n\r\n```bash\r\ncd /tmp\r\ntar xzvf ~/bf-sde-9.13.3/packages/bf-drivers-9.13.3.tgz\r\n```\r\n\r\n2. Edited the file `~/op4-rdc/open-p4studio/hw/rdc.setup.sh` as described, specifically:\r\n\r\n```bash\r\nRDC_BFD=\"/tmp/bf-drivers-9.13.3\"\r\nOS_BFD=\"/home/ubuntu/op4-rdc/open-p4studio/pkgsrc/bf-drivers\"\r\n```\r\n\r\n3. Copied the files using the procedure, described in `README.md`, specifically:\r\n\r\n```bash\r\nsource rdc_setup.sh\r\nrdc_setup\r\n```\r\n\r\nThe copying went well, without any error messages:\r\n\r\n```bash\r\nubuntu@ip-172-31-32-12:~/op4-rdc/open-p4studio/hw$ source rdc_setup.sh \r\nubuntu@ip-172-31-32-12:~/op4-rdc/open-p4studio/hw$ rdc_setup \r\nCopying src/alphawave\r\nCopying src/credo\r\nCopying src/avago\r\nCopying src/microp\r\nCopying include/avago/aapl.h\r\nCopying include/avago/avago_aapl.h\r\nCopying include/avago/avago_dox.h\r\nCopying src/port_mgr/csr\r\nCopying src/port_mgr/crdo\r\nCopying src/port_mgr/aw-gen\r\nCopying src/port_mgr/t3-csr\r\nCopying src/port_mgr/CMakeLists.txt\r\nCopying src/port_mgr/bf_ll_umac3_if.c\r\nCopying src/port_mgr/bf_ll_umac4_if.c\r\nCopying src/port_mgr/port_mgr_dev.c\r\nCopying src/port_mgr/port_mgr_physical_dev.h\r\nCopying src/port_mgr/port_mgr_umac_access.c\r\nCopying src/port_mgr/port_mgr_tof1/bf_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_access_autogen.c\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_access_autogen.h\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_def_autogen.h\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_strs.h\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_av_sd.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_av_sd.h\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_av_sd_an.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_av_sd_an.h\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_mac.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_port_diag.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_serdes.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_serdes_diag.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_serdes_sbus_map.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_ucli.c\r\nCopying src/port_mgr/port_mgr_tof1/bf_fsm_hdlrs.c\r\nCopying src/port_mgr/port_mgr_tof2/autogen-required-headers.h\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_eth100g_reg_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_eth400g_mac_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_eth400g_pcs_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_tof2_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof2/credo_sd_access.c\r\nCopying src/port_mgr/port_mgr_tof2/credo_sd_access.h\r\nCopying src/port_mgr/port_mgr_tof2/eth100g_reg_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof2/eth400g_mac_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof2/eth400g_pcs_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_bandgap.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_gpio.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_microp.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_serdes.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_umac.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_umac3.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_umac4.c\r\nCopying src/port_mgr/port_mgr_tof2/umac3c4_access.c\r\nCopying src/port_mgr/port_mgr_tof2/umac3c4_fld_access.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4_ctrs.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4_ctrs_str.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4c8_access.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4c8_fld_access.c\r\nCopying src/port_mgr/port_mgr_tof3/aw-reg-gen\r\nCopying src/port_mgr/port_mgr_tof3/aw_16ln\r\nCopying src/port_mgr/port_mgr_tof3/aw_4ln\r\nCopying src/port_mgr/port_mgr_tof3/aw_driver_sim.c\r\nCopying src/port_mgr/port_mgr_tof3/aw_driver_sim.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_if.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_io.c\r\nCopying src/port_mgr/port_mgr_tof3/aw_io.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_mss.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_reg_dbg.c\r\nCopying src/port_mgr/port_mgr_tof3/aw_reg_dbg.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_types.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_vector_types.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_aw_pmd.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_aw_vfld_pmd.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_app_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_app_rspec_if.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_mac_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_mac_rspec_if.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_sys_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_sys_rspec_if.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_tof3_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_tof3_serdes_utils.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_tof3_serdes_utils.h\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_dev.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_map.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_microp.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_port.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_serdes.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_serdes_map.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_tmac.c\r\nCopying src/port_mgr/port_mgr_tof3/svdpi.c\r\nCopying src/port_mgr/port_mgr_tof3/svdpi.h\r\nCopying src/port_mgr/port_mgr_tof3/tmac_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tmac_access.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3-autogen-required-headers.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_app_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_app_rspec_access.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_mac_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_mac_rspec_access.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_sys_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_sys_rspec_access.h\r\nCopying src/port_mgr/port_mgr_tof3/vfld_vec_name.h\r\nCopying src/port_mgr/port_mgr_tof3/vfld_vec_type.h\r\n```\r\n\r\n4. After that I tried to use `p4studio interactive` as is standard when configuring the SDE for a HW platform. Unfortunately, I got an exception:\r\n\r\n```\r\nubuntu@ip-172-31-32-12:~/op4-rdc/open-p4studio/p4studio$ ./p4studio interactive\r\nChecking system capabilities to build and install SDE:\r\n ?? Free space >= 20GB: 20.60GB\r\n ?? Free space in /tmp >= 2.5GB: 20.60GB\r\n ?? OS is supported: Ubuntu 20.04\r\n ?? Basic tools are installed: sudo ??\r\n\r\nDefault settings allow to run P4-16 examples for all tofino chip types on ASIC model.\r\nDo you want to install SDE using default settings (suitable for beginners)?: No\r\n   Yes\r\n> No\r\n\r\nDo you want to install missing third-party dependencies?: Yes\r\n> Yes\r\n   No\r\n\r\nPlease select deployment target: Hardware\r\n> Hardware\r\n   ASIC Model\r\n\r\nPlease select platform: montara/mavericks (tofino)\r\n> montara/mavericks (tofino)\r\n   newport (tofino2)\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/__main__.py\", line 26, in <module>\r\n    p4studio_main()\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/main.py\", line 85, in p4studio_main\r\n    p4studio_cli.main(\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/third_party/click/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/third_party/click/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/third_party/click/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/third_party/click/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/third_party/click/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/interactive/interactive_command.py\", line 86, in interactive_command\r\n    default=workspace.bsp_path(),\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/workspace/sde_workspace.py\", line 67, in bsp_path\r\n    version = self._sde_version()\r\n  File \"/home/ubuntu/op4-rdc/open-p4studio/p4studio/workspace/sde_workspace.py\", line 76, in _sde_version\r\n    manifest_filename = Path(manifest_possible_file_paths[0]).name\r\nIndexError: list index out of range\r\n```\r\n\r\nI also tried with SDE-9.13.4, with the same result. :(\r\n"}, {"user": "vgurevich", "created_at": "2025-02-21T18:56:45Z", "body": "The easy workaround is to create the manifest file, e.g.\r\n\r\n```bash\r\ntouch ~/open-p4studio/bf-sde-open-p4studio.manifest\r\n```\r\n\r\nIt might be useful to add it for better tool compatibility anyway."}, {"user": "vgurevich", "created_at": "2025-02-21T21:30:24Z", "body": "Even after the workaround the build ended up in this failure:\r\n\r\n```\r\n2025-02-21 21:41:55,165: Begin bf-platforms setup\r\n2025-02-21 21:41:55,166: CMake Error at CMakeLists.txt:256 (add_subdirectory):\r\n2025-02-21 21:41:55,166:   add_subdirectory given source \"pkgsrc/bf-platforms\" which is not an\r\n2025-02-21 21:41:55,166:   existing directory.\r\n```\r\n\r\n@alexandergall -- did you use the `p4studio` tool or your own infra?\r\n@jafingerhut, @pkotikal  -- I tried on a regular `open-p4studio` (that did not have @alexandergall 's changes), but the result was worse -- the patching didn't go through:\r\n\r\n```\r\nubuntu@ip-172-31-32-12:~/open-p4studio/hw$ rdc_setup \r\nCopying src/alphawave\r\nCopying src/credo\r\nCopying src/firmware\r\ncp: cannot stat '/tmp/bf-drivers-9.13.3/src/firmware': No such file or directory\r\nCopying src/microp\r\nCopying src/port_mgr/csr\r\nCopying src/port_mgr/crdo\r\nCopying src/port_mgr/aw-gen\r\nCopying src/port_mgr/CMakeLists.txt\r\nCopying src/port_mgr/bf_ll_umac3_if.c\r\nCopying src/port_mgr/bf_ll_umac_4_if.c\r\ncp: cannot stat '/tmp/bf-drivers-9.13.3/src/port_mgr/bf_ll_umac_4_if.c': No such file or directory\r\nCopying src/port_mgr/port_mgr_dev.c\r\nCopying src/port_mgr/post_mgr_physical_dev.c\r\ncp: cannot stat '/tmp/bf-drivers-9.13.3/src/port_mgr/post_mgr_physical_dev.c': No such file or directory\r\nCopying src/port_mgr/port_mgr_umac_access.c\r\nCopying src/port_mgr/t3-csr/tf3-csr-gen.py\r\ncp: cannot create regular file '/home/ubuntu/open-p4studio/pkgsrc/bf-drivers/src/port_mgr/t3-csr/tf3-csr-gen.py': No such file or directory\r\nCopying src/port_mgr/port_mgr_tof1/bf_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_access_autogen.c\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_access_autogen.h\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_def_autogen.h\r\nCopying src/port_mgr/port_mgr_tof1/comira_reg_strs.h\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_av_sd.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_av_sd_an.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_mac.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_port_diag.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_serdes.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_serdes_diag.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_serdes_sbus_map.c\r\nCopying src/port_mgr/port_mgr_tof1/port_mgr_ucli.c\r\nCopying src/port_mgr/port_mgr_tof2/autogen-required-headers.h\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_eth100g_reg_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_eth400g_mac_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_eth400g_pcs_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_ll_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof2/bf_tof2_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof2/credo_sd_access.c\r\nCopying src/port_mgr/port_mgr_tof2/credo_sd_access.h\r\nCopying src/port_mgr/port_mgr_tof2/eth100g_reg_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof2/eth400g_mac_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof2/eth400g_pcs_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_bandgap.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_gpio.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_microp.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_serdes.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_umac.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_umac3.c\r\nCopying src/port_mgr/port_mgr_tof2/port_mgr_tof2_umac4.c\r\nCopying src/port_mgr/port_mgr_tof2/umac3c4_access.c\r\nCopying src/port_mgr/port_mgr_tof2/umac3c4_fld_access.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4_ctrs.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4_ctrs_str.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4c8_access.c\r\nCopying src/port_mgr/port_mgr_tof2/umac4c8_fld_access.c\r\nCopying src/port_mgr/port_mgr_tof3/aw-reg-gen\r\nCopying src/port_mgr/port_mgr_tof3/aw_16ln\r\nCopying src/port_mgr/port_mgr_tof3/aw_driver_sim.c\r\nCopying src/port_mgr/port_mgr_tof3/aw_driver_sim.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_if.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_io.c\r\nCopying src/port_mgr/port_mgr_tof3/aw_io.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_mss.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_reg_dbg.c\r\nCopying src/port_mgr/port_mgr_tof3/aw_reg_dbg.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_types.h\r\nCopying src/port_mgr/port_mgr_tof3/aw_vector_types.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_aw_pmd.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_aw_vfld_pmd.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_app_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_app_rspec_if.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_mac_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_mac_rspec_if.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_sys_rspec_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_ll_tof3_eth400g_sys_rspec_if.h\r\nCopying src/port_mgr/port_mgr_tof3/bf_tof3_serdes_if.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_tof3_serdes_utils.c\r\nCopying src/port_mgr/port_mgr_tof3/bf_tof3_serdes_utils.h\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_dev.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_map.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_microp.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_port.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_serdes.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_serdes_map.c\r\nCopying src/port_mgr/port_mgr_tof3/port_mgr_tof3_tmac.c\r\nCopying src/port_mgr/port_mgr_tof3/svdpi.c\r\nCopying src/port_mgr/port_mgr_tof3/svdpi.h\r\nCopying src/port_mgr/port_mgr_tof3/tmac_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tmac_access.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3-autogen-required-headers.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_app_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_app_rspec_access.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_mac_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_mac_rspec_access.h\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_sys_rspec_access.c\r\nCopying src/port_mgr/port_mgr_tof3/tof3_eth400g_sys_rspec_access.h\r\nCopying src/port_mgr/port_mgr_tof3/vfld_vec_name.h\r\nCopying src/port_mgr/port_mgr_tof3/vfld_vec_type.h\r\n```"}, {"user": "jafingerhut", "created_at": "2025-02-21T22:22:28Z", "body": "Many thanks for trying it out, Vlad, and the detailed feedback.  Hopefully Alexander can determine what changes to the PR might help.\r\n\r\n@alexdandergall I am not sure if you signed your git commit, e.g. using `git commit -s <additional options>`, but something like that is required in this and other p4lang repositories in order to pass the DCO check.  I see the DCO check \"pending\", not \"failed\", which I have not seen before, so if you already did that, hopefully the next commit the check will pass smoothly."}, {"user": "vgurevich", "created_at": "2025-02-22T12:13:08Z", "body": "Just to summarize my review:\r\n\r\n1. The proposed changes are correct and necessary to get the patching going. The current `hw/rdc_setup.sh` script does not seem to be correct in the first place. Since this is the title of the pull request, I think it can be approved, since that's exactly what it does. \r\n2. Despite those changes, there are at least two more issues with the `p4studio` tool that prevent the actual build for the HW from happening. I am fairly certain that they are not related to the changes in this PR and have been there from the very beginning (@pkotikal -- was that tested in the first place)? W can file them separately and work on them in the due course.\r\n3. We can also file an enhancement request for the `hw/rdc_setup.sh` script, where instead of requiring the user to manually untar the official (RDC) SDE release and then untarring the package and then necessitating editing the paths it will do all that automatically. "}, {"user": "alexandergall", "created_at": "2025-02-23T10:18:42Z", "body": "I should have mentioned that I used my own build system to test this patch. That's why I didn't notice  the issue with the `p4studio` tool. If we agree that the patch as such is correct I would be in favor of tracking the problem with the setup tool separately. "}, {"user": "ansamalintel", "created_at": "2025-02-24T01:26:52Z", "body": "I have approved the change. In the mean time, I will review the script after comparing the open source repo with internal repo. "}, {"user": "jafingerhut", "created_at": "2025-02-24T17:44:00Z", "body": "> Just to summarize my review:\r\n> \r\n> 1. The proposed changes are correct and necessary to get the patching going. The current `hw/rdc_setup.sh` script does not seem to be correct in the first place. Since this is the title of the pull request, I think it can be approved, since that's exactly what it does.\r\n> 2. Despite those changes, there are at least two more issues with the `p4studio` tool that prevent the actual build for the HW from happening. I am fairly certain that they are not related to the changes in this PR and have been there from the very beginning (@pkotikal -- was that tested in the first place)? W can file them separately and work on them in the due course.\r\n> 3. We can also file an enhancement request for the `hw/rdc_setup.sh` script, where instead of requiring the user to manually untar the official (RDC) SDE release and then untarring the package and then necessitating editing the paths it will do all that automatically.\r\n\r\nThis PR has been approved, and I do not personally know of a reason to delay merging it.\r\n\r\nIf it is merged in its current form, it would be great if we could track your items 2 and 3 above as separate issues."}], "satisfaction_conditions": ["Correct file paths for copying proprietary files from Intel's SDE package", "Inclusion of the libavago pre-built binaries", "A script that runs without file copy errors when executed", "Fixes to typos in the original script", "Compatibility with the Tofino reference platform build process"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:59"}, "dockerfile": null, "language": "c++"}
{"number": 42, "title": "Failed to resolve app state", "created_at": "2025-03-13T12:34:16Z", "closed_at": "2025-03-13T14:10:16Z", "commit_id": "d0c1c5dfe9a14e1412dc55e955dae0e76e45fdbf", "labels": [], "url": "https://github.com/duckdb/duckdb-ui/issues/42", "body": "Hi there, \n\nThanks for creating this UI, I am excited to try it out!\n\nI was able to get it working earlier, but I am now hit with an error whenever I run `duckdb -ui`: \n```\nFailed to resolve app state with user - Error: Binder Error: Catalog \"_duckdb_ui\" does not exist!\nUser ID: unknown\nUser E-mail: unknown\n```\n\nIs this a known thing? \n\nKind regards, \nDaniel\n\n", "comments_url": "https://api.github.com/repos/duckdb/duckdb-ui/issues/42/comments", "author": "Dtenwolde", "comments": [{"user": "Y--", "created_at": "2025-03-13T13:37:28Z", "body": "Hi @Dtenwolde this is possibly because you have an outdated extension. \nCan you try to run `UPDATE EXTENSIONS` or `FORCE INSTALL ui` and be sure you have the latest version, `963e0e4`?\n\nLet me know if it helps."}, {"user": "Dtenwolde", "created_at": "2025-03-13T14:10:16Z", "body": "That worked, thank you! "}], "satisfaction_conditions": ["A solution that resolves the 'Failed to resolve app state' error", "Instructions for updating or reinstalling the UI extension", "A quick, executable fix that doesn't require complex troubleshooting"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:16"}, "dockerfile": null, "language": "c++"}
{"number": 11, "title": "docs(README.md): fix spelling", "created_at": "2025-03-29T18:24:35Z", "closed_at": "2025-03-29T18:28:51Z", "commit_id": "cba07f2ef9603d2e27386df4e339c685f5767136", "labels": [], "url": "https://github.com/EmberEmu/Hexi/pull/11", "body": "- accomodate => accommodate\r\n- determing => determining\r\n- read => reads", "comments_url": "https://api.github.com/repos/EmberEmu/Hexi/issues/11/comments", "author": "vladdoster", "comments": [{"user": "vladdoster", "created_at": "2025-03-29T18:25:39Z", "body": "Congratulations on reaching the front-page of HN!"}, {"user": "Chaosvex", "created_at": "2025-03-29T18:26:31Z", "body": "Well spotted, thanks! :)"}], "satisfaction_conditions": ["Acknowledgment of the spelling corrections identified in the README.md file", "Recognition of the contribution, however small"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:53"}, "dockerfile": null, "language": "c++"}
{"number": 1, "title": "Rechtschreibfehler in DynamischesArray/README.md korrigiert", "created_at": "2025-03-31T20:40:15Z", "closed_at": "2025-04-01T11:49:47Z", "commit_id": "b6850349e7034906263881818f37d327b23a917f", "labels": [], "url": "https://github.com/AnonymeMasse/BetterEP2/pull/1", "body": "btw gibt es einen Grund warum du Umlaute mit ae usw schreibst?", "comments_url": "https://api.github.com/repos/AnonymeMasse/BetterEP2/issues/1/comments", "author": "JulianBnder", "comments": [{"user": "bendermeister", "created_at": "2025-04-01T11:01:40Z", "body": "Ich verwende eine qwerty tastatur und hab mir keine Keybinds gesetzt, weil ich eigentlich immer auf english schreibe"}, {"user": "bendermeister", "created_at": "2025-04-01T11:05:52Z", "body": "moechtest du auch einfach ein contributor status haben? Dann kannst du selbst mergen"}, {"user": "bendermeister", "created_at": "2025-04-01T11:49:44Z", "body": "Jo ich habe jetzt deinen Typo fix selbst reingepackt weil sich die Ordner struktur geaendert hat und ich keine Ahnung habe wie ich git sage dass er das trotzdem mergen soll.\r\n\r\nDanke fuer den Typo fix!"}], "satisfaction_conditions": ["An explanation for why umlauts are written as 'ae', 'oe', etc. instead of 'ä', 'ö', etc.", "Acknowledgment of the user's contribution in fixing typos", "Information about how the contribution was handled"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:29"}, "dockerfile": null, "language": "java"}
{"number": 465, "title": "Deep copy a VectorSchemaRoot?", "created_at": "2024-12-24T12:29:02Z", "closed_at": "2024-12-26T05:46:13Z", "commit_id": "3ef5450919dd3ebc0b566d1556c33c1207a10514", "labels": ["Type: enhancement"], "url": "https://github.com/apache/arrow-java/issues/465", "body": "### Describe the enhancement requested\r\n\r\nI'm writing a convertor method to convert a base64 encoded byte array into Arrow batches and returns it to the user.\r\n\r\n```java\r\npublic List<VectorSchemaRoot> readArrowBatches(String rows, BufferAllocator allocator) {\r\n    final List<VectorSchemaRoot> batches = new ArrayList<>();\r\n    final byte[] data = Base64.getDecoder().decode(rows);\r\n    final ByteArrayInputStream stream = new ByteArrayInputStream(data);\r\n    try (final ArrowStreamReader reader = new ArrowStreamReader(stream, allocator)) {\r\n        while (reader.loadNextBatch()) {\r\n            batches.add(new Table(reader.getVectorSchemaRoot()).toVectorSchemaRoot());\r\n        }\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    }\r\n    return batches;\r\n}\r\n```\r\n\r\nSince `ArrowStreamReader` replace the batch referred by `getVectorSchemaRoot` in each iteration, I have to do a deepcopy of VectorSchemaRoot every time.\r\n\r\nCurrently, I use Table's method as a workaround, but wonder if `VectorSchemaRoot` deserves a `copy` method, or I implement such a typically use case in a wrong way.", "comments_url": "https://api.github.com/repos/apache/arrow-java/issues/465/comments", "author": "tisonkun", "comments": [{"user": "lidavidm", "created_at": "2024-12-26T03:59:54Z", "body": "You should use VectorLoader/VectorUnloader to \"move\" the contents of the reader's root into your own"}, {"user": "tisonkun", "created_at": "2024-12-26T05:38:54Z", "body": "That seems exactly what the inner of `Table` does. Do we have some util or a `copy` method for that. Or I just wrap by myself .. It seems quite a common usage and I don't want to hook outside of arrow-java.\r\n\r\n```java\r\nwhile (reader.loadNextBatch()) {\r\n    final VectorSchemaRoot source = reader.getVectorSchemaRoot();\r\n    final VectorUnloader unloader = new VectorUnloader(source);\r\n    final VectorSchemaRoot copy = VectorSchemaRoot.create(source.getSchema(), allocator);\r\n    final VectorLoader loader = new VectorLoader(copy);\r\n    loader.load(unloader.getRecordBatch());\r\n    batches.add(copy);\r\n}\r\n```"}, {"user": "lidavidm", "created_at": "2024-12-26T05:43:07Z", "body": "That is the intended usage. What is the problem?\r\n\r\n(Note that you can also just keep an array of the batches from the unloader, and load/stream them through a root as necessary.)"}, {"user": "tisonkun", "created_at": "2024-12-26T05:46:09Z", "body": "OK thanks. Yes it seems a list of ArrowRecordBatch owns the buffer and doesn't need to tune with the lifecycle of allocator."}, {"user": "tisonkun", "created_at": "2024-12-26T05:56:55Z", "body": "Emmm .. No. The ArrowRecordBatch's buffer is still bound to the allocator, and it doesn't have the schema info where we need to store elsewhere."}, {"user": "lidavidm", "created_at": "2024-12-26T06:00:19Z", "body": "Yes, there isn't really any way of untying things from an allocator (this is intentional). There are APIs to transfer memory between allocators (or you can just keep a single allocator across different contexts)."}, {"user": "tisonkun", "created_at": "2024-12-26T06:02:21Z", "body": "@lidavidm Thanks for your information! Is there some docs/cookbook for copy VectorSchemaRoot? It seems challenging to ensure the lifetime of both data and allocator are aligned and I suppose some demo code would help a lot."}, {"user": "tisonkun", "created_at": "2024-12-26T06:03:25Z", "body": "For example, when I wrote:\r\n\r\n```java\r\n        while (reader.loadNextBatch()) {\r\n            final VectorSchemaRoot source = reader.getVectorSchemaRoot();\r\n            final VectorSchemaRoot copy = VectorSchemaRoot.create(source.getSchema(), allocator);\r\n            new VectorLoader(copy).load(new VectorUnloader(source).getRecordBatch());\r\n            batches.add(copy);\r\n        }\r\n```\r\n\r\nIt seems the intermediate ArrowRecordBatch should be closed but it's very easy to get it wrong and receive a runtime exception .."}, {"user": "lidavidm", "created_at": "2024-12-26T06:16:48Z", "body": "Unfortunately not. You should do something like\r\n\r\n```java\r\ntry (var batch = unloader.getRecordBatch()) {\r\n  loader.load(batch);\r\n}\r\n```"}], "satisfaction_conditions": ["Clear guidance on the proper way to copy a VectorSchemaRoot", "Information about memory management and allocator lifecycle", "Code examples showing proper resource handling", "Documentation or cookbook references for common Arrow operations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:03"}, "dockerfile": null, "language": "java"}
{"number": 16, "title": "HeliBoard integration", "created_at": "2025-01-13T09:55:42Z", "closed_at": "2025-01-13T13:41:00Z", "commit_id": "6c52c544e250a140f4297cf45e43c150ff8063ca", "labels": [], "url": "https://github.com/woheller69/whisperIME/issues/16", "body": "Just a quick question, how do I get HeliBoard to call Whisper instead of the Google voice assistant when pressing the mic button in the toolbar?", "comments_url": "https://api.github.com/repos/woheller69/whisperIME/issues/16/comments", "author": "C-O-D", "comments": [{"user": "woheller69", "created_at": "2025-01-13T10:13:22Z", "body": "Just switch off Google voice input method in Android settings"}, {"user": "C-O-D", "created_at": "2025-01-13T12:22:11Z", "body": "Okay, thanks..."}], "satisfaction_conditions": ["A simple method to configure HeliBoard to use Whisper instead of Google voice assistant", "Instructions that don't require technical expertise or complex setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:56"}, "dockerfile": null, "language": "java"}
{"number": 23, "title": "Archived Chats cannot be found", "created_at": "2025-01-21T09:20:34Z", "closed_at": "2025-01-21T19:49:26Z", "commit_id": "ca08df92fd9177e375f292671b849b90d18936fa", "labels": [], "url": "https://github.com/jarvis2f/telegram-files/issues/23", "body": "When searching for an archived chat it cannot be found. After unarchiving, it gets found instantly. It would be nice to get a setting to enable this feature.", "comments_url": "https://api.github.com/repos/jarvis2f/telegram-files/issues/23/comments", "author": "nudelmaker", "comments": [{"user": "jarvis2f", "created_at": "2025-01-21T13:31:26Z", "body": "@nudelmaker \nIn the latest version (0.1.11), we have added support for searching archived chats. You can try it out. \nThanks for the feedback."}, {"user": "nudelmaker", "created_at": "2025-01-21T19:49:26Z", "body": "Wow. Thanks for the fast implementation. Working great!"}], "satisfaction_conditions": ["Ability to search for and find archived chats", "Timely implementation of the requested feature"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:33"}, "dockerfile": null, "language": "java"}
{"number": 22, "title": "火山引擎TTS调用没有指定sample rate，导致采样率不匹配，声音发到客户端后失真", "created_at": "2025-04-09T08:47:33Z", "closed_at": "2025-04-09T08:54:01Z", "commit_id": "a3dea4e1020956dc0beeedad9ed6241dac40a400", "labels": [], "url": "https://github.com/joey-zhou/xiaozhi-esp32-server-java/issues/22", "body": "火山引擎TTS的sample rate默认是24000，小智协议默认的是16000，两者不匹配导致客户端受到声音播放的时候失真", "comments_url": "https://api.github.com/repos/joey-zhou/xiaozhi-esp32-server-java/issues/22/comments", "author": "yijunwu", "comments": [{"user": "yijunwu", "created_at": "2025-04-09T08:47:56Z", "body": "需要在VolcengineTtsService.sendRequest方法中audio下添加rate参数，设为16000"}, {"user": "joey-zhou", "created_at": "2025-04-09T08:49:08Z", "body": "最新代码已经修改了，20分钟前刚提交的，我也刚发现"}, {"user": "yijunwu", "created_at": "2025-04-09T08:49:55Z", "body": "手速真快 [thumb up]"}], "satisfaction_conditions": ["A solution that addresses the sample rate mismatch between Volcano Engine TTS and the client protocol", "Confirmation that the sample rate parameter has been properly configured", "Timely resolution of the audio distortion problem"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:39"}, "dockerfile": null, "language": "java"}
{"number": 13, "title": "Readme is confusing", "created_at": "2024-12-19T11:39:45Z", "closed_at": "2024-12-19T11:45:16Z", "commit_id": "d7ab6e7880379caca1c4af4825e7145fed2ddfdd", "labels": ["bug"], "url": "https://github.com/yegor256/together/issues/13", "body": "This is the code in readme:\r\n```java\r\nnew Together(\r\n  () -> {\r\n    // do the job\r\n    return true;\r\n  }\r\n)\r\n```\r\n\r\nIt's not valid because `Together` accepts `Actions` which is generic function that returns `T` and accepts `int`.\r\n\r\nSo it should be:\r\n```java\r\nnew Together(\r\n  thread -> {\r\n    // do the job\r\n    return true;\r\n  }\r\n)\r\n```", "comments_url": "https://api.github.com/repos/yegor256/together/issues/13/comments", "author": "maxonfjvipon", "comments": [{"user": "yegor256", "created_at": "2024-12-19T11:44:07Z", "body": "@maxonfjvipon fixed in cde9dc6 better now?"}, {"user": "maxonfjvipon", "created_at": "2024-12-19T11:45:17Z", "body": "@yegor256 yes, thanks"}], "satisfaction_conditions": ["Correction of the code example in the README to properly demonstrate the usage of the Together class", "Proper representation of the required function signature in code examples"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:52"}, "dockerfile": null, "language": "java"}
{"number": 13, "title": "[SOLVED/CLOSED] High scores not working", "created_at": "2025-02-21T19:48:03Z", "closed_at": "2025-02-23T20:07:22Z", "commit_id": "36234251a3887e988199b8cfd98ba3a48b41b525", "labels": [], "url": "https://github.com/hathibelagal-dev/Eidetic-Memory-Trainer/issues/13", "body": "Specs\nMoto G9 Power\nLineageOS 22.1\nAndroid 15\n\nInfo:\nI take 2 or 3 seconds to complete the test but my highscore shows 82, 12 or 7 seconds. If needed i can upload a video playing with w/ a timer on screen.", "comments_url": "https://api.github.com/repos/hathibelagal-dev/Eidetic-Memory-Trainer/issues/13/comments", "author": "artur15lima", "comments": [{"user": "hathibelagal-dev", "created_at": "2025-02-23T12:46:07Z", "body": "Hi, the timer starts the moment the numbers become visible on the screen, not after you press 1. Please confirm that this is how you're timing yourself too."}, {"user": "artur15lima", "created_at": "2025-02-23T20:07:06Z", "body": "Ok, now i understand it. Tysm."}], "satisfaction_conditions": ["Clarification about when the timer starts in the game", "Explanation for the discrepancy between perceived completion time and recorded high scores"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:11"}, "dockerfile": null, "language": "java"}
{"number": 89, "title": "uni-meter does not work when at startup the mqtt input device was not available", "created_at": "2025-03-31T17:16:26Z", "closed_at": "2025-04-01T02:29:33Z", "commit_id": "62e71362344f2b2ea29f6084d9e81634d0e9782c", "labels": ["bug"], "url": "https://github.com/sdeigm/uni-meter/issues/89", "body": "uni-meter-1.1.3\n\nHaving in file `/etc/uni-meter.conf` the following MQTT input source, where 192.168.1.4:1883 is the Home Assistant MQTT broker:\n\n```\n  input-devices {\n    mqtt {\n      url = \"tcp://192.168.1.4:1883\"\n      username = \"mqttclient\"\n      password = \"****\"\n\n      power-phase-mode = \"mono-phase\"\n      energy-phase-mode = \"mono-phase\"\n\n      channels = [{\n        type = \"json\"\n        topic = \"tele/tasmota_FA33FC/SENSOR\"\n        channel = \"power-total\"\n        json-path = \"$..power\"\n      },{\n        type = \"json\"\n        topic = \"tele/tasmota_FA33FC/SENSOR\"\n        channel = \"energy-consumption-total\"\n        json-path = \"$..energy_sum\"\n      },{\n        type = \"json\"\n        topic = \"tele/tasmota_FA33FC/SENSOR\"\n        channel = \"energy-production-total\"\n        json-path = \"$..energy_supply\"\n      }]\n    }\n  }\n```\n\nWhen uni-meter systemd unit is started before the Home Assistant MQTT broker is up and running, then uni-meter will not start working at all, even not when the MQTT broker becomes available.\n\nIt would be better if uni-meter would try every e.g. 1 min again, and start working as soon as the input source becomes available.\n\nCurrently, the solution is to restart uni-meter after mqtt broker is running.", "comments_url": "https://api.github.com/repos/sdeigm/uni-meter/issues/89/comments", "author": "Gitsarry", "comments": [{"user": "sdeigm", "created_at": "2025-04-01T02:26:10Z", "body": "Can confirm the problem. In theory uni-meter is already designed to always retry failed operations. Here I didn't reinitialized the underlyling MQTT library correctly."}, {"user": "Gitsarry", "created_at": "2025-04-01T03:53:32Z", "body": "Thank you very much for your work "}, {"user": "Gitsarry", "created_at": "2025-04-03T04:22:46Z", "body": "Have just tested with 1.1.4 and can confirm issue is fixed:\n\n- stop uni-meter\n- stop Home Assistant and with it the HA MQTT broker, which is input-source of uni-meter\n- start uni-meter\n- wait until log entry `MQTT stream failed: MqttException` occurs three times in a row\n- start Home Assistant\n- log entry `MQTT stream connected` shows up and uni-meter is working\n\n\n```\n25-04-03 06:10:58.007 INFO  uni-meter                - ##################################################################\n25-04-03 06:10:58.012 INFO  uni-meter                - # Universal electric meter converter 1.1.4 (2025-04-01 05:12:23) #\n25-04-03 06:10:58.012 INFO  uni-meter                - ##################################################################\n25-04-03 06:10:58.012 INFO  uni-meter                - initializing actor system\n25-04-03 06:10:58.781 INFO  org.apache.pekko.event.slf4j.Slf4jLogger - Slf4jLogger started\n25-04-03 06:10:59.252 INFO  uni-meter.controller     - creating ShellyPro3EM output device\n25-04-03 06:10:59.277 INFO  uni-meter.controller     - creating MQTT input device\n25-04-03 06:10:59.598 INFO  uni-meter.input          - subscribing to topic: tele/tasmota_FA33FC/SENSOR\n25-04-03 06:10:59.885 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:11:00.474 INFO  uni-meter.http.port-80   - HTTP server is listening on /[0:0:0:0:0:0:0:0]:80\n25-04-03 06:11:17.980 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:11:50.041 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:12:52.478 ERROR uni-meter.input          - MQTT stream failed: MqttException\n25-04-03 06:13:52.871 INFO  uni-meter.input          - MQTT stream connected\n\n```"}], "satisfaction_conditions": ["A mechanism for uni-meter to automatically reconnect to MQTT broker when it becomes available", "Resilience to input device unavailability at startup", "Appropriate error handling with retry capability", "No manual intervention required after input source becomes available"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:47"}, "dockerfile": null, "language": "java"}
{"number": 13, "title": "waystones with rarity are hidden", "created_at": "2024-12-29T01:58:04Z", "closed_at": "2025-01-01T19:53:04Z", "commit_id": "8e6b052a5daec84a7a907a99b85bf16ebe719dc4", "labels": [], "url": "https://github.com/Shudrum/poe2-shudrum-filter/issues/13", "body": "Thanks for a great filter. I noticed that rare and magic waystones are hidden. I tweaked your filter locally to add a line to show all rarities\r\n\r\n```\r\nShow\r\n  Class \"Waystone\"\r\n  AreaLevel == 82\r\n  Rarity <= Normal\r\n  WaystoneTier >= 16\r\n  PlayAlertSound 4 300\r\n  PlayEffect White\r\n  SetTextColor 200 200 200\r\n  SetBorderColor 74 68 58\r\n  SetBackgroundColor 74 68 58\r\n  SetFontSize 35\r\n  MinimapIcon 1 White Square\r\n```\r\n\r\nI was about to submit a pull request but noticed these are generated! Sorry I can't contribute to javascript, but I still wanted to report the issue", "comments_url": "https://api.github.com/repos/Shudrum/poe2-shudrum-filter/issues/13/comments", "author": "sgodbold", "comments": [{"user": "Shudrum", "created_at": "2024-12-29T15:36:36Z", "body": "Hello and thank you!\r\n\r\nA big update is comming soon. In any case, rare waystones are not hidden, but this filter does not do any distinction between normal / magic and rare waystones because all are importants.\r\n\r\nThe only way the rare one was hidden may be because of the difference between the map tier and the current area level. Maybe the rare should always be displayed."}, {"user": "sgodbold", "created_at": "2024-12-29T16:13:42Z", "body": "I believe it was an AreaLevel 52 map and definitely tier 15 waystones. I should have taken a screenshot I suppose. Excited for the update and I think I'll just hold off that. Feel free to close, thank you!"}, {"user": "Shudrum", "created_at": "2024-12-29T18:17:38Z", "body": "Thanks for the information! I'll do some tests before closing it."}, {"user": "Shudrum", "created_at": "2025-01-01T11:08:55Z", "body": "Hello, found the issue I think, and fixed it. On Waystones tier 15, on + level monsters areas the tier 15 maps can be hidden sometimes. Thanks again."}, {"user": "sgodbold", "created_at": "2025-01-01T19:53:04Z", "body": "Man that was a big update. Looks really good I downloaded and verified that magic and rare waystones appear. Thanks again and good luck!"}], "satisfaction_conditions": ["Fix for waystones of all rarities (normal, magic, rare) being properly displayed in the filter", "Proper handling of high-tier waystones (tier 15+) in different area levels", "Acknowledgment of the reported issue and commitment to investigate", "Timely resolution of the filter functionality issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:21"}, "dockerfile": null, "language": "javascript"}
{"number": 76, "title": "拉黑删除", "created_at": "2025-03-27T11:04:40Z", "closed_at": "2025-03-30T10:35:37Z", "commit_id": "757eb4c003deb455d47a95036960160f35f9b72c", "labels": [], "url": "https://github.com/StrayMeteor3337/WechatRealFriends/issues/76", "body": "如果同时被拉黑删除了 会显示啥呀", "comments_url": "https://api.github.com/repos/StrayMeteor3337/WechatRealFriends/issues/76/comments", "author": "gd123-ui", "comments": [{"user": "lonelywjx", "created_at": "2025-03-29T16:29:44Z", "body": "我看来应该只能检测拉黑，因为同时拉黑删除只提示拉黑"}, {"user": "gd123-ui", "created_at": "2025-03-30T03:43:23Z", "body": "好的 感谢\n\n\n\n在 2025-03-30 00:30:07，\"YeZi\" ***@***.***> 写道：\n\n我看来应该只能检测拉黑，因为同时拉黑删除只提示拉黑\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\n\nlonelywjx left a comment (StrayMeteor3337/WechatRealFriends#76)\n\n我看来应该只能检测拉黑，因为同时拉黑删除只提示拉黑\n\n—\nReply to this email directly, view it on GitHub, or unsubscribe.\nYou are receiving this because you authored the thread.Message ID: ***@***.***>"}], "satisfaction_conditions": ["Information about what notification appears when someone both blocks and deletes a user on WeChat", "Clarification about the detection capabilities of the tool regarding WeChat friend status"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:12"}, "dockerfile": null, "language": "javascript"}
{"number": 14, "title": "win10安装了cursor但是脚本提示未安装", "created_at": "2025-04-01T08:50:22Z", "closed_at": "2025-04-01T08:52:25Z", "commit_id": "fd531cfec18325924e381fe4bcf1ab30b00eb09d", "labels": [], "url": "https://github.com/isboyjc/cursor-reset/issues/14", "body": null, "comments_url": "https://api.github.com/repos/isboyjc/cursor-reset/issues/14/comments", "author": "chenhanzxc", "comments": [{"user": "isboyjc", "created_at": "2025-04-01T08:52:25Z", "body": "请勿更改安装位置"}, {"user": "chenhanzxc", "created_at": "2025-04-01T08:53:40Z", "body": "可以了谢谢"}], "satisfaction_conditions": ["Guidance on not changing the installation location of Cursor", "A simple, direct solution to resolve the error message about Cursor not being installed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:29"}, "dockerfile": null, "language": "javascript"}
{"number": 1, "title": "build with bun doesn't return version", "created_at": "2025-03-05T22:47:56Z", "closed_at": "2025-03-22T05:46:13Z", "commit_id": "c4d8fc8528040f5d0432493bec557d38bd356d81", "labels": [], "url": "https://github.com/ravitemer/mcp-hub/issues/1", "body": "when i build the package with bun (i cannot install -g packages with npm under nixos)\n\n```bash\n bun install -g mcp-hub@latest\nbun add v1.2.4 (fd9a5ea6)\n\ninstalled mcp-hub@1.4.0 with binaries:\n - mcp-hub\n\n[877.00ms] done\n❯ mcp-hub\n{\"type\":\"error\",\"code\":\"CLI_ARGS_ERROR\",\"message\":\"Failed to parse command line arguments\",\"data\":{\"message\":\"Missing required arguments: port, config\",\"help\":\"Use --help to see usage information\"},\"timestamp\":\"2025-03-05T22:47:17.068Z\"}\n❯ mcp-hub --version\nunknown\n```\n\n```json\n❯ mcp-hub --port 3000 --config ~/mcpservers.json\n{\"type\":\"info\",\"message\":\"Initializing MCP Hub\",\"data\":{},\"timestamp\":\"2025-03-05T22:33:05.567Z\"}\n{\"type\":\"info\",\"message\":\"Config loaded successfully from /Users/luxus/mcpservers.json\",\"data\":{\"path\":\"/Users/luxus/mcpservers.json\",\"serverCount\":2},\"timestamp\":\"2025-03-05T22:33:05.568Z\"}\n{\"type\":\"info\",\"message\":\"Starting 2 configured MCP servers in parallel\",\"data\":{\"count\":2},\"timestamp\":\"2025-03-05T22:33:05.568Z\"}\n{\"type\":\"info\",\"message\":\"Initializing MCP server 'fetch'\",\"data\":{\"server\":\"fetch\"},\"timestamp\":\"2025-03-05T22:33:05.568Z\"}\n{\"type\":\"info\",\"message\":\"Initializing MCP server 'todoist'\",\"data\":{\"server\":\"todoist\"},\"timestamp\":\"2025-03-05T22:33:05.571Z\"}\n{\"type\":\"debug\",\"message\":\"Server 'todoist' does not support capability 'resources/templates/list'\",\"data\":{\"server\":\"todoist\",\"error\":\"MCP error -32601: Method not found\"},\"timestamp\":\"2025-03-05T22:33:05.745Z\"}\n{\"type\":\"debug\",\"message\":\"Server 'todoist' does not support capability 'resources/list'\",\"data\":{\"server\":\"todoist\",\"error\":\"MCP error -32601: Method not found\"},\"timestamp\":\"2025-03-05T22:33:05.745Z\"}\n{\"type\":\"info\",\"message\":\"'todoist' MCP server connected\",\"data\":{\"server\":\"todoist\",\"tools\":5,\"resources\":0},\"timestamp\":\"2025-03-05T22:33:05.746Z\"}\n{\"type\":\"debug\",\"message\":\"Server 'fetch' does not support capability 'resources/templates/list'\",\"data\":{\"server\":\"fetch\",\"error\":\"MCP error -32601: Method not found\"},\"timestamp\":\"2025-03-05T22:33:06.077Z\"}\n{\"type\":\"debug\",\"message\":\"Server 'fetch' does not support capability 'resources/list'\",\"data\":{\"server\":\"fetch\",\"error\":\"MCP error -32601: Method not found\"},\"timestamp\":\"2025-03-05T22:33:06.077Z\"}\n{\"type\":\"info\",\"message\":\"'fetch' MCP server connected\",\"data\":{\"server\":\"fetch\",\"tools\":1,\"resources\":0},\"timestamp\":\"2025-03-05T22:33:06.077Z\"}\n{\"type\":\"info\",\"message\":\"Server initialization completed\",\"data\":{\"total\":2,\"successful\":2,\"failed\":0,\"disabled\":0,\"failedServers\":[]},\"timestamp\":\"2025-03-05T22:33:06.077Z\"}\n{\"type\":\"info\",\"message\":\"Starting HTTP server on port 3000\",\"data\":{\"port\":3000},\"timestamp\":\"2025-03-05T22:33:06.078Z\"}\n{\"type\":\"info\",\"message\":\"MCP_HUB_STARTED\",\"data\":{\"status\":\"ready\",\"port\":3000},\"timestamp\":\"2025-03-05T22:33:06.078Z\"}\n```", "comments_url": "https://api.github.com/repos/ravitemer/mcp-hub/issues/1/comments", "author": "luxus", "comments": [{"user": "ravitemer", "created_at": "2025-03-06T05:32:44Z", "body": "The issue is how `bun` installs global packages. mcp-hub looks for package.json to resolve version currently. In bun the symlinking and installation of global node_modules is little tricky. Will be fixed soon. \n\nThank you."}, {"user": "luxus", "created_at": "2025-03-06T17:20:40Z", "body": "yes it works, but as in the commit its hardcoded.. i guess this should stay open :D"}, {"user": "ravitemer", "created_at": "2025-03-22T05:46:13Z", "body": "This is now solved with the build step. "}], "satisfaction_conditions": ["The --version command should return the actual package version when installed with Bun", "The package should properly identify its version when installed through alternative package managers", "The version detection should work without hardcoding", "The package should function correctly for its primary purpose even when installed with Bun"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:03"}, "dockerfile": null, "language": "javascript"}
{"number": 6, "title": "如何修改容器80端口", "created_at": "2025-03-13T15:23:17Z", "closed_at": "2025-03-13T15:57:02Z", "commit_id": "49855b7b7cde8d8b330f64d1b5964b0c88092022", "labels": [], "url": "https://github.com/levywang/avhub/issues/6", "body": "80，81被NPM占用", "comments_url": "https://api.github.com/repos/levywang/avhub/issues/6/comments", "author": "Hansen1018", "comments": [{"user": "levywang", "created_at": "2025-03-13T15:56:52Z", "body": "举例\n```bash\ndocker run -d -p 8080:80 -v $PWD:/app --name avhub levywang/avhub:latest \n``` \n"}, {"user": "Hansen1018", "created_at": "2025-03-13T16:04:04Z", "body": "> 举例\n> \n> docker run -d -p 8080:80 -v $PWD:/app --name avhub levywang/avhub:latest\n\n是docker内部端口"}, {"user": "levywang", "created_at": "2025-03-14T01:23:42Z", "body": "你这个需求太小众，需要手动构建一个自己的镜像：\n克隆仓库后，修改`nginx.example.conf`中的端口为你自己想要的端口\n再修改`Dockerfile`中的`EXPOSE 80`端口，与上面的保持一致\n最后手动构建\n```bash\ndocker build -t <your_image_name> .\ndocker run ... <your_image_name>\n```\n"}, {"user": "Hansen1018", "created_at": "2025-03-14T02:51:08Z", "body": "> 你这个需求太小众，需要手动构建一个自己的镜像： 克隆仓库后，修改`nginx.example.conf`中的端口为你自己想要的端口 再修改`Dockerfile`中的`EXPOSE 80`端口，与上面的保持一致 最后手动构建\n> \n> docker build -t <your_image_name> .\n> docker run ... <your_image_name>\n\n好的，感谢"}], "satisfaction_conditions": ["Instructions for modifying the container's internal port configuration", "A solution that works around ports 80 and 81 being occupied by NPM", "Step-by-step guidance for creating a custom Docker image with modified port settings", "Information about which specific files need modification to change the container's port configuration"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:44"}, "dockerfile": null, "language": "javascript"}
{"number": 300, "title": "Brave search mcp server error: fetch is not defined", "created_at": "2024-12-10T23:16:32Z", "closed_at": "2024-12-11T00:30:53Z", "commit_id": "1c30f54b2dd27f50003a9b1f85c4fce93c09b08d", "labels": ["bug"], "url": "https://github.com/modelcontextprotocol/servers/issues/300", "body": "**Describe the bug**\r\nI have configured the brave search mcp server on my mac, but when Claude tried to used got **Error: fetch is not defined**\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Configure the brave search mcp server like this\r\n\"brave-search\": {\r\n        \"command\": \"npx\",\r\n        \"args\": [\r\n          \"-y\",\r\n          \"@modelcontextprotocol/server-brave-search\"\r\n        ],\r\n        \"env\": {\r\n          \"BRAVE_API_KEY\": \"BSASaoHXXXXXXXXXXXX\"\r\n        }\r\n      }\r\n2. Restart Claude Desktop app\r\n3. look for the MCP available tools\r\n4. got the brave_web_search tools listed\r\n5. ask about something like: look something about steve jobs \r\n6. Claude tried to do\r\n{\r\n  `count`: 5,\r\n  `query`: `Steve Jobs biography achievements Apple history`\r\n}\r\n\r\n**Expected behavior**\r\nTo use the results from the brave search API\r\n\r\n**Logs**\r\nIf applicable, add logs to help explain your problem.\r\n\r\n**Additional context**\r\nMy current Claude Desktop version is Version 0.7.5 (0.7.5)\r\n", "comments_url": "https://api.github.com/repos/modelcontextprotocol/servers/issues/300/comments", "author": "juanmacedan1co", "comments": [{"user": "juanmacedan1co", "created_at": "2024-12-11T00:31:49Z", "body": "the fix was to include the correct node version in the ENV \r\nPATH=/Users/username/.nvm/versions/node/v20.18.0/bin:/usr/local/bin:/usr/bin:/bin"}, {"user": "wolf019", "created_at": "2025-01-07T09:26:50Z", "body": "Thanks for reporting this issue! I encountered the same \"Error: fetch is not defined\" problem on my mac.\r\n\r\nThe solution that worked for me was updating the Node version in the PATH environment variable in the claude_desktop_config.json:\r\n\r\n```\r\n{\r\n  \"mcpServers\": {\r\n    \"brave-search\": {\r\n      \"command\": \"npx\",\r\n      \"args\": [\r\n        \"-y\",\r\n        \"@modelcontextprotocol/server-brave-search\"\r\n      ],\r\n      \"env\": {\r\n        \"BRAVE_API_KEY\": \"your-api-key\",\r\n        \"PATH\": \"/Users/username/.nvm/versions/node/v20.18.0/bin:/usr/local/bin:/usr/bin:/bin\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nKey points:\r\n1. Make sure to use a recent Node version (I updated from v16 to v20)\r\n2. Include the full PATH with all system directories\r\n3. Restart Claude Desktop after making these changes\r\n\r\nThis resolved the fetch not defined error and now the Brave search functionality works perfectly!"}], "satisfaction_conditions": ["A solution that resolves the 'fetch is not defined' error when using Brave search MCP server", "A proper Node.js environment configuration for the MCP server", "Clear instructions for modifying the Claude Desktop configuration", "A working integration between Claude Desktop and Brave search functionality"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:01:48"}, "dockerfile": "FROM node:20-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install necessary tools\nRUN apt-get update && \\\n    apt-get install -y git && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/modelcontextprotocol/servers.git . && \\\n    git checkout 1c30f54b2dd27f50003a9b1f85c4fce93c09b08d\n\n# Install dependencies for brave-search server\nWORKDIR /app/src/brave-search\n\n# Install dependencies and build the project\nRUN npm ci && \\\n    npm run build\n\n# Set environment variables (user will need to provide their own API key)\nENV BRAVE_API_KEY=\"\"\n\n# Set working directory back to the project root\nWORKDIR /app\n\n# Comment explaining the issue and solution\n# This Dockerfile sets up an environment to address the \"fetch is not defined\" error\n# in the brave search MCP server. The issue is likely due to a missing polyfill \n# in the Node.js environment. Building the project from the specific commit ensures\n# we're working with the version that exhibits the issue.", "language": "javascript"}
{"number": 155, "title": "Github connection is not working", "created_at": "2024-12-01T11:23:54Z", "closed_at": "2024-12-03T14:06:52Z", "commit_id": "6135c62c699fa39f71e4d33c8c226c57128dc1c3", "labels": ["bug"], "url": "https://github.com/modelcontextprotocol/servers/issues/155", "body": "I have tried connecting with Claude and Github using MCP for windows\r\nI tried the first method given by the Anthropic but couldn't connect.\r\n\r\nNow I have tried the second method using the following method and this Sql lite is connected but other servers are not getting connected.\r\n\r\n\"Step-by-Step Guide:\r\n1. Locate Node.js and npm paths\r\nOpen Command Prompt (CMD) as administrator and run:\r\n\r\nwhere node\r\nThis will show your Node.js executable path. Example output:\r\n\r\nD:\\Program\\nvm\\node.exe\r\nThen find your global npm packages location:\r\n\r\nnpm root -g\r\nExample output:\r\n\r\nD:\\Program\\nvm\\node_modules\r\n2. Install Required Packages Globally\r\nRun these commands in CMD:\r\n\r\nnpm install -g @modelcontextprotocol/server-filesystem\r\nnpm install -g @modelcontextprotocol/server-github\r\nnpm install -g @modelcontextprotocol/server-memory\r\nnpm install -g @modelcontextprotocol/server-puppeteer\r\nnpm install -g @modelcontextprotocol/server-brave-search\r\nnpm install -g @modelcontextprotocol/server-google-maps\r\nnpm install -g @modelcontextprotocol/server-postgres\r\n3. Verify Installations\r\nCheck each package installation:\r\n\r\nnpm list -g @modelcontextprotocol/server-filesystem\r\nnpm list -g @modelcontextprotocol/server-github\r\nnpm list -g @modelcontextprotocol/server-memory\r\nnpm list -g @modelcontextprotocol/server-puppeteer\r\nnpm list -g @modelcontextprotocol/server-brave-search\r\nnpm list -g @modelcontextprotocol/server-google-maps\r\nnpm list -g @modelcontextprotocol/server-postgres\r\nExpected output format:\r\n\r\nD:\\Program\\nvm -> .\\\r\n`-- @modelcontextprotocol/server-[package-name]@0.5.1\r\n4. Update Configuration File\r\nModify your claude_desktop_config.json with the following content (adjust paths according to your system):\r\n\r\n{\r\n  \"mcpServers\": {\r\n    \"sqlite\": {\r\n      \"command\": \"uvx\",\r\n      \"args\": [\r\n        \"mcp-server-sqlite\",\r\n        \"--db-path\",\r\n        \"D:\\\\github_repository\\\\test.db\"\r\n      ]\r\n    },\r\n    \"filesystem\": {\r\n      \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\r\n      \"args\": [\r\n        \"D:\\\\Program\\\\nvm\\\\node_modules\\\\@modelcontextprotocol\\\\server-filesystem\\\\dist\\\\index.js\",\r\n        \"D:\\\\github_repository\",\r\n        \"D:\\\\github_repository\\\\image-generator\"\r\n      ]\r\n    },\r\n    \"github\": {\r\n      \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\r\n      \"args\": [\r\n        \"D:\\\\Program\\\\nvm\\\\node_modules\\\\@modelcontextprotocol\\\\server-github\\\\dist\\\\index.js\"\r\n      ],\r\n      \"env\": {\r\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"\"\r\n      }\r\n    },\r\n    \"postgres\": {\r\n      \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\r\n      \"args\": [\r\n        \"D:\\\\Program\\\\nvm\\\\node_modules\\\\@modelcontextprotocol\\\\server-postgres\\\\dist\\\\index.js\",\r\n        \"postgresql://localhost/mydb\"\r\n      ]\r\n    },\r\n    \"memory\": {\r\n      \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\r\n      \"args\": [\r\n        \"D:\\\\Program\\\\nvm\\\\node_modules\\\\@modelcontextprotocol\\\\server-memory\\\\dist\\\\index.js\"\r\n      ]\r\n    },\r\n    \"puppeteer\": {\r\n      \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\r\n      \"args\": [\r\n        \"D:\\\\Program\\\\nvm\\\\node_modules\\\\@modelcontextprotocol\\\\server-puppeteer\\\\dist\\\\index.js\"\r\n      ]\r\n    },\r\n    \"brave-search\": {\r\n      \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\r\n      \"args\": [\r\n        \"D:\\\\Program\\\\nvm\\\\node_modules\\\\@modelcontextprotocol\\\\server-brave-search\\\\dist\\\\index.js\"\r\n      ],\r\n      \"env\": {\r\n        \"BRAVE_API_KEY\": \"\"\r\n      }\r\n    },\r\n    \"google-maps\": {\r\n      \"command\": \"D:\\\\Program\\\\nvm\\\\node.exe\",\r\n      \"args\": [\r\n        \"D:\\\\Program\\\\nvm\\\\node_modules\\\\@modelcontextprotocol\\\\server-google-maps\\\\dist\\\\index.js\"\r\n      ],\r\n      \"env\": {\r\n        \"GOOGLE_MAPS_API_KEY\": \"\"\r\n      }\r\n    },\r\n    \"fetch\": {\r\n      \"command\": \"uvx\",\r\n      \"args\": [\r\n        \"mcp-server-fetch\"\r\n      ]\r\n    }\r\n  },\r\n  \"globalShortcut\": \"Ctrl+Q\"\r\n}\r\n\"\r\n\r\nI have been trying it for more than 2 days but couldn't get it connected.\r\n", "comments_url": "https://api.github.com/repos/modelcontextprotocol/servers/issues/155/comments", "author": "experienceswithanish", "comments": [{"user": "hemangjoshi37a", "created_at": "2024-12-02T01:55:16Z", "body": "This is similar to my issue in #152 that is solved in #40"}, {"user": "experienceswithanish", "created_at": "2024-12-02T09:47:24Z", "body": "> This is similar to my issue in #152 that is solved in #40\r\n\r\nI have just used your config file and it worked.\r\nI don't know how to thank you, seriously I have been trying to get it worked for more than 3 days and finally got it\r\nThank you"}], "satisfaction_conditions": ["A working configuration for connecting Claude to GitHub via MCP on Windows", "A configuration file that properly specifies the correct paths and settings for MCP servers", "A solution that works with minimal troubleshooting", "Clear guidance on how to properly configure the GitHub connection specifically"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:56"}, "dockerfile": null, "language": "javascript"}
{"number": 22, "title": "你好，关于只转存大于多少集的写法不太懂。", "created_at": "2025-04-08T03:57:15Z", "closed_at": "2025-04-08T05:28:50Z", "commit_id": "8af1d2293e10041147420e5ffd00b06002e912c1", "labels": [], "url": "https://github.com/1307super/cloud189-auto-save/issues/22", "body": "我转存的片名是：S01E25.2024.2160p.WEB-DL.H264.AAC.mp4，如果我只想转存24集以上的是不是前面填S01E(\\d+).2024.2160p.WEB-DL.H264.AAC.mp4中间选大于，后面值填24？但是实测没用，应该怎么填？", "comments_url": "https://api.github.com/repos/1307super/cloud189-auto-save/issues/22/comments", "author": "Tincichow", "comments": [{"user": "1307super", "created_at": "2025-04-08T03:59:14Z", "body": "你翻一翻已关闭的issue里 有一个和你的问题一样，里面有个正则表达式 对你这个也有效，可以直接用"}, {"user": "Tincichow", "created_at": "2025-04-08T03:59:34Z", "body": "> 我转存的片名是:S01E25.2024.2160p.WEB-DL.H264.AAC.mp4\n\n\n\n> 你翻一翻已关闭的issue里 有一个和你的问题一样，里面有个正则表达式 对你这个也有效，可以直接用\n\n我看看"}, {"user": "Tincichow", "created_at": "2025-04-08T04:02:57Z", "body": "> 你翻一翻已关闭的issue里 有一个和你的问题一样，里面有个正则表达式 对你这个也有效，可以直接用\n\n你好，我前面填了(?<=E)\\\\d+中间选大于后面写24然后点执行还是不会转存第25集- -，是错了吗？"}, {"user": "1307super", "created_at": "2025-04-08T04:04:50Z", "body": "用这个 (?<=E)\\\\d+ 动态执行正则需要给反斜杠加转义"}, {"user": "Tincichow", "created_at": "2025-04-08T04:07:09Z", "body": "> 用这个 (?<=E)\\d+ 动态执行正则需要给反斜杠加转义\n\n终于可以了，万分感谢。"}, {"user": "1307super", "created_at": "2025-04-08T04:08:03Z", "body": "不用客气"}], "satisfaction_conditions": ["A working regular expression pattern to filter TV episodes by episode number", "Guidance on proper syntax for regular expressions in the application context", "A solution that correctly extracts and compares episode numbers from filenames"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:14"}, "dockerfile": null, "language": "javascript"}
{"number": 27, "title": "Dear, so now it is not possible to use five devices under the same account?", "created_at": "2025-01-06T11:52:50Z", "closed_at": "2025-01-06T14:51:34Z", "commit_id": "e17f233791675664a9cfcddb61731d324f942066", "labels": [], "url": "https://github.com/recitativonika/blockless-bless-network-bot/issues/27", "body": "Dear, so now it is not possible to use five devices under the same account?", "comments_url": "https://api.github.com/repos/recitativonika/blockless-bless-network-bot/issues/27/comments", "author": "youngyeh310", "comments": [{"user": "recitativonika", "created_at": "2025-01-06T11:57:40Z", "body": "Install extension - login - copy your nodeid - delete extension, repeat till 5 node in your account."}, {"user": "youngyeh310", "created_at": "2025-01-06T13:31:51Z", "body": "THX"}], "satisfaction_conditions": ["A method to use the same account across five devices", "A step-by-step process that works within the current system constraints"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:40"}, "dockerfile": null, "language": "javascript"}
{"number": 12, "title": "Can multiple proxies run on 1 usertoken?", "created_at": "2024-11-25T07:46:33Z", "closed_at": "2024-11-25T07:49:36Z", "commit_id": "f138367ec841e68df44b25ce91f79b501c7e7080", "labels": [], "url": "https://github.com/recitativonika/blockless-bless-network-bot/issues/12", "body": "I have a question: if a userToken has one nodeId and one hardwareId, can it run multiple proxies? Will it receive multiple rewards?\r\nOr can each userToken, nodeId, and hardwareId only run on one proxy?\r\n\r\nThanks for reading. I look forward to your reply.", "comments_url": "https://api.github.com/repos/recitativonika/blockless-bless-network-bot/issues/12/comments", "author": "lenhu96", "comments": [{"user": "recitativonika", "created_at": "2024-11-25T07:49:36Z", "body": "1 nodeid is only can run 1 process, so you can't run 1 nodeid with multiple proxies. But usertoken can run multiple different nodeid (5 max)"}, {"user": "lenhu96", "created_at": "2024-11-25T08:08:39Z", "body": "Thank you for sharing.\r\nBut I noticed in the config.js file, there is a format like the one below:\r\n\r\njavascript\r\nCopy code\r\nusertoken: 'usertoken1',\r\nnodes: [\r\n    { nodeId: 'nodeid1', hardwareId: 'hardwareid1', proxy: 'proxy1' },\r\n    { nodeId: 'nodeid2', hardwareId: 'hardwareid2', proxy: 'proxy2' },\r\n    { nodeId: 'nodeid3', hardwareId: 'hardwareid3', proxy: 'proxy3' },\r\n    { nodeId: 'nodeid4', hardwareId: 'hardwareid4', proxy: 'proxy4' },\r\n    { nodeId: 'nodeid5', hardwareId: 'hardwareid5', proxy: 'proxy5' }\r\n]\r\nFrom this, I see that on the same PC, if there are 2 browsers, there will be 2 userTokens and 2 nodeIds but the same hardwareId.\r\nSo, in what situation would there be a case like the format you shared, where one userToken has multiple nodeIds?\r\n\r\nI hope you understand as I still don’t fully grasp it.\r\nLooking forward to your response.\r\n\r\nThank you very much."}, {"user": "recitativonika", "created_at": "2024-11-25T08:13:57Z", "body": "You only need one usertoken for one account, each time you login the account in the different browser or device you will have a different usertoken, just copy one. For nodeid, each extension installation will have a different nodeid and hardwareid comes from the hardware identification of your device, you will need to install extension in different device to have different hardwareid."}, {"user": "lenhu96", "created_at": "2024-11-25T08:18:09Z", "body": "I got it\r\nAwesome\r\nThanks for sharing\r\nLove you <3 "}], "satisfaction_conditions": ["Clear explanation of the relationship between usertoken, nodeId, and hardwareId", "Clarification on the maximum number of proxies that can be run with a single configuration", "Explanation of how the config.js file structure relates to real-world usage", "Information about how hardware identification works across different devices"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:46"}, "dockerfile": null, "language": "javascript"}
{"number": 5, "title": "Position B7S_AUTH_TOKEN ", "created_at": "2024-11-17T01:18:24Z", "closed_at": "2024-11-17T01:23:59Z", "commit_id": "62dd9dcd52b1df1a3021274794d56ed16bad8e10", "labels": [], "url": "https://github.com/recitativonika/blockless-bless-network-bot/issues/5", "body": "bang posisi B7S_AUTH_TOKEN dimana ya barusan pake console tab dan check distorage application,,, kosong..", "comments_url": "https://api.github.com/repos/recitativonika/blockless-bless-network-bot/issues/5/comments", "author": "rezzachuky2", "comments": [{"user": "recitativonika", "created_at": "2024-11-17T01:23:59Z", "body": "you must already logged in and in the dashboard to get the token, please read it again slowly"}, {"user": "rezzachuky2", "created_at": "2024-11-17T01:56:07Z", "body": "> you must already logged in and in the dashboard to get the token, please read it again slowly\r\n\r\nthanks\r\n"}], "satisfaction_conditions": ["Clear instructions on where to find the B7S_AUTH_TOKEN", "Explanation of prerequisites needed to access the token"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:52"}, "dockerfile": null, "language": "javascript"}
{"number": 1, "title": "Can i run more than one nodeid?", "created_at": "2024-11-14T20:58:41Z", "closed_at": "2024-11-15T11:48:13Z", "commit_id": "29238b7df408fa30be40ab888d4f86b2a877913d", "labels": [], "url": "https://github.com/recitativonika/blockless-bless-network-bot/issues/1", "body": "Can i run more than one node id at the same time? Coz when i put second nodeid in the new line it doean't react", "comments_url": "https://api.github.com/repos/recitativonika/blockless-bless-network-bot/issues/1/comments", "author": "mizaty", "comments": [{"user": "recitativonika", "created_at": "2024-11-14T22:26:31Z", "body": "no, I only test using one node id. maybe I will make it to support multi nodeid/hardwareid later when I have time"}, {"user": "recitativonika", "created_at": "2024-11-15T11:48:14Z", "body": "Now support multi nodeid,  please pull the repo again."}, {"user": "mizaty", "created_at": "2024-11-15T11:50:35Z", "body": "Bro you're legend,do you have any telegram contact or whatever i can catch up"}, {"user": "recitativonika", "created_at": "2024-11-15T11:58:22Z", "body": "Please pull the repo again,  I forgot to add ipfetch.\r\n\r\nFor my contact,  sorry I can't give to anyone for my privacy sake."}, {"user": "mizaty", "created_at": "2024-11-15T12:04:41Z", "body": "It works now thanksyou, its oke mate i respect that"}], "satisfaction_conditions": ["Support for running multiple node IDs simultaneously", "Clear instructions on how to obtain the updated functionality", "Complete implementation with all necessary components"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:59"}, "dockerfile": null, "language": "javascript"}
{"number": 68, "title": "Cannot to use both layout and catchall at the same level", "created_at": "2025-04-02T22:24:43Z", "closed_at": "2025-04-03T16:26:11Z", "commit_id": "43f8e332f3234921a6a61ade5be2f0475f10b7df", "labels": [], "url": "https://github.com/colinlienard/sv-router/issues/68", "body": "I cannot use both a layout and a catchall route at the same level. For instance, this renders MyPage but not Layout:\n\n```\n      '/poc': {\n        '*breadcrumbs': MyPage,\n        layout: Layout,\n      }\n```", "comments_url": "https://api.github.com/repos/colinlienard/sv-router/issues/68/comments", "author": "lmaccherone", "comments": [{"user": "colinlienard", "created_at": "2025-04-03T16:28:38Z", "body": "Hey @lmaccherone thanks for reporting this!\nI published a new version with the fix, should be good now"}, {"user": "lmaccherone", "created_at": "2025-04-03T17:34:34Z", "body": "Thanks! That fixed it."}], "satisfaction_conditions": ["A fix that allows simultaneous use of layout and catchall routes at the same level", "Proper rendering of both the Layout and MyPage components when configured together", "Timely resolution through a version update"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:37"}, "dockerfile": null, "language": "javascript"}
{"number": 51, "title": "Python < 3.11 backward compatibility for timeout.", "created_at": "2025-03-24T13:25:20Z", "closed_at": "2025-03-26T02:47:23Z", "commit_id": "509e513f3aedf59f47cb78cdb1f68d9953f87261", "labels": [], "url": "https://github.com/willmiao/ComfyUI-Lora-Manager/pull/51", "body": "Hi,\r\n\r\nasyncio.timeout is only available starting with python 3.11. I made this small change to make it work for earlier versions too.", "comments_url": "https://api.github.com/repos/willmiao/ComfyUI-Lora-Manager/issues/51/comments", "author": "AlUlkesh", "comments": [{"user": "willmiao", "created_at": "2025-03-26T02:47:15Z", "body": "Thanks for the fix! Merging now."}, {"user": "willmiao", "created_at": "2025-03-26T11:22:43Z", "body": "@AlUlkesh Hi, just an update here. I tested the code and found that it caused an empty recipe cache when running on Python 3.12.7. After reviewing the implementation, it seems that the timeout is no longer necessary, so I’ve removed the related code. Everything is working fine so far. Thanks again for your PR—I really appreciate the effort to improve compatibility!"}, {"user": "AlUlkesh", "created_at": "2025-03-26T14:30:20Z", "body": "Oh, glad you caught that so soon.  Thanks."}], "satisfaction_conditions": ["A solution that maintains compatibility across different Python versions", "A solution that ensures the code functions correctly without errors", "Acknowledgment of their contribution effort"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:27"}, "dockerfile": null, "language": "javascript"}
{"number": 37, "title": "Illustrious, Sorting, and bits", "created_at": "2025-03-12T15:34:25Z", "closed_at": "2025-03-13T03:12:06Z", "commit_id": "2ea0fa8471aa82e7860ca644450e0169dea8e754", "labels": [], "url": "https://github.com/willmiao/ComfyUI-Lora-Manager/issues/37", "body": "This is a fantastic node its really awesome, thank you! I love the improvements of the tags and sort by lora type. \nCouple of things through, the ILL (I think that Illustrious) are not bringing back any results (I have some Illustrious Loras) there any more sorting options which could be used, date and name are great but I added new loras and the have not appeared. \nHave you any plans to make something similar for checkpoints? that would be awesome!\nThanks", "comments_url": "https://api.github.com/repos/willmiao/ComfyUI-Lora-Manager/issues/37/comments", "author": "AllanKustom", "comments": [{"user": "willmiao", "created_at": "2025-03-13T03:11:23Z", "body": "Hi, thanks for the support! The issue with the Illustrious base model was due to an inconsistent naming bug, which I've now fixed.\n\nRegarding your feature requests: checkpoint management is already planned. However, for the upcoming week, I'll be fully focused on an exciting new feature. So while I'll add your suggestion to the list, its priority will be lower for now. Appreciate your patience!"}, {"user": "AllanKustom", "created_at": "2025-03-13T10:07:26Z", "body": "> Hi, thanks for the support! The issue with the Illustrious base model was due to an inconsistent naming bug, which I've now fixed.\n> \n> Regarding your feature requests: checkpoint management is already planned. However, for the upcoming week, I'll be fully focused on an exciting new feature. So while I'll add your suggestion to the list, its priority will be lower for now. Appreciate your patience!\n\nThank you :)"}], "satisfaction_conditions": ["Fix for the Illustrious Lora model search functionality", "Acknowledgment of the checkpoint management feature request", "Transparency about development priorities and timeline"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:34"}, "dockerfile": null, "language": "javascript"}
{"number": 36, "title": "No more scrolling functionality", "created_at": "2025-03-10T08:40:16Z", "closed_at": "2025-03-11T18:58:12Z", "commit_id": "72a82707ea4d9192d2fe63d53ac893e1a7e0797a", "labels": [], "url": "https://github.com/willmiao/ComfyUI-Lora-Manager/issues/36", "body": "### **LoRA Manager Version**\n- Version: latest\n\n### **Environment Information**\n- **Operating System**:Windows 11, running ComfyUI in Conda Environment\n- **Browser & Version**: Edge 134\n\n### **Issue Description**\n- The manager only shows 16 Loras, where as I used to be able to scroll and see all loras.\n- I can still search and the other loras will come up, but no more scroll function.\n", "comments_url": "https://api.github.com/repos/willmiao/ComfyUI-Lora-Manager/issues/36/comments", "author": "fredericklessing", "comments": [{"user": "willmiao", "created_at": "2025-03-11T14:47:37Z", "body": "Hi, this issue was likely introduced by a previous change that was meant to fix a layout issue reported by another user. However, since it didn’t work as intended, I’ve reverted the commit in the latest release (v0.7.36).\n\nPlease try updating to see if the problem is resolved. Let me know if it works!"}, {"user": "fredericklessing", "created_at": "2025-03-11T18:58:12Z", "body": "Thank you so much, it is working again. Much appreciated."}], "satisfaction_conditions": ["Restoration of the scrolling functionality in the LoRA Manager", "Access to the complete collection of LoRAs beyond the initial 16 displayed", "A timely fix that doesn't require complex user intervention"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:40"}, "dockerfile": null, "language": "javascript"}
{"number": 35, "title": "Stale file_path metadata", "created_at": "2025-03-09T05:12:33Z", "closed_at": "2025-03-26T08:42:55Z", "commit_id": "250e8445bbd0d511c916b143571e8474aed9ae65", "labels": [], "url": "https://github.com/willmiao/ComfyUI-Lora-Manager/issues/35", "body": "I moved some folders, and while the preview images refreshed, the file_path in the metadata.json files did not, so none of those loras can be loaded in the LoraManager Lora Loader anymore. Can you either deduce the file_path each time or update it on refresh? Thanks!\n\nps. Love the component! +1 to the lora_stack and /models endpt feature requests.", "comments_url": "https://api.github.com/repos/willmiao/ComfyUI-Lora-Manager/issues/35/comments", "author": "broken", "comments": [{"user": "willmiao", "created_at": "2025-03-09T11:32:23Z", "body": "Thanks for the feedback! When you say \"moved some folders,\" do you mean you manually moved them in the file explorer?"}, {"user": "broken", "created_at": "2025-03-12T07:49:51Z", "body": "Yes. That's what I mean."}, {"user": "willmiao", "created_at": "2025-03-12T09:43:37Z", "body": "I wouldn't recommend manually moving folders at this time. The watchdog monitors additions and deletions within loras_root, but when files are moved manually, it only detects new additions—not deletions. Plus due to the unpredictable order of multiple file moves (e.g., the LoRA file moving before its metadata file), cache inconsistencies or even metadata loss may occur.\n\nIf I have time, I’ll look into a more sophisticated solution to handle this better. For now, I recommend using the bulk operation feature in LoRA Manager to move files within the interface safely.\n\nThat said, I've submitted a fix that will attempt to correct incorrect file paths when rebuilding the cache on startup. If you're experiencing issues where metadata errors prevent LoRAs from loading, please try restarting ComfyUI and see if that resolves the problem.\n\nAlso, LoRA Stack is already supported in v0.7.36, and checkpoint management is planned for a future update."}, {"user": "broken", "created_at": "2025-03-12T16:34:06Z", "body": "Yeah, I noticed that behavior with the monitor.\n\nI'm away atm, but will test this change and the lora stacks when I get back home in a few days. Thanks!"}, {"user": "broken", "created_at": "2025-03-26T08:42:55Z", "body": "Confirming this was fixed. Thanks!"}], "satisfaction_conditions": ["A solution that addresses the mismatch between moved files and their metadata paths", "A way to maintain consistency between actual file locations and their recorded paths in metadata", "Support for LoRA stacks functionality", "Clear guidance on proper file management practices within the system"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:48"}, "dockerfile": null, "language": "javascript"}
{"number": 29, "title": "Suggestion - Add Support for download through HTTPS Connections", "created_at": "2025-03-07T20:09:24Z", "closed_at": "2025-03-12T10:54:14Z", "commit_id": "e8e5012f0c1b83c23d6ff8864fe91c0885fb1aab", "labels": [], "url": "https://github.com/willmiao/ComfyUI-Lora-Manager/issues/29", "body": "Thanks again for your work!\n\nI’d be really grateful if you could look into another issue. When accessing the LoRA loader page via an HTTP Cloudflare address (e.g., when deploying on RunPod) instead of through TCP, attempting to download a LoRA results in the following error:\n\nFailed to construct 'WebSocket': An insecure WebSocket connection may not be initiated from a page loaded over HTTPS.\n\nI'm not sure how easily this can be fixed, but if you have time to address it, it could significantly enhance the usability of your plugin for cloud deployments.\n\nThanks!", "comments_url": "https://api.github.com/repos/willmiao/ComfyUI-Lora-Manager/issues/29/comments", "author": "jnxmx", "comments": [{"user": "willmiao", "created_at": "2025-03-09T11:37:03Z", "body": "Thank you! I'll look into this."}, {"user": "willmiao", "created_at": "2025-03-11T14:51:13Z", "body": "Hi, I’ve added a fix for this issue in the latest release (v0.7.36). Please try updating and let me know if the problem is resolved.\n5a6c4128455a5b23e909a89fc3f201f183fe868b"}, {"user": "jnxmx", "created_at": "2025-03-12T09:40:41Z", "body": "Works like charm!\nThank you a lot"}], "satisfaction_conditions": ["A solution that enables downloading LoRA files when accessing the loader page via HTTPS", "Compatibility with cloud deployment environments", "Resolution of the WebSocket security error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:56"}, "dockerfile": null, "language": "javascript"}
{"number": 118, "title": "Japanese language not working properly", "created_at": "2025-02-28T12:57:41Z", "closed_at": "2025-03-01T10:16:40Z", "commit_id": "b15ef354b2db4de4654fb521b52908c1c2d79c7e", "labels": [], "url": "https://github.com/hexgrad/kokoro/issues/118", "body": "When selecting \"j\" as language code and a Japanese voice, it only creates a very short audio with unrecognizable utterance. The produced phonemes are correct though, and other languages such as English work fine.", "comments_url": "https://api.github.com/repos/hexgrad/kokoro/issues/118/comments", "author": "kaieberl", "comments": [{"user": "hexgrad", "created_at": "2025-02-28T19:08:40Z", "body": "Do you have the latest versions of `kokoro` and `misaki` both at 0.8.2? You can `pip show kokoro misaki` to check.\n\nI just ran this code in Google Colab and got audio:\n```py\n!pip install -q kokoro>=0.8.2 soundfile\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n!pip install \"misaki[ja]>=0.8.2\"\n\nfrom kokoro import KPipeline\nfrom IPython.display import display, Audio\nimport soundfile as sf\nimport torch\npipeline = KPipeline(lang_code='j')\ntext = '「もしおれがただ偶然、そしてこうしようというつもりでなくここに立っているのなら、ちょっとばかり絶望するところだな」と、そんなことが彼の頭に思い浮かんだ。'\n\ngenerator = pipeline(\n    text, voice='jf_alpha',\n    speed=1, split_pattern=r'\\n+'\n)\n\nfor i, (gs, ps, audio) in enumerate(generator):\n    print(i)  # i => index\n    print(gs) # gs => graphemes/text\n    print(ps) # ps => phonemes\n    display(Audio(data=audio, rate=24000, autoplay=i==0))\n    sf.write(f'{i}.wav', audio, 24000) # save each audio file\n```"}, {"user": "kaieberl", "created_at": "2025-03-01T10:15:13Z", "body": "Thank you for the quick reply!\nI just tried again with Python 3.11 and it worked, so probably there is no pip version for 3.9"}], "satisfaction_conditions": ["A working solution for Japanese language audio generation", "Information about compatibility requirements or dependencies", "A way to verify correct installation and configuration"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:03:06"}, "dockerfile": "FROM python:3.10-slim\n\n# Set up environment variables\nENV PYTHONUNBUFFERED=1 \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    build-essential \\\n    espeak-ng \\\n    mecab \\\n    libmecab-dev \\\n    mecab-ipadic-utf8 \\\n    unidic-mecab \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/hexgrad/kokoro.git . \\\n    && git checkout b15ef354b2db4de4654fb521b52908c1c2d79c7e\n\n# Install Python dependencies with optimizations for faster build\nRUN pip install --upgrade pip \\\n    && pip install -e . \\\n    && pip install misaki[ja] torch soundfile\n\n# Set up Japanese language support\nENV PYTHONIOENCODING=utf-8 \\\n    LANG=C.UTF-8 \\\n    LC_ALL=C.UTF-8\n\n# The project is now built and ready for testing Japanese language functionality\nCMD [\"python\", \"-c\", \"print('Kokoro is ready for use with Japanese language support')\"]", "language": "javascript"}
{"number": 330, "title": "Dark theme is not very readable in some places", "created_at": "2025-02-12T14:07:50Z", "closed_at": "2025-02-14T17:49:42Z", "commit_id": "e83a591acd0c9d2b8240fa8efa42069dec119543", "labels": [], "url": "https://github.com/clusterzx/paperless-ai/issues/330", "body": "**Describe the bug**\nThe dark theme seems to be forgotten in some places\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Switch to dark theme\n2. browse the page\n\nI believe it does not need any additional information.\n\nOtherwise thank you for the nice tool! <3", "comments_url": "https://api.github.com/repos/clusterzx/paperless-ai/issues/330/comments", "author": "woozar", "comments": [{"user": "clusterzx", "created_at": "2025-02-12T15:04:01Z", "body": "I just stumbled over the dashboard in the Task Runner. Is there anything else you have seen?"}, {"user": "bat1939", "created_at": "2025-02-12T16:48:26Z", "body": "Under Settings and AI Function Limits, the titles for the check boxes are in black and hard to read."}, {"user": "Analog4Lyfe", "created_at": "2025-02-12T18:03:47Z", "body": "in dark mode that white box background is very hard to read"}, {"user": "woozar", "created_at": "2025-02-12T20:35:10Z", "body": "Task runner status and black text in the settings are the two things, that draw my attention in the first place.\n\nAlso I just realised, that the theme trigger (that button in the top right corner) on the \"Manual\" page is not working.\n\nI also found some \"minor\" stuff (I would not have created a ticket for that).\n* would be awesome if the box with the text \"The application is already configured. You can update the configuration below.\" was also darker in dark mode. \n* the background of the pie chart in \"Document Type Distribution\"\n* paperless itself has a dark mode for its pdf preview tiles. is it somehow possible to use that in the Playground in paperless-ai in dark mode? (that is probably more of a feature request)"}, {"user": "clusterzx", "created_at": "2025-02-12T20:56:54Z", "body": "> Task runner status and black text in the settings are the two things, that draw my attention in the first place.\n> \n> Also I just realised, that the theme trigger (that button in the top right corner) on the \"Manual\" page is not working.\n> \n> I also found some \"minor\" stuff (I would not have created a ticket for that).\n> \n> * would be awesome if the box with the text \"The application is already configured. You can update the configuration below.\" was also darker in dark mode.\n> * the background of the pie chart in \"Document Type Distribution\"\n> * paperless itself has a dark mode for its pdf preview tiles. is it somehow possible to use that in the Playground in paperless-ai in dark mode? (that is probably more of a feature request)\n\nThanks for the specific information. Funny how I never realized that the switch on the manual page is not working. 😆 "}, {"user": "woozar", "created_at": "2025-02-12T23:27:14Z", "body": "A pleasure to work with people, who react on bug reports. Happy to help with my reports."}, {"user": "clusterzx", "created_at": "2025-02-14T17:49:41Z", "body": "Fixed the issues + also added the invert of documents in playground for better visibility. \nWill be available with the next release."}], "satisfaction_conditions": ["Fix readability issues in dark theme across the application", "Fix the theme toggle button functionality on the Manual page", "Improve dark mode consistency across UI elements", "Acknowledgment and communication about the reported issues", "Implementation of fixes in an upcoming release"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:26"}, "dockerfile": null, "language": "javascript"}
{"number": 316, "title": "OpenAI API Key is not valid. Please check the key.", "created_at": "2025-02-10T20:59:29Z", "closed_at": "2025-02-10T22:01:15Z", "commit_id": "86c531da4d9c16e178afaaea898c3d6c9462716e", "labels": [], "url": "https://github.com/clusterzx/paperless-ai/issues/316", "body": "Hey Clusterzx,\n I'm just about to set up paperless-ai, now it only fails because of the OpenAI key \"OpenAI API Key is not valid. Please check the key.\" The key is 100% correct... Do you have any idea what else I'm doing wrong?", "comments_url": "https://api.github.com/repos/clusterzx/paperless-ai/issues/316/comments", "author": "UncleCCC", "comments": [{"user": "clusterzx", "created_at": "2025-02-10T21:08:44Z", "body": "Do you have positive balance on this key? Free-Tier does not work."}, {"user": "UncleCCC", "created_at": "2025-02-10T21:23:15Z", "body": "Ohhh sorry... That was the mistake, I have little experience with OpenAI... I have a ChatGPT license, I thought this also applies to OpenAI. Now it works thanks for your quick help"}], "satisfaction_conditions": ["Identification of the root cause for the OpenAI API key validation error", "Clarification about OpenAI API access requirements", "A prompt response that addresses the specific error message"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:32"}, "dockerfile": null, "language": "javascript"}
{"number": 309, "title": "Rescan after Prompt Description change.", "created_at": "2025-02-09T17:00:39Z", "closed_at": "2025-02-09T19:39:54Z", "commit_id": "964c1bceefaf54502b606944b0dcdf5b4735eb15", "labels": [], "url": "https://github.com/clusterzx/paperless-ai/issues/309", "body": "Hi,\n\nThanks for this great tool.\n\nI have one question regarding understanding.\n\nToday I set up paperless-ai and successfully scanned over 400 documents. Tags and types are working fine, and all documents have an \"ai-processed\" tag.\n\nNow I decided to change many settings in the \"Prompt Description\" to optimize the output. I thought I could simply rescan everything and that the optimized prompts would be applied to all documents, but it's not working.\n\nHow can I rescan all documents with the new Prompt Description?\n\nThanks a lot.\n", "comments_url": "https://api.github.com/repos/clusterzx/paperless-ai/issues/309/comments", "author": "kolossboss", "comments": [{"user": "clusterzx", "created_at": "2025-02-09T17:03:10Z", "body": "You could easily go to History and delete the documents you want to reprocess. "}, {"user": "kolossboss", "created_at": "2025-02-09T19:22:58Z", "body": "Thx a lot.\n\nWorks great👍"}], "satisfaction_conditions": ["A method to reprocess documents with updated prompt descriptions", "A solution that doesn't require re-uploading the original documents", "A straightforward process that can be applied to multiple documents"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:36"}, "dockerfile": null, "language": "javascript"}
{"number": 297, "title": "Failed to get own user ID.  Abort scanning.", "created_at": "2025-02-07T19:10:11Z", "closed_at": "2025-02-07T19:21:06Z", "commit_id": "35c0f7fed119c39adaf3b09e4eb39b07593fe985", "labels": [], "url": "https://github.com/clusterzx/paperless-ai/issues/297", "body": "\nI am setting up Paperless-AI for the first time, and after configuration I get \"Failed to get own user ID.  Abort scanning.\"\n\nWhat does that mean and how do I fix it?\n", "comments_url": "https://api.github.com/repos/clusterzx/paperless-ai/issues/297/comments", "author": "Tarpon907", "comments": [{"user": "clusterzx", "created_at": "2025-02-07T19:14:59Z", "body": "You have to set there the login username of the user that is also the owner of the api key. "}, {"user": "Tarpon907", "created_at": "2025-02-07T19:15:49Z", "body": "It is.\n"}, {"user": "clusterzx", "created_at": "2025-02-07T19:17:19Z", "body": "Does the user have the rights to access the api and also the /api/users endpoint ?"}, {"user": "Tarpon907", "created_at": "2025-02-07T19:19:53Z", "body": "My API URL had a trailing slash.  It worked when I removed that.\n"}, {"user": "clusterzx", "created_at": "2025-02-07T19:20:59Z", "body": "Ok glad it works now. "}], "satisfaction_conditions": ["Identification of the root cause of the 'Failed to get own user ID. Abort scanning' error", "A solution that allows Paperless-AI to successfully connect to the API", "Guidance on proper API configuration for Paperless-AI"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:41"}, "dockerfile": null, "language": "javascript"}
{"number": 194, "title": "Chat feature not working with Custom AI agents", "created_at": "2025-01-20T18:51:42Z", "closed_at": "2025-01-20T19:30:20Z", "commit_id": "808d3a373a7b889be959fc29c2f14368c80eb051", "labels": ["bug"], "url": "https://github.com/clusterzx/paperless-ai/issues/194", "body": "**Describe the bug**\nChat not working with \"Custom\" agents.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Configure a Custom provider, in my case local llama.cpp.\n2. Process a document.\n3. Open a chat for it, from any access.\n\n**Expected behavior**\nChat to start using the custom provider.\n\n\n**Desktop (please complete the following information):**\n - OS: Any\n - Browser: Any\n - Version: 2.30\n\n**Additional context**\nI can see in the logs that the chat feature seems to expect OpenAI:\npaperless-ai           | [ERRO] initializing chat: Error: OpenAI client not initialized\npaperless-ai           |     at ChatService.initializeChat (/app/services/chatService.js:64:15)\n", "comments_url": "https://api.github.com/repos/clusterzx/paperless-ai/issues/194/comments", "author": "chwoa", "comments": [{"user": "clusterzx", "created_at": "2025-01-20T19:25:16Z", "body": "You are right! Forgot to implement it there. Pushing an update today!\nThank you very much for reporting."}, {"user": "clusterzx", "created_at": "2025-01-20T19:30:20Z", "body": "Fixed :)"}, {"user": "chwoa", "created_at": "2025-01-20T19:57:23Z", "body": "That was quick! Confirmed it is working in 2.3.1. Thank you very much!"}, {"user": "clusterzx", "created_at": "2025-01-20T20:04:05Z", "body": "You are very welcome 👍 "}], "satisfaction_conditions": ["Enable chat functionality to work with Custom AI agents/providers", "Remove dependency on OpenAI for chat functionality", "Provide a timely fix for the reported issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:46"}, "dockerfile": null, "language": "javascript"}
{"number": 156, "title": "Login failing", "created_at": "2025-01-14T21:10:53Z", "closed_at": "2025-01-14T21:41:57Z", "commit_id": "89a4ca1b7e216d422ba1903fd14b0f6799996e43", "labels": [], "url": "https://github.com/clusterzx/paperless-ai/issues/156", "body": "**Describe the bug**\r\nAfter finalizing the setup I am not able to log back into the dashboard.\r\n\r\n**To Reproduce**\r\n- finalize setup with user name and password\r\n- save settings\r\n- once restarted, log in through the UI\r\n\r\nlog shows:\r\nLogin attempt for user: PaperlessAI\r\nPassword validation result: false\r\n\r\nI have tried removing the .env (and config) files.\r\nremoved the whole container\r\n\r\nPlease let me know if there is more information I can provide.\r\n", "comments_url": "https://api.github.com/repos/clusterzx/paperless-ai/issues/156/comments", "author": "CreekDuzz", "comments": [{"user": "clusterzx", "created_at": "2025-01-14T21:37:11Z", "body": "You could look into the database what is stored in the \"users\" table. But the password is bcrypt encrypted. \n\nYou can go to any website you want where you can generate a bcrypt hash and paste it the generated hash over the Old one. \n\nBut normally there is no bug or issue known regarding your description. "}, {"user": "CreekDuzz", "created_at": "2025-01-14T21:41:57Z", "body": "That was it. I did not think about the DB containing the login info. I removed the old DB and once the new were created, its working. Thank you!"}, {"user": "clusterzx", "created_at": "2025-01-14T21:45:47Z", "body": "Maybe you entered only some false login credentials thinking of a different password or capslock. I don't know 😅\n\nBut there is no bug currently known, the login page is present since some versions now. If there was then there would be more people with the same issue. \n\nSo who knows what it was, luckily it works now! Have a great day ❤️"}], "satisfaction_conditions": ["A solution that resolves the login failure issue", "Information about where login credentials are stored", "A method to reset or recreate authentication data"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:51"}, "dockerfile": null, "language": "javascript"}
{"number": 1, "title": "希望增加文字长度上限", "created_at": "2024-12-10T09:29:12Z", "closed_at": "2024-12-11T04:55:56Z", "commit_id": "483a2e9d7e38bec610e9f6a9f3026241fdfddc14", "labels": ["enhancement"], "url": "https://github.com/bestZwei/ciallo-tts/issues/1", "body": "目前有字符限制，大概看了一下您的代码\r\n发现请求api使用的是get方法，长度过长会报error\r\n期待您的优化\r\n", "comments_url": "https://api.github.com/repos/bestZwei/ciallo-tts/issues/1/comments", "author": "uniqueww", "comments": [{"user": "bestZwei", "created_at": "2024-12-10T13:48:34Z", "body": "问题是，api 返回的音频最长10分钟"}, {"user": "bestZwei", "created_at": "2024-12-10T17:38:49Z", "body": "你试试，做了个智能分段，2500中文字符，或者5000其他字符，分成一段。长文本将自动切分，优先根据分段-句号-逗号切分"}, {"user": "uniqueww", "created_at": "2024-12-11T00:37:08Z", "body": "> 你试试，做了个智能分段，2500中文字符，或者5000其他字符，分成一段。长文本将自动切分，优先根据分段-句号-逗号切分\r\n\r\n好的，原来是api的返回限制了字符，我尝试优化一下，感谢你的回复\r\n"}], "satisfaction_conditions": ["A solution that handles longer text input beyond the current character limit", "An approach that works within the API's limitations", "Automatic text segmentation functionality"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:01"}, "dockerfile": null, "language": "javascript"}
{"number": 1, "title": "Command chaining operators are not allowed (;, &, |, `)", "created_at": "2024-12-17T16:51:36Z", "closed_at": "2024-12-17T23:54:14Z", "commit_id": "dc7ecccd2945cf9074a11d455bd1ffbfd1e42685", "labels": [], "url": "https://github.com/SimonB97/win-cli-mcp-server/issues/1", "body": "Hi, came across this issue:\r\n\r\n{\r\n  `shell`: `powershell`,\r\n  `command`: `[some powershell with a query paramater]' 'AzureDiagnostics | take 2'`\r\n}\r\n\r\nSo when passing parameters like a kudo query in this case should allow the pipe character\r\n\r\nI know i can disable that check globally, but that's risky.\r\n\r\n", "comments_url": "https://api.github.com/repos/SimonB97/win-cli-mcp-server/issues/1/comments", "author": "BartNetJS", "comments": [{"user": "SimonB97", "created_at": "2024-12-17T23:54:14Z", "body": "I have added a shell-specific `blockedOperators` setting to the config in version `0.1.8`. You can pass a list of operators to be blocked (if `enableInjectionProtection` is set to `true`):\r\n\r\n```json\r\n{\r\n  \"security\": {\r\n    \"enableInjectionProtection\": true\r\n  }\r\n  \"cmd\": {\r\n        \"enabled\": true,\r\n        \"command\": \"cmd.exe\",\r\n        \"args\": [\"/c\"],\r\n        \"blockedOperators\": [\"&\", \"|\", \";\", \"`\"]\r\n  }\r\n}\r\n```\r\n\r\nLet me know if this doesn't solve the issue, otherwise I'll consider this solved and close.\r\n\r\nP.S.: Thanks for being my first issue! 🥇 "}, {"user": "BartNetJS", "created_at": "2024-12-19T08:11:09Z", "body": "Hi @SimonB97, thanks for the quick fix"}], "satisfaction_conditions": ["A way to selectively allow specific command chaining operators for particular shell configurations", "A security-conscious solution that doesn't compromise overall injection protection", "A configuration-based approach that doesn't require code changes"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:15"}, "dockerfile": null, "language": "javascript"}
{"number": 1, "title": "Error: unprocessable entity", "created_at": "2024-12-17T10:16:44Z", "closed_at": "2024-12-17T15:18:13Z", "commit_id": "6e0f3d66b904fcb069f625feef45b0c893b5ce0c", "labels": [], "url": "https://github.com/public-transport/db-vendo-client/issues/1", "body": "The int.bahn.de server seems to respond with an unprocessable entity when using `journeys`.\r\nJust running /p/db/example.js with \r\n\r\n```\r\n let data = await client.journeys(berlinJungfernheide, münchenHbf, {\r\n        results: 1,\r\n        tickets: true,\r\n })\r\n```\r\n\r\nerrors out at\r\n\r\n```\r\n\t\tconst err = new Error(res.statusText);\r\n\t\t            ^\r\n\r\nError: Unprocessable Entity\r\n    at Object.request (file:///home/tgrossen/Projekte/Technical/db-vendo-client/lib/request.js:142:15)\r\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\r\n    at async Object.journeys (file:///home/tgrossen/Projekte/Technical/db-vendo-client/index.js:213:25)\r\n    at async file:///home/tgrossen/Projekte/Technical/db-vendo-client/p/db/example.js:38:13\r\n```\r\n\r\n", "comments_url": "https://api.github.com/repos/public-transport/db-vendo-client/issues/1/comments", "author": "grssnbchr", "comments": [{"user": "traines-source", "created_at": "2024-12-17T15:01:10Z", "body": "Sorry, I miscommitted something yesterday :( Should be fixed now. Thanks for reporting!"}, {"user": "grssnbchr", "created_at": "2024-12-17T15:18:13Z", "body": "No worries - can confirm it works now. Thank you."}], "satisfaction_conditions": ["A fix for the 'unprocessable entity' error when using the journeys function", "Ability to successfully retrieve journey data from the int.bahn.de server", "Proper functionality of the example code provided in the repository"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 00:59:22"}, "dockerfile": "FROM node:18-alpine\nWORKDIR /app\n\n# Install git for cloning the repository\nRUN apk add --update git\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/public-transport/db-vendo-client.git . && \\\n    git checkout 6e0f3d66b904fcb069f625feef45b0c893b5ce0c\n\n# Install dependencies (using npm install instead of npm ci)\nRUN npm install\n\n# Set the default command to keep the container running\nCMD [\"tail\", \"-f\", \"/dev/null\"]", "language": "javascript"}
{"number": 2, "title": "results option is being ignored by journeys function", "created_at": "2024-12-17T15:23:53Z", "closed_at": "2024-12-17T20:00:21Z", "commit_id": "73d9c88ffb31b5b05ee6031013e404e8e8f07c46", "labels": [], "url": "https://github.com/public-transport/db-vendo-client/issues/2", "body": "## expected:\r\n\r\nwhen calling the `journeys` function with the `results: 1` option, I expected the max number of results to be 1.\r\n\r\n## actual:\r\n\r\n```\r\n let data = await client.journeys(berlinJungfernheide, münchenHbf, {\r\n        results: 1,\r\n        tickets: true,\r\n })\r\n```\r\n\r\nreturns more than one journey:\r\n\r\n```\r\n{\r\n  earlierRef: '3|OB|MTµ14µ541001µ541001µ541272µ541272µ0µ0µ485µ540980µ1µ0µ26µ0µ0µ-2147483648µ1µ2|PDHµ5cad74a0d15ed317fb4ba0dde7ed8b36|RDµ17122024|RTµ162300|USµ0|RSµINIT',\r\n  laterRef: '3|OF|MTµ14µ541180µ541180µ541441µ541441µ0µ0µ485µ541122µ5µ0µ26µ0µ0µ-2147483648µ1µ2|PDHµ5cad74a0d15ed317fb4ba0dde7ed8b36|RDµ17122024|RTµ162300|USµ0|RSµINIT',\r\n  journeys: [\r\n    [Object], [Object],\r\n    [Object], [Object],\r\n    [Object], [Object],\r\n    [Object]\r\n  ],\r\n  realtimeDataUpdatedAt: null\r\n}\r\n```", "comments_url": "https://api.github.com/repos/public-transport/db-vendo-client/issues/2/comments", "author": "grssnbchr", "comments": [{"user": "traines-source", "created_at": "2024-12-17T20:28:03Z", "body": "Yes, the new backend API does unfortunately not allow specifying the number of desired results. There are quite a few parameters like that that are not known to exist in the new API or I that was too lazy to implement so far (see `TODO`s scattered across the code).\r\nThe journey list is now truncated if `results` is set, however, you will not be able to increase the number of returned results (use `laterThan` with additional requests for that)."}, {"user": "grssnbchr", "created_at": "2024-12-18T08:06:23Z", "body": "Got it, thanks! "}], "satisfaction_conditions": ["An explanation of why the 'results' parameter doesn't limit results as expected", "Information about current behavior of the 'results' parameter", "Alternative approaches to achieve the desired functionality"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:17"}, "dockerfile": null, "language": "javascript"}
{"number": 32, "title": "steam与绑定的bot强关联", "created_at": "2024-12-24T09:42:44Z", "closed_at": "2024-12-24T13:33:30Z", "commit_id": "f018be25be2b35b50f4c4c5a326ced519e905744", "labels": [], "url": "https://github.com/XasYer/steam-plugin/issues/32", "body": "大佬，目前发现一个问题，如果是在1号QQ绑定的steam，因为某些不可抗性的原因1号QQ封号了，yunzai更改另外一个QQ登录的时候，steam不会再进行播报，查了一下插件下的data目录里的db文件，发现绑定信息似乎和BOT的qq号绑定，从而导致新的QQ没办法推送状态", "comments_url": "https://api.github.com/repos/XasYer/steam-plugin/issues/32/comments", "author": "chz091", "comments": [{"user": "XasYer", "created_at": "2024-12-24T10:05:43Z", "body": "如果是TRSS, 更新后发`#steam设置随机Bot开启`, 如果新Bot和旧Bot在同一个群则会继续推送, 如果是Miao自行解决"}, {"user": "chz091", "created_at": "2024-12-24T13:27:20Z", "body": "> 如果是TRSS, 更新后发`#steam设置随机Bot开启`, 如果新Bot和旧Bot在同一个群则会继续推送, 如果是Miao自行解决\r\n\r\n感谢佬"}], "satisfaction_conditions": ["A solution that allows Steam notifications to continue working when changing to a new QQ bot account", "Clear instructions specific to their bot platform (TRSS or Miao)", "A simple command-based solution that doesn't require complex technical intervention"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:01"}, "dockerfile": null, "language": "javascript"}
{"number": 1, "title": "Cannot remove plates from Known Plates Dashboard", "created_at": "2024-11-18T15:07:37Z", "closed_at": "2024-11-18T18:28:00Z", "commit_id": "95ee6d78b3c5f4466defc24c9212c6596125261b", "labels": [], "url": "https://github.com/algertc/ALPR-Database/issues/1", "body": "Running on Linux Docker with the latest repo changes.\r\n\r\nTested in both Chrome and Safari.\r\n\r\nClicking on the delete button within the table of known plates doesn't delete the plate.\r\n\r\nConsole Log:\r\n\r\n```Failed to remove from known plates: ReferenceError: removeFromKnownPlates is not defined```", "comments_url": "https://api.github.com/repos/algertc/ALPR-Database/issues/1/comments", "author": "TinyShark", "comments": [{"user": "algertc", "created_at": "2024-11-18T18:28:00Z", "body": "Thank you. Fix pushed. \r\n\r\n`docker compose down`, then `docker compose up -d` should fix. If not, `docker pull algertc/alpr-dashboard` and that should pull the latest version."}, {"user": "TinyShark", "created_at": "2024-11-19T00:11:27Z", "body": "pulling the new image got it to work. Known plates are being removed correctly now.\r\n\r\nMany thanks!"}], "satisfaction_conditions": ["A solution that fixes the 'removeFromKnownPlates is not defined' error", "Clear instructions on how to apply the fix to their Docker environment", "Restoration of the ability to remove plates from the Known Plates Dashboard"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:01:39"}, "dockerfile": "FROM node:18-bullseye AS builder\n\n# Set working directory\nWORKDIR /app\n\n# Clone the repository and checkout specific commit\nRUN apt-get update && apt-get install -y git && \\\n    git clone https://github.com/algertc/ALPR-Database.git . && \\\n    git checkout 95ee6d78b3c5f4466defc24c9212c6596125261b\n\n# Copy package files and install dependencies\n# Force the install to proceed despite errors\nRUN npm install --legacy-peer-deps\n\n# Build the application\nRUN npm run build\n\n# Create the production image\nFROM node:18-bullseye\n\n# Set working directory\nWORKDIR /app\n\n# Copy built assets and dependencies from builder stage\nCOPY --from=builder /app/.next ./.next\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/public ./public\nCOPY --from=builder /app/package.json ./package.json\nCOPY --from=builder /app/next.config.js ./next.config.js\n\n# Create config and auth directories to persist data\nRUN mkdir -p /app/config /app/auth\nVOLUME [\"/app/config\", \"/app/auth\"]\n\n# Copy specific files needed for runtime\nCOPY --from=builder /app/schema.sql ./schema.sql\nCOPY --from=builder /app/lib ./lib\nCOPY --from=builder /app/middleware.js ./middleware.js\nCOPY --from=builder /app/app ./app\nCOPY --from=builder /app/components ./components\nCOPY --from=builder /app/hooks ./hooks\n\n# Expose the application port\nEXPOSE 3000\n\n# Set environment variables\nENV NODE_ENV=production\n\n# Command to run the application\nCMD [\"npm\", \"start\"]", "language": "javascript"}
{"number": 12, "title": "[BUG] No valid Proxmox VE nodes configured. Please check your environment variables", "created_at": "2025-03-12T23:31:38Z", "closed_at": "2025-03-13T13:24:21Z", "commit_id": "33d3168353714fb9a5432f13502f83a976deeb12", "labels": ["bug"], "url": "https://github.com/rcourtman/Pulse/issues/12", "body": "The error happens once updated to 1.6.0", "comments_url": "https://api.github.com/repos/rcourtman/Pulse/issues/12/comments", "author": "walterzilla", "comments": [{"user": "Tukamok", "created_at": "2025-03-12T23:48:55Z", "body": "This appears to be because the variable names have changed.\n\nPROXMOX_HOST=\nPROXMOX_NODE=\nPROXMOX_TOKEN_ID=\nPROXMOX_TOKEN_SECRET=\n\n...I'm sure it was because I'm a problem child.  :)\n\n\n\n....or pull 1.6.1, appears this has been reverted there."}, {"user": "rcourtman", "created_at": "2025-03-12T23:51:22Z", "body": "This was a compatibility issue in v1.6.0 - I changed the environment variable format without providing backward compatibility.\n\nI've just released v1.6.1 which fixes this issue by reverting to the original format. Your existing configuration will work again without any changes.\n\nThis was my fault, it's late and I'm going to bed!  \n\n**edit - please let me know if 1.6.1 solves it for you. "}, {"user": "walterzilla", "created_at": "2025-03-13T10:28:53Z", "body": "> This appears to be because the variable names have changed.\n\nDidn't notice!\n\n> let me know if 1.6.1 solves it for you.\n\nAffirmative!\n"}], "satisfaction_conditions": ["A solution that restores compatibility with the user's existing environment variable configuration", "A fix that resolves the 'No valid Proxmox VE nodes configured' error", "A solution that doesn't require the user to change their environment variables"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:20"}, "dockerfile": null, "language": "javascript"}
{"number": 8, "title": "Non-desired tags when plugin is installled on several PC with synced database", "created_at": "2024-12-13T09:32:50Z", "closed_at": "2024-12-18T15:50:11Z", "commit_id": "9fe22a6c8d701bee9a68a4389a655d075fb5bcb9", "labels": ["enhancement"], "url": "https://github.com/SciImage/zotero-attachment-scanner/issues/8", "body": "Hi\r\nI use Zotero on several Pcs, on which I installed attachment-scanner. My database in synchronized.\r\nIt seems that a scan has been operated quickly after the plugin installation (I installed the plugin at the very opening of application)  : non-desired tags  (all three categories and with default simpole format) appeared since I had scanned previously on another PC with non-default options. I guess this scan is triggered by the monitoring option which is set as on by default. If so and if I'm not wrong perhaps would it be more secure to set it as off by default ?\r\nThis is not a great problem since tags can be easily removed.\r\nBest regards\r\nYves", "comments_url": "https://api.github.com/repos/SciImage/zotero-attachment-scanner/issues/8/comments", "author": "ynedelec3", "comments": [{"user": "SciImage", "created_at": "2024-12-18T15:50:11Z", "body": "The default is changed to off in v0.3.0. Thanks!"}, {"user": "ynedelec3", "created_at": "2024-12-18T18:33:26Z", "body": "Great, thanks "}], "satisfaction_conditions": ["Changing the default monitoring setting from 'on' to 'off' in the plugin", "Preventing unexpected tag creation when the plugin is installed on multiple synced devices", "Acknowledgment of the user's feedback about default settings"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:57"}, "dockerfile": null, "language": "javascript"}
{"number": 2, "title": "Plugin can't be stopped", "created_at": "2024-12-06T18:05:02Z", "closed_at": "2024-12-11T03:57:44Z", "commit_id": "df66fd0a25445cb989c5397fd03eb26f133359f7", "labels": ["enhancement"], "url": "https://github.com/SciImage/zotero-attachment-scanner/issues/2", "body": "There's no option or command to cancel the scan, which is useful when scanning too many records.", "comments_url": "https://api.github.com/repos/SciImage/zotero-attachment-scanner/issues/2/comments", "author": "gvlx", "comments": [{"user": "SciImage", "created_at": "2024-12-11T03:57:44Z", "body": "v 0.2.0, as you wished\r\n- A \"Cancel Attachment Scanning\" menu item is now available in the “Tools” menu."}, {"user": "gvlx", "created_at": "2024-12-19T16:33:30Z", "body": "Works as designed on 0.3.0"}], "satisfaction_conditions": ["A way to cancel an ongoing scan operation", "An accessible interface element to trigger the cancellation"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:03"}, "dockerfile": null, "language": "javascript"}
{"number": 15, "title": "CF有多个组织时部署报错", "created_at": "2024-12-23T05:18:30Z", "closed_at": "2024-12-23T11:34:51Z", "commit_id": "1833a3c855b914fa47fbc9286b309267179be771", "labels": [], "url": "https://github.com/ling-drag0n/CloudPaste-old/issues/15", "body": "我的Cloudflare账户链接了多个组织，在自动部署该项目时，发生了错误无法完成部署，错误信息如下：\r\n```\r\n✘ [ERROR] More than one account available but unable to select one in non-interactive mode.\r\n  Please set the appropriate `account_id` in your Wrangler configuration file.\r\n  Available accounts are (`<name>`: `<account_id>`):\r\n```\r\n虽然设置了CF_ACCOUNT_ID的环境变量，但仍然提示无法指定账户，麻烦作者排查一下，兼容这种情况", "comments_url": "https://api.github.com/repos/ling-drag0n/CloudPaste-old/issues/15/comments", "author": "DreamFerry", "comments": [{"user": "ling-drag0n", "created_at": "2024-12-23T08:58:57Z", "body": "你试试修改一下deploy.yml文件最后的name: Deploy to Cloudflare Workers的env：\r\n```yml\r\n      - name: Deploy to Cloudflare Workers\r\n        uses: cloudflare/wrangler-action@2.0.0\r\n        with:\r\n          apiToken: ${{ secrets.CF_API_TOKEN }}\r\n          command: deploy --var ADMIN_USERNAME:${{ secrets.ADMIN_USERNAME }} --var ADMIN_PASSWORD:${{ secrets.ADMIN_PASSWORD }}\r\n        env:\r\n          CLOUDFLARE_API_TOKEN: ${{ secrets.CF_API_TOKEN }}\r\n          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}\r\n          ADMIN_USERNAME: ${{ secrets.ADMIN_USERNAME }}\r\n          ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD }}\r\n```"}, {"user": "DreamFerry", "created_at": "2024-12-23T11:28:23Z", "body": "> 你试试修改一下deploy.yml文件最后的name: Deploy to Cloudflare Workers的env：\r\n> \r\n> ```yaml\r\n>       - name: Deploy to Cloudflare Workers\r\n>         uses: cloudflare/wrangler-action@2.0.0\r\n>         with:\r\n>           apiToken: ${{ secrets.CF_API_TOKEN }}\r\n>           command: deploy --var ADMIN_USERNAME:${{ secrets.ADMIN_USERNAME }} --var ADMIN_PASSWORD:${{ secrets.ADMIN_PASSWORD }}\r\n>         env:\r\n>           CLOUDFLARE_API_TOKEN: ${{ secrets.CF_API_TOKEN }}\r\n>           CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}\r\n>           ADMIN_USERNAME: ${{ secrets.ADMIN_USERNAME }}\r\n>           ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD }}\r\n> ```\r\n\r\n这样修改之后就可以了"}], "satisfaction_conditions": ["A solution that allows deployment to work with multiple Cloudflare organizations", "A way to properly specify the Cloudflare account ID during automated deployment", "Correct configuration of environment variables in the GitHub Actions workflow file"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:09"}, "dockerfile": null, "language": "javascript"}
{"number": 64, "title": "Reloading application.css when constituent files are changed (Sprockets)", "created_at": "2025-01-06T19:22:24Z", "closed_at": "2025-01-25T08:49:13Z", "commit_id": "00e840ecab40245d1ff790963e42fb823ee0fddb", "labels": [], "url": "https://github.com/hotwired/spark/issues/64", "body": "Thanks for making this! :star_struck: We're still running sprockets, so it was nice to see #41 adding basic sprockets support – thanks for that @codergeek121!\r\n\r\nI was still struggling to make it work, and have had to make some modifications to get to a walking skeleton of CSS reloading:\r\n\r\n1. Add some subfolders of `app/assets/stylesheets` to `css_paths`\r\n1. Add `scss` to `css_extensions`\r\n1. Monkey patch `Hotwire::Spark::Change#canonical_changed_path` to always return \"/application.css\" when `action == :reload_css`.\r\n\r\nThe first two points was necessary for any events to get fired in the first place, as my CSS is mainly defined in scss files in subfolders of `app/assets/stylesheets`, imported in `app/assets/stylesheets/application.scss`. Part 3 was to get `application.css` reloaded, in stead of the constituent file that was actually modified.\r\n\r\nIs there something I've misunderstood here? If not, perhaps some additions could be made to streamline this a bit :thinking: ", "comments_url": "https://api.github.com/repos/hotwired/spark/issues/64/comments", "author": "rogerkk", "comments": [{"user": "codergeek121", "created_at": "2025-01-07T21:37:20Z", "body": "I don't think you misunderstood! Currently, there's only basic support for Sprockets, meaning it simply doesn't raise an error if you're using Sprockets. There's no support for sass/scss/coffeescript right now. I think this would also be kind of hard to add in a non-buggy way, without parsing sass imports.\r\n\r\nIf you don't want to monkey patch, you could also try the following instead:\r\n\r\n1. Change the reload method to `:replace`, since this will also reload the `<head>` if there are changes\r\n2. Add the `scss` extension and paths to the **html_paths** and **html_extensions**, which will then trigger an `:replace` reload if a `scss` file is changed\r\n\r\nThis will not do a fine-grained css reload, but a full Turbo visit instead, but maybe this is good enough for your use case."}, {"user": "rogerkk", "created_at": "2025-01-08T14:16:16Z", "body": "Ah, nice to know about an alternative approach. The monkey patching is working at the moment, but now I have somewhere to go if/when it causes me too much pain. :sweat_smile:  Thanks again!"}, {"user": "jorgemanrubia", "created_at": "2025-01-25T08:49:13Z", "body": "Thanks for the help here @codergeek121 "}], "satisfaction_conditions": ["A solution for reloading application.css when constituent SCSS files in subfolders are changed", "An explanation of the current limitations of Sprockets support in the library", "A workable alternative to monkey patching for handling SCSS file changes", "Clear technical guidance that acknowledges the user's current approach while offering alternatives"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:09"}, "dockerfile": null, "language": "javascript"}
{"number": 15, "title": "undefined method `hotwire' for #<Rails::Application::Configuration:0x00000000007648> (NoMethodError)", "created_at": "2024-12-18T18:50:23Z", "closed_at": "2024-12-18T20:22:18Z", "commit_id": "c8ce327654dc370ce8c217d984e59d6614bad1c0", "labels": [], "url": "https://github.com/hotwired/spark/issues/15", "body": "Rails 7.1.3.4\r\n\r\nAdded the gem to the development group and ran `bundle install`\r\n\r\nUpdated `development.rb`\r\n\r\n```ruby\r\nconfig.hotwire.spark.html_paths += ['app/components']\r\nconfig.hotwire.spark.stimulus_paths += ['app/components']\r\n```\r\n\r\nReceived error:\r\n\r\n```gems/railties-7.1.3.4/lib/rails/railtie/configuration.rb:109:in `method_missing': undefined method `hotwire' for #<Rails::Application::Configuration:0x00000000007648> (NoMethodError)```", "comments_url": "https://api.github.com/repos/hotwired/spark/issues/15/comments", "author": "t2", "comments": [{"user": "robzolkos", "created_at": "2024-12-18T19:54:42Z", "body": "I think you need Rails 8+ for this."}, {"user": "t2", "created_at": "2024-12-18T20:22:08Z", "body": "Thank you!"}], "satisfaction_conditions": ["Information about version compatibility for the hotwire configuration", "Explanation for why the NoMethodError is occurring"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:14"}, "dockerfile": null, "language": "javascript"}
{"number": 64, "title": "hello, seem the url is broken", "created_at": "2025-04-07T07:29:10Z", "closed_at": "2025-04-09T00:31:28Z", "commit_id": "a70e1f2f921888724d64a9bfe06f1fa64c118a09", "labels": [], "url": "https://github.com/itcon-pty-au/stremio-ai-search/issues/64", "body": "thanks.", "comments_url": "https://api.github.com/repos/itcon-pty-au/stremio-ai-search/issues/64/comments", "author": "ericvlog", "comments": [{"user": "itcon-pty-au", "created_at": "2025-04-07T11:37:42Z", "body": "Seems like a DNS propogation issue affecting some regions. I have raised a ticket with my new domain provider. Started after I switched my domain provider on Sunday."}, {"user": "itcon-pty-au", "created_at": "2025-04-08T07:53:30Z", "body": "Is it working for you now?"}, {"user": "ericvlog", "created_at": "2025-04-08T08:50:59Z", "body": "> Is it working for you now?\n\nYup it workings now, maybe just URL down.\n👍"}], "satisfaction_conditions": ["Restoration of access to the previously broken URL", "Acknowledgment of the issue and its status"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:20"}, "dockerfile": null, "language": "javascript"}
{"number": 44, "title": "[Bug Report] RPDB API Key", "created_at": "2025-03-13T18:26:33Z", "closed_at": "2025-03-14T13:33:38Z", "commit_id": "e7b9abb37c78df235e728f5ff7bea336a7fb3a91", "labels": ["issue"], "url": "https://github.com/itcon-pty-au/stremio-ai-search/issues/44", "body": "## Bug Report\n\n**Device Type:** mac\n\n**Error Details:**\n```\nRPDB API Key\n```\n\n**Description:**\nI removed my RPDB API Key but it keeps showing that im using it\n\n---\n*Submitted via Stremio AI Search Addon*", "comments_url": "https://api.github.com/repos/itcon-pty-au/stremio-ai-search/issues/44/comments", "author": "itcon-pty-au", "comments": [{"user": "itcon-pty-au", "created_at": "2025-03-14T02:58:47Z", "body": "It uses default free RPDB API key if you don't provide one."}, {"user": "Djlilyazii", "created_at": "2025-03-14T13:30:56Z", "body": "> It uses default free RPDB API key if you don't provide one.\n\nthanks. close ticket. "}], "satisfaction_conditions": ["Explanation of why the RPDB API key appears to be in use even after removal", "Clarification about the default behavior of the system regarding API keys"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:25"}, "dockerfile": null, "language": "javascript"}
{"number": 131, "title": "WSL2 Ubuntu: cache_video failed, error: result type Float can't be cast to the desired output type Byte", "created_at": "2025-03-01T17:20:49Z", "closed_at": "2025-03-01T20:22:42Z", "commit_id": "a326079926a4a347ecda8863dc40ba2d7680a294", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/131", "body": "\n\n\npython generate.py  --task t2v-1.3B --size 480*832 --ckpt_dir ./Wan2.1-T2V-1.3B --prompt \"a metallic skeleton robot on a cooking show, preparing a recipe with a whole chicken\" --save_file ./output.mp4\n\n\n[2025-03-01 14:01:36,940] INFO: offload_model is not specified, set to True.\n[2025-03-01 14:01:36,940] INFO: Generation job args: Namespace(task='t2v-1.3B', size='480*832', frame_num=81, ckpt_dir='./Wan2.1-T2V-1.3B', offload_model=True, ulysses_size=1, ring_size=1, t5_fsdp=False, t5_cpu=False, dit_fsdp=False, save_file='./output.mp4', prompt='a metallic skeleton robot on a cooking show, preparing a recipe with a whole chicken', use_prompt_extend=False, prompt_extend_method='local_qwen', prompt_extend_model=None, prompt_extend_target_lang='ch', base_seed=4277550218863685172, image=None, sample_solver='unipc', sample_steps=50, sample_shift=5.0, sample_guide_scale=5.0)\n[2025-03-01 14:01:36,940] INFO: Generation model config: {'__name__': 'Config: Wan T2V 1.3B', 't5_model': 'umt5_xxl', 't5_dtype': torch.bfloat16, 'text_len': 512, 'param_dtype': torch.bfloat16, 'num_train_timesteps': 1000, 'sample_fps': 16, 'sample_neg_prompt': '色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质 量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走', 't5_checkpoint': 'models_t5_umt5-xxl-enc-bf16.pth', 't5_tokenizer': 'google/umt5-xxl', 'vae_checkpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n[2025-03-01 14:01:36,940] INFO: Input prompt: a metallic skeleton robot on a cooking show, preparing a recipe with a whole chicken\n[2025-03-01 14:01:36,940] INFO: Creating WanT2V pipeline.\n[2025-03-01 14:02:33,960] INFO: loading ./Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth\n[2025-03-01 14:03:39,030] INFO: loading ./Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\n[2025-03-01 14:03:41,640] INFO: Creating WanModel from ./Wan2.1-T2V-1.3B\n[2025-03-01 14:07:17,091] INFO: Generating video ...\n100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [08:27<00:00, 10.16s/it]\n[2025-03-01 14:16:14,586] INFO: Saving generated video to ./output.mp4\ncache_video failed, error: result type Float can't be cast to the desired output type Byte\n[2025-03-01 14:16:15,400] INFO: Finished.", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/131/comments", "author": "egaralmeida", "comments": [{"user": "egaralmeida", "created_at": "2025-03-01T20:22:42Z", "body": "Fixed by installing imageio-ffmpeg, which is in the requirements. Not sure why it didn't install for me along many other requirements."}, {"user": "garysdevil", "created_at": "2025-03-02T03:21:53Z", "body": "```log\nheckpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n[2025-03-02 10:33:59,629] INFO: Input prompt: Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\n[2025-03-02 10:33:59,629] INFO: Creating WanT2V pipeline.\n[2025-03-02 10:34:36,622] INFO: loading ./Wan2.1-T2V-1.3B\\models_t5_umt5-xxl-enc-bf16.pth\n[2025-03-02 10:34:41,096] INFO: loading ./Wan2.1-T2V-1.3B\\Wan2.1_VAE.pth\n[2025-03-02 10:34:41,508] INFO: Creating WanModel from ./Wan2.1-T2V-1.3B\n[2025-03-02 10:34:43,656] INFO: Generating video ...\n100%|████████████████████████████████████████████████████████████████████████████████████████| 50/50 [15:22<00:00, 18.45s/it]\n[2025-03-02 10:52:29,068] INFO: Saving generated video to 1.pm4\ncache_video failed, error: result type Float can't be cast to the desired output type Byte\n[2025-03-02 10:52:29,291] INFO: Finished.\n(wan2.1) PS D:\\Dev\\Wan2.1> pip install imageio-ffmpeg                                                                         \nRequirement already satisfied: imageio-ffmpeg in c:\\users\\gary\\.conda\\envs\\wan2.1\\lib\\site-packages (0.6.0)                   \n(wan2.1) PS D:\\Dev\\Wan2.1> \n```"}, {"user": "dieptran2500", "created_at": "2025-03-02T17:38:19Z", "body": "i have same problem , any one know how to fix?"}, {"user": "lxm065", "created_at": "2025-03-04T02:15:27Z", "body": "i have the same problem , and i install imageio-ffmpeg\n\nError opening output files: Invalid argument\n\n\n[2025-03-04 10:03:31,847] INFO: Saving generated video to t2v-1.3B_832*480_1_1_Two_anthropomorphic_cats_in_comfy_boxing_gear_and__20250304_100331.mp4\n[out#0/mp4 @ 00000128ae1f02c0] Error opening output D:\\ai\\Wan2.1\\t2v-1.3B_832*480_1_1_Two_anthropomorphic_cats_in_comfy_boxing_gear_and__20250304_100331.mp4: Invalid argument\nError opening output file D:\\ai\\Wan2.1\\t2v-1.3B_832*480_1_1_Two_anthropomorphic_cats_in_comfy_boxing_gear_and__20250304_100331.mp4.\nError opening output files: Invalid argument\ncache_video failed, error: result type Float can't be cast to the desired output type Byte\n[2025-03-04 10:03:32,273] INFO: Finished."}, {"user": "garysdevil", "created_at": "2025-03-13T14:03:56Z", "body": "> ```\n> heckpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n> [2025-03-02 10:33:59,629] INFO: Input prompt: Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\n> [2025-03-02 10:33:59,629] INFO: Creating WanT2V pipeline.\n> [2025-03-02 10:34:36,622] INFO: loading ./Wan2.1-T2V-1.3B\\models_t5_umt5-xxl-enc-bf16.pth\n> [2025-03-02 10:34:41,096] INFO: loading ./Wan2.1-T2V-1.3B\\Wan2.1_VAE.pth\n> [2025-03-02 10:34:41,508] INFO: Creating WanModel from ./Wan2.1-T2V-1.3B\n> [2025-03-02 10:34:43,656] INFO: Generating video ...\n> 100%|████████████████████████████████████████████████████████████████████████████████████████| 50/50 [15:22<00:00, 18.45s/it]\n> [2025-03-02 10:52:29,068] INFO: Saving generated video to 1.pm4\n> cache_video failed, error: result type Float can't be cast to the desired output type Byte\n> [2025-03-02 10:52:29,291] INFO: Finished.\n> (wan2.1) PS D:\\Dev\\Wan2.1> pip install imageio-ffmpeg                                                                         \n> Requirement already satisfied: imageio-ffmpeg in c:\\users\\gary\\.conda\\envs\\wan2.1\\lib\\site-packages (0.6.0)                   \n> (wan2.1) PS D:\\Dev\\Wan2.1> \n> ```\n\nI resolve this question by setting an absolute path `--save_file \"D:\\Dev\\Wan2.1\\2.1.mp4\" `"}], "satisfaction_conditions": ["A solution that resolves the 'cache_video failed, error: result type Float can't be cast to the desired output type Byte' error", "A way to successfully save the generated video output to a file", "A solution that addresses dependency or configuration issues in the video generation pipeline", "A workaround for file path handling issues"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:12:35"}, "dockerfile": "FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PYTHONUNBUFFERED=1\nENV PATH=\"/usr/local/cuda/bin:${PATH}\"\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    curl \\\n    python3-dev \\\n    python3-pip \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository at the specific commit\nRUN git clone https://github.com/Wan-Video/Wan2.1.git /app && \\\n    cd /app && \\\n    git checkout a326079926a4a347ecda8863dc40ba2d7680a294\n\n# Upgrade pip and install PyTorch 2.4.0 with CUDA support first\nRUN pip3 install --no-cache-dir --upgrade pip && \\\n    pip3 install --no-cache-dir torch>=2.4.0 torchvision>=0.17.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install project dependencies in batches to improve build reliability\nRUN pip3 install --no-cache-dir numpy scipy matplotlib && \\\n    pip3 install --no-cache-dir opencv-python pillow && \\\n    pip3 install --no-cache-dir tqdm transformers einops && \\\n    pip3 install --no-cache-dir huggingface_hub modelscope && \\\n    pip3 install --no-cache-dir -r requirements.txt\n\n# Create model and output directories\nRUN mkdir -p /models /output\n\n# Fix for the Float to Byte casting error in cache_video\n# Modify the code to handle the type conversion properly\nRUN sed -i 's/np.array(frames)/np.array(frames, dtype=np.uint8)/g' wan/utils/utils.py\n\n# Set environment variables for better performance\nENV OMP_NUM_THREADS=1\nENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n\n# Set up a volume for models and output\nVOLUME [\"/models\", \"/output\"]\n\n# Set the working directory\nWORKDIR /app\n\n# Example usage:\n# docker run --gpus all -v /path/to/models:/models -v /path/to/output:/output wan2-1-image \\\n#   python generate.py --task t2v-1.3B --size 480*832 --ckpt_dir /models/Wan2.1-T2V-1.3B \\\n#   --prompt \"your prompt here\" --save_file /output/output.mp4", "language": "python"}
{"number": 50, "title": "运行1.3B的gradio会自动下载14B的模型", "created_at": "2025-02-26T09:25:40Z", "closed_at": "2025-03-04T09:47:36Z", "commit_id": "73648654c5242bd8e11bd05ea36ffa87a6424ff6", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/50", "body": "运行时会下载一个models--Qwen--Qwen2.5-14B-Instruct文件夹，28G大小", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/50/comments", "author": "jasonlbx13", "comments": [{"user": "Memoriaaa", "created_at": "2025-02-26T09:40:03Z", "body": "gradio的demo默认开了提示词增强，会调用Qwen2.5，你可以改下代码关了\n\n参考：\n```python\n# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.\nimport argparse\nimport os.path as osp\nimport sys\nimport warnings\n\nimport gradio as gr\n\nwarnings.filterwarnings('ignore')\n\n# Model\nsys.path.insert(0, '/'.join(osp.realpath(__file__).split('/')[:-2]))\nimport wan\nfrom wan.configs import WAN_CONFIGS\nfrom wan.utils.prompt_extend import DashScopePromptExpander, QwenPromptExpander\nfrom wan.utils.utils import cache_video\n\n# Global Var\nprompt_expander = None\nwan_t2v = None\n\n\n# Button Func\ndef prompt_enc(prompt, tar_lang):\n    return prompt\n    # global prompt_expander\n    # prompt_output = prompt_expander(prompt, tar_lang=tar_lang.lower())\n    # if prompt_output.status == False:\n    #     return prompt\n    # else:\n    #     return prompt_output.prompt\n\n\ndef t2v_generation(txt2vid_prompt, resolution, sd_steps, guide_scale,\n                   shift_scale, seed, n_prompt):\n    global wan_t2v\n    # print(f\"{txt2vid_prompt},{resolution},{sd_steps},{guide_scale},{shift_scale},{seed},{n_prompt}\")\n\n    W = int(resolution.split(\"*\")[0])\n    H = int(resolution.split(\"*\")[1])\n    video = wan_t2v.generate(\n        txt2vid_prompt,\n        size=(W, H),\n        shift=shift_scale,\n        sampling_steps=sd_steps,\n        guide_scale=guide_scale,\n        n_prompt=n_prompt,\n        seed=seed,\n        offload_model=False)\n\n    cache_video(\n        tensor=video[None],\n        save_file=\"example.mp4\",\n        fps=16,\n        nrow=1,\n        normalize=True,\n        value_range=(-1, 1))\n\n    return \"example.mp4\"\n\n\n# Interface\ndef gradio_interface():\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\n                    <div style=\"text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;\">\n                        Wan2.1 (T2V-1.3B)\n                    </div>\n                    <div style=\"text-align: center; font-size: 16px; font-weight: normal; margin-bottom: 20px;\">\n                        Wan: Open and Advanced Large-Scale Video Generative Models.\n                    </div>\n                    \"\"\")\n\n        with gr.Row():\n            with gr.Column():\n                txt2vid_prompt = gr.Textbox(\n                    label=\"Prompt\",\n                    placeholder=\"Describe the video you want to generate\",\n                )\n                tar_lang = gr.Radio(\n                    choices=[\"CH\", \"EN\"],\n                    label=\"Target language of prompt enhance\",\n                    value=\"CH\")\n                run_p_button = gr.Button(value=\"Prompt Enhance\")\n\n                with gr.Accordion(\"Advanced Options\", open=True):\n                    resolution = gr.Dropdown(\n                        label='Resolution(Width*Height)',\n                        choices=[\n                            '480*832',\n                            '832*480',\n                            '624*624',\n                            '704*544',\n                            '544*704',\n                        ],\n                        value='480*832')\n\n                    with gr.Row():\n                        sd_steps = gr.Slider(\n                            label=\"Diffusion steps\",\n                            minimum=1,\n                            maximum=1000,\n                            value=50,\n                            step=1)\n                        guide_scale = gr.Slider(\n                            label=\"Guide scale\",\n                            minimum=0,\n                            maximum=20,\n                            value=6.0,\n                            step=1)\n                    with gr.Row():\n                        shift_scale = gr.Slider(\n                            label=\"Shift scale\",\n                            minimum=0,\n                            maximum=20,\n                            value=8.0,\n                            step=1)\n                        seed = gr.Slider(\n                            label=\"Seed\",\n                            minimum=-1,\n                            maximum=2147483647,\n                            step=1,\n                            value=-1)\n                    n_prompt = gr.Textbox(\n                        label=\"Negative Prompt\",\n                        placeholder=\"Describe the negative prompt you want to add\"\n                    )\n\n                run_t2v_button = gr.Button(\"Generate Video\")\n\n            with gr.Column():\n                result_gallery = gr.Video(\n                    label='Generated Video', interactive=False, height=600)\n\n        run_p_button.click(\n            fn=prompt_enc,\n            inputs=[txt2vid_prompt, tar_lang],\n            outputs=[txt2vid_prompt])\n\n        run_t2v_button.click(\n            fn=t2v_generation,\n            inputs=[\n                txt2vid_prompt, resolution, sd_steps, guide_scale, shift_scale,\n                seed, n_prompt\n            ],\n            outputs=[result_gallery],\n        )\n\n    return demo\n\n\n# Main\ndef _parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"Generate a video from a text prompt or image using Gradio\")\n    parser.add_argument(\n        \"--ckpt_dir\",\n        type=str,\n        default=\"cache\",\n        help=\"The path to the checkpoint directory.\")\n    parser.add_argument(\n        \"--prompt_extend_method\",\n        type=str,\n        default=\"local_qwen\",\n        choices=[\"dashscope\", \"local_qwen\", \"None\"],\n        help=\"The prompt extend method to use.\")\n    parser.add_argument(\n        \"--prompt_extend_model\",\n        type=str,\n        default=None,\n        help=\"The prompt extend model to use.\")\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == '__main__':\n    args = _parse_args()\n\n    # print(\"Step1: Init prompt_expander...\", end='', flush=True)\n    # if args.prompt_extend_method == \"dashscope\":\n    #     prompt_expander = DashScopePromptExpander(\n    #         model_name=args.prompt_extend_model, is_vl=False)\n    # elif args.prompt_extend_method == \"local_qwen\":\n    #     prompt_expander = QwenPromptExpander(\n    #         model_name=args.prompt_extend_model, is_vl=False, device=0)\n    # else:\n    #     raise NotImplementedError(\n    #         f\"Unsupport prompt_extend_method: {args.prompt_extend_method}\")\n    # print(\"done\", flush=True)\n\n    print(\"Step2: Init 1.3B t2v model...\", end='', flush=True)\n    cfg = WAN_CONFIGS['t2v-1.3B']\n    wan_t2v = wan.WanT2V(\n        config=cfg,\n        checkpoint_dir=args.ckpt_dir,\n        device_id=0,\n        rank=0,\n        t5_fsdp=False,\n        dit_fsdp=False,\n        use_usp=False,\n    )\n    print(\"done\", flush=True)\n\n    demo = gradio_interface()\n    demo.launch(server_name=\"0.0.0.0\", share=False, server_port=8904)\n```"}, {"user": "jasonlbx13", "created_at": "2025-02-26T09:42:36Z", "body": "感谢您的解答！\n\n"}, {"user": "fallbernana123456", "created_at": "2025-02-26T09:48:38Z", "body": "> 感谢您的解答！\n\n你在阿里云上申请一个 api-key，再使用--prompt_extend_method 'dashscope'参数就可以使用了\n"}], "satisfaction_conditions": ["A solution that prevents the automatic download of the large 14B Qwen model", "Code modification guidance to disable the prompt enhancement feature", "Understanding of why the large model was being downloaded"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:13:08"}, "dockerfile": "FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PYTHONUNBUFFERED=1\nENV PATH=\"/usr/local/cuda/bin:${PATH}\"\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    curl \\\n    python3-dev \\\n    python3-pip \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository at the specific commit\nRUN git clone https://github.com/Wan-Video/Wan2.1.git /app && \\\n    cd /app && \\\n    git checkout 73648654c5242bd8e11bd05ea36ffa87a6424ff6\n\n# Upgrade pip and install PyTorch 2.4.0 with CUDA support first\nRUN pip3 install --no-cache-dir --upgrade pip && \\\n    pip3 install --no-cache-dir torch>=2.4.0 torchvision>=0.17.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Fix for Issue #50: Prevent automatic download of Qwen2.5-14B-Instruct model\n# Create a modified requirements.txt file without the Qwen model dependency\nRUN grep -v \"qwen\" requirements.txt > requirements_modified.txt || true\n\n# Install project dependencies in batches to improve build reliability\nRUN pip3 install --no-cache-dir numpy scipy matplotlib && \\\n    pip3 install --no-cache-dir opencv-python pillow && \\\n    pip3 install --no-cache-dir tqdm transformers einops && \\\n    pip3 install --no-cache-dir huggingface_hub modelscope && \\\n    pip3 install --no-cache-dir -r requirements_modified.txt\n\n# Create model directories for user to mount models\nRUN mkdir -p /models/Wan2.1-T2V-1.3B /models/Wan2.1-T2V-14B /models/Wan2.1-I2V-14B-480P /models/Wan2.1-I2V-14B-720P\n\n# Set environment variables for better performance\nENV OMP_NUM_THREADS=1\nENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n\n# Set up a volume for models and output\nVOLUME [\"/models\", \"/output\"]\n\n# Set the working directory\nWORKDIR /app", "language": "python"}
{"number": 132, "title": "Killed when generated video", "created_at": "2025-03-02T05:04:44Z", "closed_at": "2025-03-04T09:54:01Z", "commit_id": "a326079926a4a347ecda8863dc40ba2d7680a294", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/132", "body": "[2025-03-02 12:01:03,397] INFO: Input image: examples/i2v_input.JPG\n[2025-03-02 12:01:03,542] INFO: Creating WanI2V pipeline.\n[2025-03-02 12:01:54,569] INFO: loading .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P/models_t5_umt5-xxl-enc-bf16.pth\n[2025-03-02 12:02:05,031] INFO: loading .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P/Wan2.1_VAE.pth\n[2025-03-02 12:02:05,867] INFO: loading .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth\n[2025-03-02 12:02:11,709] INFO: Creating WanModel from .cache/modelscope/hub/models/Wan-AI/Wan2___1-I2V-14B-480P\n[2025-03-02 12:02:35,384] INFO: Generating video ...\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [45:42<00:00, 68.55s/it]\nKilled", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/132/comments", "author": "DorothyDoro", "comments": [{"user": "wxwwt", "created_at": "2025-03-02T06:26:00Z", "body": "I also have this problem\n\n\n(myenv) dministrator@DESKTOP-C3RIDG2:/opt/project/Wan2.1$ python generate.py  --task t2v-1.3B --size 832*480 --ckpt_dir ./Wan2.1-T2V-1.3B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n[2025-03-02 13:59:35,781] INFO: offload_model is not specified, set to True.\n[2025-03-02 13:59:35,781] INFO: Generation job args: Namespace(task='t2v-1.3B', size='832*480', frame_num=81, ckpt_dir='./Wan2.1-T2V-1.3B', offload_model=True, ulysses_size=1, ring_size=1, t5_fsdp=False, t5_cpu=False, dit_fsdp=False, save_file=None, prompt='Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.', use_prompt_extend=False, prompt_extend_method='local_qwen', prompt_extend_model=None, prompt_extend_target_lang='ch', base_seed=8478258736304712572, image=None, sample_solver='unipc', sample_steps=50, sample_shift=5.0, sample_guide_scale=5.0)\n[2025-03-02 13:59:35,781] INFO: Generation model config: {'__name__': 'Config: Wan T2V 1.3B', 't5_model': 'umt5_xxl', 't5_dtype': torch.bfloat16, 'text_len': 512, 'param_dtype': torch.bfloat16, 'num_train_timesteps': 1000, 'sample_fps': 16, 'sample_neg_prompt': '色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质 量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走', 't5_checkpoint': 'models_t5_umt5-xxl-enc-bf16.pth', 't5_tokenizer': 'google/umt5-xxl', 'vae_checkpoint': 'Wan2.1_VAE.pth', 'vae_stride': (4, 8, 8), 'patch_size': (1, 2, 2), 'dim': 1536, 'ffn_dim': 8960, 'freq_dim': 256, 'num_heads': 12, 'num_layers': 30, 'window_size': (-1, -1), 'qk_norm': True, 'cross_attn_norm': True, 'eps': 1e-06}\n[2025-03-02 13:59:35,781] INFO: Input prompt: Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\n[2025-03-02 13:59:35,781] INFO: Creating WanT2V pipeline.\nKilled"}, {"user": "FurkanGozukara", "created_at": "2025-03-02T06:29:22Z", "body": "Killed is out of ram\n\nIncrease virtual ram "}, {"user": "wxwwt", "created_at": "2025-03-02T08:08:50Z", "body": "> Killed is out of ram\n> \n> Increase virtual ram\n\nthx  it`s work~"}], "satisfaction_conditions": ["An explanation of why the video generation process is being killed", "A solution to prevent the video generation process from being killed", "Identification of resource constraints causing the process termination"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:12:25"}, "dockerfile": "FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PYTHONUNBUFFERED=1\nENV PATH=\"/usr/local/cuda/bin:${PATH}\"\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    curl \\\n    python3-dev \\\n    python3-pip \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository at the specific commit\nRUN git clone https://github.com/Wan-Video/Wan2.1.git /app && \\\n    cd /app && \\\n    git checkout a326079926a4a347ecda8863dc40ba2d7680a294\n\n# Upgrade pip and install PyTorch 2.4.0 with CUDA support first\n# This prevents dependency conflicts and speeds up the build\nRUN pip3 install --no-cache-dir --upgrade pip && \\\n    pip3 install --no-cache-dir torch>=2.4.0 torchvision>=0.17.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install project dependencies in batches to prevent memory issues\nRUN pip3 install --no-cache-dir numpy scipy matplotlib && \\\n    pip3 install --no-cache-dir opencv-python pillow && \\\n    pip3 install --no-cache-dir tqdm transformers einops && \\\n    pip3 install --no-cache-dir huggingface_hub modelscope && \\\n    pip3 install --no-cache-dir -r requirements.txt\n\n# Set up directories for model caching\nRUN mkdir -p /root/.cache/modelscope/hub/models/Wan-AI\nRUN mkdir -p /root/.cache/torch/hub/checkpoints\n\n# Create directories for data and output\nRUN mkdir -p /data /output\n\n# Set the working directory\nWORKDIR /app\n\n# Set environment variables for memory management\nENV OMP_NUM_THREADS=1\nENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n\n# The image is now ready for use\n# Users can mount their model files and run the generation scripts\n# Example: docker run --gpus all --shm-size=16g -v /path/to/models:/data -v /path/to/output:/output wan-video", "language": "python"}
{"number": 294, "title": "about vae causality", "created_at": "2025-04-01T08:41:46Z", "closed_at": "2025-04-02T09:29:01Z", "commit_id": "82c6bf86e27679810d40ef07bf35a54e6caae460", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/294", "body": "Hello, I have two questions regarding the following statement from the paper:\n\n\"In terms of architecture, we replace all GroupNorm layers (Wu & He, 2018) with RMSNorm layers (Zhang & Sennrich, 2019) to preserve temporal causality.\"\n\nWhat is the motivation behind replacing GroupNorm with RMSNorm?\n\nWhen RMSNorm performs normalization, it uses statistics from all frames. My understanding is that this does not preserve causality because earlier frames, when being normalized, rely on statistics computed from the entire sequence—meaning they effectively \"see\" future frames.", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/294/comments", "author": "bobopit", "comments": [{"user": "wpy1999", "created_at": "2025-04-01T10:03:34Z", "body": "Hello. For a standard GroupNorm(group=g), the mean and variance of the input x (shape [b,c,t,h,w]) are computed as mean=x.view(b, g,-1).mean(); var=x.view(b, g,-1).var(). Thus the statistics of the mean and variance of each frame will involve the entire time dimension and are not causally. Whereas RMSNorm normalizes only along the channel dimension, i.e., x_norm=F.normalize(x, dim=1), the normalization process between different frames is independent, making it causal."}, {"user": "bobopit", "created_at": "2025-04-01T12:18:37Z", "body": "Thank you for your response. I now understand the causality aspect. However, to preserve causality, one could also use GroupNorm2D instead of GroupNorm. What are the advantages of using RMSNorm over GroupNorm2D?"}, {"user": "wpy1999", "created_at": "2025-04-01T15:46:04Z", "body": "In fact, we tried using GroupNorm2D, but our preliminary experiments show that RMSNorm performs better."}], "satisfaction_conditions": ["An explanation of how RMSNorm preserves temporal causality compared to GroupNorm", "A comparison of the performance benefits of RMSNorm over alternative causal normalization approaches", "Technical details about how normalization is computed across different dimensions"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:12:10"}, "dockerfile": null, "language": "python"}
{"number": 279, "title": "微调wan2.1-t2v-1.3B时，flow_shift要设置成多少比较合适呢？", "created_at": "2025-03-26T09:28:57Z", "closed_at": "2025-03-26T11:43:56Z", "commit_id": "bc3249d61c2de11ca37c74440fb67114fbaa4860", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/279", "body": null, "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/279/comments", "author": "cuijh26", "comments": [{"user": "danielzyy1990", "created_at": "2025-03-26T11:03:22Z", "body": "480P下设置成2就可以了。"}, {"user": "cuijh26", "created_at": "2025-03-26T11:43:56Z", "body": "好的 谢谢"}], "satisfaction_conditions": ["A specific recommended value for the flow_shift parameter when fine-tuning wan2.1-t2v-1.3B", "Context-specific parameter recommendations that account for resolution settings"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:12:17"}, "dockerfile": null, "language": "python"}
{"number": 127, "title": "单卡4090用kj的workflow跑720P的问题", "created_at": "2025-03-01T08:53:47Z", "closed_at": "2025-03-01T09:37:36Z", "commit_id": "a326079926a4a347ecda8863dc40ba2d7680a294", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/127", "body": "嗨，大家好。\n\n我现在是单卡4090用kj的workflow跑14B-fp8的，分辨率是1280*720的，还用了flash attention2和torch compile，目前2个多小时了，官方那个5秒81帧的例子，还没生成出来，有人试过同等配置的嘛？\n\n但是这个配置跑512*512的，只需要4分多钟。", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/127/comments", "author": "zm4341", "comments": [{"user": "able2608", "created_at": "2025-03-01T09:26:14Z", "body": "torch compile might take a (long) while depending on your specific setup in the first run as it tries to optimize inference speed on your machine. You might want to disable it if you are just testing it. But judging from the fact that 512×512 can be generated without a problem, it might simply be that your VRAM is not enough for 720p generation and you are relying on your system RAM as fallback, which is dog slow. You can verify it by checking if system RAM usage skyrockets during generation."}, {"user": "zm4341", "created_at": "2025-03-01T09:37:27Z", "body": "> torch compile might take a (long) while depending on your specific setup in the first run as it tries to optimize inference speed on your machine. You might want to disable it if you are just testing it. But judging from the fact that 512×512 can be generated without a problem, it might simply be that your VRAM is not enough for 720p generation and you are relying on your system RAM as fallback, which is dog slow. You can verify it by checking if system RAM usage skyrockets during generation.\n\nOK, thanks a lot for your explanation. You're right for sure. It must have been allocated to the RAM. I just tried 1024*1024 and it directly gave an error: Allocation on device."}], "satisfaction_conditions": ["An explanation for why the 720p video generation is taking much longer than the 512x512 generation", "Information about hardware resource constraints when running high-resolution video generation", "Practical troubleshooting advice for diagnosing performance issues in the workflow", "Insights about torch compile's impact on initial processing time"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:12:42"}, "dockerfile": null, "language": "python"}
{"number": 109, "title": "--ulysses_size 和 --ring_size怎么调合适", "created_at": "2025-02-28T07:10:06Z", "closed_at": "2025-03-05T08:29:33Z", "commit_id": "a326079926a4a347ecda8863dc40ba2d7680a294", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/109", "body": "多卡序列并行 这两个好像乘起来需要等于总的进程数，\nassert args.ulysses_size * args.ring_size == world_size, f\"The number of ulysses_size and ring_size should be equal to the world size.\"\n然后ulysses_size 需要是num_heads 的银子\n        assert cfg.num_heads % args.ulysses_size == 0, f\"`num_heads` must be divisible by `ulysses_size`.\"\n\n但是具体这两个参数是什么意义 该怎么调有没有谁能解释一下。", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/109/comments", "author": "aspatic", "comments": [{"user": "wan-x-ai", "created_at": "2025-03-04T09:23:07Z", "body": "If running on a single machine, set --ulysses_size to the number of GPUs on that machine. For multi-machine inference, set --ulysses_size to the number of GPUs per machine and --ring_size to the total number of machines. Finally, ensure --ulysses_size is a divisor of cfg.num_heads."}, {"user": "aspatic", "created_at": "2025-03-05T08:29:30Z", "body": "thanks, of great help"}], "satisfaction_conditions": ["Clear explanation of the purpose and meaning of the --ulysses_size and --ring_size parameters", "Practical guidance on how to configure these parameters in different deployment scenarios", "Explanation of the relationship between these parameters and other system constraints"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:12:51"}, "dockerfile": null, "language": "python"}
{"number": 107, "title": "使用comfyui能不能多卡跑14b模型", "created_at": "2025-02-28T04:12:34Z", "closed_at": "2025-03-04T09:27:25Z", "commit_id": "a326079926a4a347ecda8863dc40ba2d7680a294", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/107", "body": "比如8个4090能不能在comfyui中跑wan2.1模型,有单卡最低限制吗? 因为之前看别的即使支持多卡推理,也需要单卡有很高的内存才可以,wan也是这样吗", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/107/comments", "author": "papandadj", "comments": [{"user": "aspatic", "created_at": "2025-02-28T06:31:57Z", "body": "有序列和数据并行 没看见有模型并行的 看起来14bfp32模型最低显存要求8*22G"}, {"user": "papandadj", "created_at": "2025-02-28T06:49:32Z", "body": "comfyui不是只能用一个gpu么, wan有设置多gpu的参数吗\n"}, {"user": "aspatic", "created_at": "2025-03-03T02:53:15Z", "body": "Wan2.1原始代码的多卡只能增加生成速度 并不能让一张显卡爆显存的情况下模型分开放两张卡上"}, {"user": "papandadj", "created_at": "2025-03-03T02:55:37Z", "body": "好的, 感谢介绍"}], "satisfaction_conditions": ["Information about whether ComfyUI can distribute a large model (specifically WAN 2.1 14B) across multiple GPUs", "Clarification on minimum GPU memory requirements for running the model", "Explanation of how multi-GPU support functions with this specific model", "Clear distinction between parallel processing for speed versus memory distribution"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:12:56"}, "dockerfile": null, "language": "python"}
{"number": 66, "title": "No such file or directory: '..Wan2.1/Wan2.1-T2V-14B/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", "created_at": "2025-02-26T15:57:10Z", "closed_at": "2025-02-26T18:33:03Z", "commit_id": "190e9286813452351f44e9fd11b8614cf5d91fd3", "labels": [], "url": "https://github.com/Wan-Video/Wan2.1/issues/66", "body": "These weights aren't in this folder. Do I need to download them separately somehow?", "comments_url": "https://api.github.com/repos/Wan-Video/Wan2.1/issues/66/comments", "author": "onoregleb", "comments": [{"user": "antonsb", "created_at": "2025-02-26T16:08:11Z", "body": "are you running image to video on text to video download directory?\ndownload i2v model, roberta is included in those"}, {"user": "onoregleb", "created_at": "2025-02-26T16:30:27Z", "body": "> are you running image to video on text to video download directory? download i2v model, roberta is included in those\n\nI'm using image to video. I download using \n`huggingface-cli download Wan-AI/Wan2.1-T2V-14B --local-dir./Wan2.1-T2V-14B`"}, {"user": "antonsb", "created_at": "2025-02-26T16:38:09Z", "body": "well you are trying to use  I2V (image to video) on T2V (text to video) download.\n\n`huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./I2V-14B-480P`\n\nyou will need A LOT of VRAM tho"}, {"user": "onoregleb", "created_at": "2025-02-26T18:33:03Z", "body": "@antonsb oh, my bad. Thank u very much :)\n\nIt requires really a lot of memory. An OOM error occurs on 48GB\n"}], "satisfaction_conditions": ["Clarification on which model repository to download for image-to-video functionality", "Correct command to download the appropriate model files", "Information about hardware requirements for running the model"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:13:02"}, "dockerfile": null, "language": "python"}
{"number": 144, "title": "Prevent Timeout", "created_at": "2025-03-20T20:04:09Z", "closed_at": "2025-03-22T18:23:31Z", "commit_id": "c4ad14be09186e3286fe68dc891bb8ad1845d9dd", "labels": [], "url": "https://github.com/ezyang/codemcp/issues/144", "body": "Hello, quick question, how do you manage the Timeout errors that you encounter when dealing with long answers ?", "comments_url": "https://api.github.com/repos/ezyang/codemcp/issues/144/comments", "author": "Pekno", "comments": [{"user": "ezyang", "created_at": "2025-03-20T22:30:59Z", "body": "Each of the builtin tool actions is quick so you never get close to the 60s timeout. This is more of a problem for custom commands which could take a long time to run. I think probably the right way to handle this when we come to it is to run the command asynchronously, block the tool call 55sec or so, and if the async command is not done yet we return and ask the LLM to do another tool call to wait some more."}, {"user": "Pekno", "created_at": "2025-03-20T22:37:06Z", "body": "Maybe my actions are too broad, but I encounter a lot of timeout when juste asking things like \"Implement X feature\", wich it seems to understand and try to implement, but then after a while just timeout and the conversation closes."}, {"user": "borrelan", "created_at": "2025-03-21T05:21:04Z", "body": "I've experienced random cannot connect to Claude and occasionally cannot connect to codemcp.  I either reload the mcp or restart Claude Desktop, which resolves the issue for a while."}, {"user": "ezyang", "created_at": "2025-03-21T06:45:43Z", "body": "Oh so there is an infinite loop bug on main I need to push a fix for lol. If the logs say \"advanced patch apply\" before it hangs it's that"}, {"user": "notschema", "created_at": "2025-03-21T07:43:02Z", "body": "I'm also having a similar issue where when codemcp makes a file change, it hangs at doing the write task, even if it completes it."}, {"user": "ezyang", "created_at": "2025-03-21T23:32:48Z", "body": "I just cut a new release with the infinite loop fix. Please give it a try. I'll close this issue in a week or so if no one reports that it's still happening on the newest version."}, {"user": "Pekno", "created_at": "2025-03-21T23:41:34Z", "body": "Everything seems back in order, didn't encounter any timeout for the time being. Will try with more tests tomorow and will close if no issues. Thanks for the quick fix ! And great work for this MCP !"}, {"user": "notschema", "created_at": "2025-03-22T02:30:57Z", "body": "I'm not sure if should open a different issue or not; just because it's sort of related to timeout issues; but even when creating a file for example a simple txt document. the file is created, then it will hang for another ~30 seconds until saying **\"I've created a test file named test_file.txt in the %name% directory. The file contains a simple test function that adds two numbers together\"**\n\nIs this normal behavior? "}, {"user": "ezyang", "created_at": "2025-03-22T02:52:49Z", "body": "If the MCP finished running then further delay is an Anthropic problem. I have noticed this happens sometimes.\n\nAnother cause for hang is if something bad happens to codemcp server. Then Claude Desktop is just wedged and you need to restart it.\n\nIf you have evidence (eg logs) that it is specifically a codemcp problem I will look more. But a lot of slowness I have noticed in practice is Anthropic"}, {"user": "Pekno", "created_at": "2025-03-22T18:23:32Z", "body": "Everything is working as intended, no more Timeout ! Thanks again for the fix !"}], "satisfaction_conditions": ["A fix for timeout errors when dealing with long answers", "Ability to complete complex tasks without the conversation closing prematurely", "Stable performance without requiring manual restarts or reloads"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:07:48"}, "dockerfile": null, "language": "python"}
{"number": 139, "title": "Unable to make any changes", "created_at": "2025-03-19T22:19:15Z", "closed_at": "2025-03-20T20:18:09Z", "commit_id": "c4ad14be09186e3286fe68dc891bb8ad1845d9dd", "labels": [], "url": "https://github.com/ezyang/codemcp/issues/139", "body": "Hi, \nThis is my first attempt using this tool and it's a bit confusing. I created `codemcp.toml` which is empty because I do not need it to do anything except make changes to the files and commit. No run/test etc. However, it keeps saying permission denied even though I selected the \"Allow for this chat\" prompt when I request a change to my repo. Here's what I see:\n\n```\nNow that I've initialized the project, I'll first search for the Streamlit file we need to migrate to Gradio.\n{\n  `path`: `<path>`,\n  `chat_id`: `3-refactor-migrate-ui-from-streamlit-to-gradio`,\n  `subtool`: `LS`\n}\nError: File is not in a git repository. Permission denied.\n\nLet me try a different approach to find the streamlit file:\n{\n  `path`: `<path>`,\n  `chat_id`: `3-refactor-migrate-ui-from-streamlit-to-gradio`,\n  `command`: `find`,\n  `subtool`: `RunCommand`,\n  `arguments`: `[\\\"<path>\", \\\"-name\\\", \\\"streamlit*\\\"]`\n}\nError: No find command configured in codemcp.toml\nand so on. \n```\n\nDo I need to create commands for ls/find etc for it to work? What am I missing?", "comments_url": "https://api.github.com/repos/ezyang/codemcp/issues/139/comments", "author": "shekhars-li", "comments": [{"user": "ezyang", "created_at": "2025-03-20T02:22:36Z", "body": "The second tool use is a red herring, Sonnet is very prone to hallucinating when a tool that should work doesn't.\n\nDid you also init a git repository in your codemcp.toml? This smells like you didn't."}, {"user": "notschema", "created_at": "2025-03-20T15:00:29Z", "body": "Okay. i spent like 2 hours trying to work this out. as i was having the same issue.\n\ngit config --global user.email \"\" \ngit config --global user.name \"\"\n\nAfter running these, i was able to write.  Give this a try @shekhars-li "}, {"user": "shekhars-li", "created_at": "2025-03-20T20:18:09Z", "body": "Thanks a lot @notschema. That worked for me! :) "}, {"user": "ezyang", "created_at": "2025-03-24T10:42:36Z", "body": "I removed some exception rethrowing which hopefully makes this clearer in the future"}], "satisfaction_conditions": ["A solution that enables the user to make changes to files in their repository using the tool", "Clear instructions for resolving git-related permission issues", "A straightforward fix that doesn't require complex configuration of the codemcp.toml file"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:07:55"}, "dockerfile": null, "language": "python"}
{"number": 341, "title": "question: why observability/prom-adapter.yaml only handles vllm_num_requests_waiting metric", "created_at": "2025-04-01T16:29:08Z", "closed_at": "2025-04-03T01:45:53Z", "commit_id": "1a36a9be9ba6d3d4fa1d9c908241f7e28142aad7", "labels": [], "url": "https://github.com/vllm-project/production-stack/issues/341", "body": "In the observability/prom-adapter.yaml configuration, why does the rule only match the metric vllm_num_requests_waiting\n\nCurrent\n\n```\nrules:\n  custom:\n    - seriesQuery: '{__name__=~\"^vllm:num_requests_waiting$\"}'  # Only matches vllm metric\n```\n\nHow to determine additional metrics (e.g., gpu_prefix_cache_hit_rate)", "comments_url": "https://api.github.com/repos/vllm-project/production-stack/issues/341/comments", "author": "ZhouBoXiao", "comments": [{"user": "YuhanLiu11", "created_at": "2025-04-02T05:48:13Z", "body": "Hi @ZhouBoXiao thanks for submitting the issue! \nThis observability/prom-adapter.yaml is used as the metrics for a basic version of autoscaling. You may add more metrics to it if you want to use them for more advanced autoscaling. \n"}, {"user": "ZhouBoXiao", "created_at": "2025-04-03T01:45:53Z", "body": "Thank you, I can add personalized metrics as needed,  not limited in the prom-adapter.yaml"}], "satisfaction_conditions": ["Clarification on the purpose of the prom-adapter.yaml configuration file", "Confirmation that additional custom metrics can be added to the configuration", "Understanding the flexibility of the metrics configuration system"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:18:42"}, "dockerfile": null, "language": "python"}
{"number": 58, "title": "./data_processing_pipeline.sh 为什么CPU占用率这么高？", "created_at": "2025-01-09T17:59:53Z", "closed_at": "2025-01-13T07:50:17Z", "commit_id": "0c3489c87c53fdc03820def9cf9100c0e81e2964", "labels": [], "url": "https://github.com/bytedance/LatentSync/issues/58", "body": "@chunyu-li 您运行代码是用什么配置? 我两张4090，但是CPU100%使用率。\r\n\r\n出现这个 INFO: Created TensorFlow Lite XNNPACK delegate for CPU. 是不是TensorFlow 没有安装GPU的版本？", "comments_url": "https://api.github.com/repos/bytedance/LatentSync/issues/58/comments", "author": "wangaocheng", "comments": [{"user": "chunyu-li", "created_at": "2025-01-10T07:39:34Z", "body": "`per_gpu_num_workers` 那个参数是每张卡用多少进程 ，如果你是两张卡那就是总共 40 个进程，如果你的 CPU 没有 40 核以上就会占用率很高"}, {"user": "wangaocheng", "created_at": "2025-01-10T12:57:35Z", "body": "了解，我今天看了一下代码，预处理有很多项目用不到GPU"}], "satisfaction_conditions": ["An explanation of why CPU usage is high when running the data processing pipeline", "Clarification about the relationship between GPU configuration and CPU utilization", "Information about how the number of worker processes affects system resource utilization"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:07:24"}, "dockerfile": "FROM pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wget \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1-mesa-glx \\\n    python3-dev \\\n    gcc \\\n    g++ \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone repository and checkout specific commit\nRUN git clone https://github.com/bytedance/LatentSync.git . && \\\n    git checkout 0c3489c87c53fdc03820def9cf9100c0e81e2964\n\n# Install Python dependencies with specific tensorflow-gpu version first to avoid conflicts\n# Install tensorflow with GPU support and avoid the timeout by splitting installations\nRUN pip install --no-cache-dir tensorflow==2.10.0 && \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Create directories for checkpoints\nRUN mkdir -p checkpoints/whisper checkpoints/auxiliary\n\n# Set environment variables for GPU usage\nENV PYTHONPATH=/app:$PYTHONPATH\nENV CUDA_VISIBLE_DEVICES=0\nENV TF_FORCE_GPU_ALLOW_GROWTH=true\nENV TF_GPU_THREAD_MODE=gpu_private\n\n# Create necessary directory structure\nRUN mkdir -p checkpoints/whisper checkpoints/auxiliary\n\n# Set lower CPU priority for TensorFlow operations to prevent 100% CPU usage\nENV TF_CPU_PARALLELISM_THREADS=4\nENV OMP_NUM_THREADS=4\nENV MKL_NUM_THREADS=4", "language": "python"}
{"number": 134, "title": "Some questions about SyncNet evaluation", "created_at": "2025-02-17T06:50:04Z", "closed_at": "2025-02-18T10:18:23Z", "commit_id": "5289c629cd23b4b3dffaebf805e0e012ea90ed23", "labels": [], "url": "https://github.com/bytedance/LatentSync/issues/134", "body": "Great job, I would like to know how to reproduce SyncNet's 94% accuracy metric on the HDFF test set", "comments_url": "https://api.github.com/repos/bytedance/LatentSync/issues/134/comments", "author": "OdingdongO", "comments": [{"user": "chunyu-li", "created_at": "2025-02-17T08:22:59Z", "body": "1. Use the data processing pipeline to process HDTF data\n2. Run `./eval/eval_syncnet_acc.sh` to calculate the SyncNet accuracy"}, {"user": "OdingdongO", "created_at": "2025-02-18T09:29:54Z", "body": "> 1. Use the data processing pipeline to process HDTF data\n> 2. Run `./eval/eval_syncnet_acc.sh` to calculate the SyncNet accuracy\n\nyes, I have noticed this script. In the paper, 'During evaluation, we randomly selected 30 videos from the test set of HDTF or VoxCeleb2', is the 94% accuracy obtained on these 30 videos? Do you have a name list for these 30 videos?"}, {"user": "chunyu-li", "created_at": "2025-02-18T09:41:13Z", "body": "1. 'During evaluation, we randomly selected 30 videos from the test set of HDTF or VoxCeleb2' This line is for U-Net evaluation. Not related to SyncNet evaluation. \n2. In paper section 3.3 discussion: For SyncNet evaluation, we trained it on VoxCeleb2 and test it on HDTF, which is an out-of-distribution setting, so all videos in HDTF can be considered as test test, no need for name list."}, {"user": "OdingdongO", "created_at": "2025-02-18T09:48:27Z", "body": "> 1. 'During evaluation, we randomly selected 30 videos from the test set of HDTF or VoxCeleb2' This line is for U-Net evaluation. Not related to SyncNet evaluation.\n> 2. In paper section 3.3 discussion: For SyncNet evaluation, we trained it on VoxCeleb2 and test it on HDTF, which is an out-of-distribution setting, so all videos in HDTF can be considered as test test, no need for name list.\n\nok,thank you"}], "satisfaction_conditions": ["Clarification about which dataset was used for the 94% accuracy metric mentioned in the paper", "Explanation of the difference between SyncNet evaluation and U-Net evaluation methodologies", "Information about the test dataset composition for reproducing the SyncNet accuracy results"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:07:18"}, "dockerfile": null, "language": "python"}
{"number": 7, "title": "ModuleNotFoundError: No module named 'mcp'", "created_at": "2025-03-01T23:49:53Z", "closed_at": "2025-03-02T04:44:41Z", "commit_id": "4c990f1fd163c4e62b50fc0b704dc48a08acf201", "labels": ["bug"], "url": "https://github.com/HazyResearch/minions/issues/7", "body": "When following the instructions for the demo, after running streamlit run app.py, the browser displays this error:\n\nTraceback\nFile \"/Users/username/code/python/minions/.venv/lib/python3.13/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 121, in exec_func_with_error_handling\n    result = func()\nFile \"/Users/username/code/python/minions/.venv/lib/python3.13/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 591, in code_to_exec\n    exec(code, module.__dict__)\n    ~~~~^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/username/code/python/minions/app.py\", line 4, in <module>\n    from minions.minions_mcp import SyncMinionsMCP, MCPConfigManager\nFile \"/Users/username/code/python/minions/minions/minions_mcp.py\", line 14, in <module>\n    from mcp import ClientSession, StdioServerParametersusername", "comments_url": "https://api.github.com/repos/HazyResearch/minions/issues/7/comments", "author": "developer-nome", "comments": [{"user": "ANarayan", "created_at": "2025-03-02T00:36:58Z", "body": "@developer-nome please make sure to install mcp.\n\nRun `pip install mcp`"}, {"user": "danbider", "created_at": "2025-03-02T04:20:48Z", "body": "or alternatively rerun \n```\npip install -e .\n```\nsince `mcp` was introduced recently to the `setup.py` file."}, {"user": "developer-nome", "created_at": "2025-03-02T04:44:41Z", "body": "Works now that mcp was introduced recently to the setup.py file. Thank you!"}], "satisfaction_conditions": ["A solution that resolves the missing 'mcp' module dependency", "Instructions that can be executed within the user's existing environment", "An explanation of why the dependency was missing"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:04:51"}, "dockerfile": null, "language": "python"}
{"number": 91, "title": "cannot import name 'apply_mod' from 'comfy.ldm.flux.layers'", "created_at": "2025-03-22T06:30:48Z", "closed_at": "2025-03-24T03:30:56Z", "commit_id": "f0c6c56778d626b23db99ea1fee41736f7f55720", "labels": [], "url": "https://github.com/welltop-cn/ComfyUI-TeaCache/issues/91", "body": "Hello author, teacache has encountered an error on my other computer. The error message is as follows\n\nD:\\ComfyUI_windows_portable_xiaosan>.\\python_embeded\\python.exe -s ComfyUI\\main.py --windows-standalone-build --disable-cuda-malloc  --listen 10.0.0.2 --cuda-device 1 --port 8198\n[START] Security scan\n[DONE] Security scan\nComfyUI-Manager: installing dependencies done.\n** ComfyUI startup time: 2025-03-22 14:22:47.944\n** Platform: Windows\n** Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n** Python executable: D:\\ComfyUI_windows_portable_xiaosan\\python_embeded\\python.exe\n** ComfyUI Path: D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\n** User directory: D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\user\n** ComfyUI-Manager config path: D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\user\\default\\ComfyUI-Manager\\config.ini\n** Log path: D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\user\\comfyui.log\n\nPrestartup times for custom nodes:\n   0.0 seconds: D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\custom_nodes\\rgthree-comfy\n   0.0 seconds: D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\custom_nodes\\comfyui-easy-use\n   4.9 seconds: D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\custom_nodes\\ComfyUI-Manager\n\nSet cuda device to: 1\nCheckpoint files will always be loaded safely.\nTotal VRAM 32626 MB, total RAM 131071 MB\npytorch version: 2.4.1+cu124\nSet vram state to: NORMAL_VRAM\nDevice: cuda:0 Tesla V100S-PCIE-32GB : native\nUsing pytorch attention\n\n[0;33m[ReActor]\u001b[0m - \u001b[38;5;173mSTATUS\u001b[0m - \u001b[0;32mRunning v0.5.1-b2 in ComfyUI\u001b[0m\nTorch version: 2.4.1+cu124\n**Traceback (most recent call last):\n  File \"D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\nodes.py\", line 2106, in load_custom_node\n    module_spec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\custom_nodes\\ComfyUI-TeaCache\\__init__.py\", line 1, in <module>\n    from .nodes import NODE_CLASS_MAPPINGS as NODES_CLASS, NODE_DISPLAY_NAME_MAPPINGS as NODES_DISPLAY\n  File \"D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\custom_nodes\\ComfyUI-TeaCache\\nodes.py\", line 10, in <module>\n    from comfy.ldm.flux.layers import timestep_embedding, apply_mod\nImportError: cannot import name 'apply_mod' from 'comfy.ldm.flux.layers' (D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\comfy\\ldm\\flux\\layers.py)**\n\n**Cannot import D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\custom_nodes\\ComfyUI-TeaCache module for custom nodes: cannot import name 'apply_mod' from 'comfy.ldm.flux.layers' (D:\\ComfyUI_windows_portable_xiaosan\\ComfyUI\\comfy\\ldm\\flux\\layers.py)**", "comments_url": "https://api.github.com/repos/welltop-cn/ComfyUI-TeaCache/issues/91/comments", "author": "xs315431", "comments": [{"user": "YunjieYu", "created_at": "2025-03-22T15:10:44Z", "body": "Your ComfyUI version is too old. Please update ComfyUI to the latest."}, {"user": "xs315431", "created_at": "2025-03-24T03:30:48Z", "body": "> Your ComfyUI version is too old. Please update ComfyUI to the latest.\n\nThank you, this is very useful to me."}], "satisfaction_conditions": ["Clear identification of the root cause of the import error", "A straightforward solution to resolve the compatibility issue", "Brief, concise guidance without unnecessary technical complexity"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:05:33"}, "dockerfile": null, "language": "python"}
{"number": 79, "title": "mac mini 2 pro 16g", "created_at": "2025-03-15T10:51:22Z", "closed_at": "2025-03-17T03:28:25Z", "commit_id": "7d9fd8644c5de20b4c9c4f7f8e3cdf7f173ddb78", "labels": [], "url": "https://github.com/welltop-cn/ComfyUI-TeaCache/issues/79", "body": "There is no effect at all. I tested it with flux parameters of 0.12 and 0.4 and it had basically no effect.", "comments_url": "https://api.github.com/repos/welltop-cn/ComfyUI-TeaCache/issues/79/comments", "author": "cjkcr", "comments": [{"user": "cjkcr", "created_at": "2025-03-15T10:55:14Z", "body": "The gguf model I used has 4 steps to produce the graph. Maybe the number of steps is too small. I will test it with 20 steps."}, {"user": "YunjieYu", "created_at": "2025-03-15T12:04:05Z", "body": "hi @cjkcr, TeaCache will have no effect on low-step models. I think the speedup can be felt at least at 10 or more steps."}, {"user": "cjkcr", "created_at": "2025-03-15T12:26:29Z", "body": "ok thank you..."}], "satisfaction_conditions": ["Explanation of when TeaCache is effective based on model step count", "Guidance on minimum step count threshold for observable TeaCache benefits"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:05:39"}, "dockerfile": null, "language": "python"}
{"number": 67, "title": "update forward for image2video model support", "created_at": "2025-03-10T13:12:56Z", "closed_at": "2025-03-27T07:55:32Z", "commit_id": "1a11412deac8f6fc9fa2a487bf952000c279ed39", "labels": [], "url": "https://github.com/welltop-cn/ComfyUI-TeaCache/pull/67", "body": "Updated a code for image2video hunyuan model support", "comments_url": "https://api.github.com/repos/welltop-cn/ComfyUI-TeaCache/issues/67/comments", "author": "attashe", "comments": [{"user": "attashe", "created_at": "2025-03-10T13:16:00Z", "body": "Oh, I late)\r\n\r\nBut I think in current version one thing still need to be fixed:\r\n\r\n```python\r\nfrom comfy.ldm.flux.layers import timestep_embedding, apply_mod\r\n...\r\nmodulated_inp = apply_mod(modulated_inp, (1 + img_mod1.scale), img_mod1.shift, modulation_dims)\r\n```\r\n\r\nIt will fix this error:\r\n```\r\nnodes.py\", line 254, in teacache_hunyuanvideo_forward\r\n    modulated_inp = (1 + img_mod1.scale) * modulated_inp + img_mod1.shift\r\n                    ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~\r\nRuntimeError: The size of tensor a (2) must match the size of tensor b (34560) at non-singleton dimension 1\r\n```"}, {"user": "YunjieYu", "created_at": "2025-03-25T11:29:35Z", "body": "Hi @attashe, first of all, thank you for the contribution. I'm sorry to reply you so late, because I've been a bit busy recently. Indeed, you are right, `apply_mod` need be applied in `teacache_hunyuanvideo_forward` function, which has been fixed in my previous version.\r\n\r\nOf course, I still welcome any PRs for supporting TeaCache in other models if you can implement it.\r\n\r\nThank you again for your contribution to the TeaCache project!"}, {"user": "attashe", "created_at": "2025-03-25T19:33:20Z", "body": "Thanks for you work! If I could optimize some memory consumption then will create another pull request)"}, {"user": "YunjieYu", "created_at": "2025-03-27T07:55:18Z", "body": "Of course, welcome pull request!"}], "satisfaction_conditions": ["Acknowledgment of the bug report regarding the dimension mismatch in the apply_mod function", "Confirmation that the issue has been addressed in a previous version", "Openness to future contributions from the user", "Recognition of the user's contribution to the project"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:05:45"}, "dockerfile": null, "language": "python"}
{"number": 16, "title": " No module named 'comfy.ldm.hunyuan_video'", "created_at": "2025-01-11T17:18:34Z", "closed_at": "2025-01-14T12:27:10Z", "commit_id": "e1939f0822f30afa4fe46017f310a28b66fa38d1", "labels": [], "url": "https://github.com/welltop-cn/ComfyUI-TeaCache/issues/16", "body": "I'm trying to run the flux example, installed via node manager, but when loading the example it fails to recognize the node and stays red.\r\n\r\nchecking the output in comfy I see: \r\n  \r\nFile \"C:\\Comfy\\ComfyUI\\custom_nodes\\ComfyUI-TeaCache\\nodes.py\", line 9, in <module>\r\n    from comfy.ldm.hunyuan_video.model import HunyuanVideo\r\nModuleNotFoundError: No module named 'comfy.ldm.hunyuan_video'\r\n\r\nCannot import C:\\Comfy\\ComfyUI\\custom_nodes\\ComfyUI-TeaCache module for custom nodes: No module named 'comfy.ldm.hunyuan_video'\r\n\r\nis there a dependency that I'm missing?", "comments_url": "https://api.github.com/repos/welltop-cn/ComfyUI-TeaCache/issues/16/comments", "author": "apeironn8", "comments": [{"user": "maizhouzi", "created_at": "2025-01-12T08:57:31Z", "body": "Updating ComfyUI to the latest version should resolve the issue."}, {"user": "id-Colin", "created_at": "2025-01-16T00:24:15Z", "body": "Thanks, it's been solved"}, {"user": "id-Colin", "created_at": "2025-01-16T00:24:45Z", "body": "> Updating ComfyUI to the latest version should resolve the issue.\r\n\r\nThanks, it's been solved"}], "satisfaction_conditions": ["A solution that resolves the module import error for 'comfy.ldm.hunyuan_video'", "A straightforward fix that doesn't require complex troubleshooting", "A solution that enables the flux example to run properly"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:05:58"}, "dockerfile": null, "language": "python"}
{"number": 1046, "title": "[BUG] Can't get the MCP tools to work: RuntimeError: Event loop is closed", "created_at": "2025-03-21T16:08:17Z", "closed_at": "2025-03-23T09:56:34Z", "commit_id": "5b2882d4117a6d8a5d50a08b7d56aff1c3a25211", "labels": ["bug"], "url": "https://github.com/huggingface/smolagents/issues/1046", "body": "**Describe the bug**\nI am trying to replace the normal tools by tools coming from a MCP server. My code is runnning inside a poetry venv.\n\n```\nserver_parameters = StdioServerParameters(\n    command=\"uvx\",\n    args=[\"mcp-server-time\"],\n    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n)\nwith ToolCollection.from_mcp(server_parameters) as tool_collection:\n    agent = CodeAgent(\n        tools=[*tool_collection.tools],\n        model=model,\n        prompt_templates=code_prompt_templates,\n        additional_authorized_imports=[\"time\", \"numpy\", \"pandas\", \"json\"],\n    )\nresponse = agent.run(\n    task=\"Answer the user request with the tools you have. User input is: What is the time in Berlin?\"\n)\n```\ngives me\n```\n\n  berlin_time = get_current_time(timezone=\"Europe/Berlin\")                                                                                                                                                                                                      \n  print(berlin_time)                                                                                                                                                                                                                                            \n ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nCode execution failed at line 'berlin_time = get_current_time(timezone=\"Europe/Berlin\")' due to: RuntimeError: Event loop is closed\n[Step 1: Duration 2.95 seconds| Input tokens: 2,330 | Output tokens: 58]\n```\n\nIn another mcp server, I can see that a log message coming from the server \n\n`Processing request of type ListToolsRequest`\n\nSo the server is spawned, but once it tries to access the tool, I get the same error as above\n\n**Code to reproduce the error**\nSee above. Running `npx @modelcontextprotocol/inspector uvx mcp-server-time` I can access the mpc server just fine.\n\n**Error logs (if any)**\nSee above\n\n**Expected behavior**\nThe agent calls the tool\n\n**Packages version:**\nsmolagents==1.12.0\n\n**Additional context**\nAdd any other context about the problem here.\n", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/1046/comments", "author": "wirtsi", "comments": [{"user": "albertvillanova", "created_at": "2025-03-21T17:34:50Z", "body": "Thanks for reporting.\n\nCould you please provide your versions of:\n- mcp\n- mcpadapt\n\nCC: @grll "}, {"user": "wirtsi", "created_at": "2025-03-22T09:10:39Z", "body": "Hey @albertvillanova, thanks for looking into this 😍\n\n```\nmcp==1.4.1\nmcpadapt==0.0.15\n```"}, {"user": "grll", "created_at": "2025-03-22T18:35:50Z", "body": "Hi @wirtsi, thanks for reporting the issue. I will try to reproduce, any chance you are running this in a Jupyter Notebook? Or as a regular python script?"}, {"user": "wirtsi", "created_at": "2025-03-23T08:52:30Z", "body": "No, my code runs in a poetry pyenv (so `poetry run python main.py`)"}, {"user": "grll", "created_at": "2025-03-23T08:57:04Z", "body": "Hmm actually after second thought you need to run your agent.run statement within the context manager otherwise the mcp server is not running. The mcp server / client only runs within the context manager "}, {"user": "grll", "created_at": "2025-03-23T08:58:04Z", "body": "TLDR; just indent your response = ... statement "}, {"user": "wirtsi", "created_at": "2025-03-23T09:56:34Z", "body": "Ah blimey 🤦‍♂️ You are right, it totally makes sense. I thought the context is only needed when instantiating the tools. Thank you 🙏"}, {"user": "phpmac", "created_at": "2025-04-05T07:10:55Z", "body": "How to add multiple mcp services???\n\n"}], "satisfaction_conditions": ["A solution that correctly explains how to use MCP tools with the CodeAgent", "Clarification about the proper scope/lifetime of the MCP server connection", "A simple, actionable fix to the RuntimeError about the closed event loop"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:11:03"}, "dockerfile": "FROM python:3.12-slim\n\n# Set environment variable to avoid prompts during installation\nENV DEBIAN_FRONTEND=noninteractive\nENV UV_SYSTEM_PYTHON=1\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    curl \\\n    make \\\n    build-essential \\\n    nodejs \\\n    npm \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install UV for package management\nRUN pip install --upgrade uv\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/huggingface/smolagents.git . \\\n    && git checkout 5b2882d4117a6d8a5d50a08b7d56aff1c3a25211\n\n# Install the Model Context Protocol CLI tools\nRUN npm install -g @modelcontextprotocol/inspector\n\n# Install the package with all dependencies\n# Include test and all extras to ensure we have everything needed\nRUN uv pip install -e \".[dev,test,all]\"\n\n# Install additional dependencies needed for MCP tools\nRUN uv pip install uvx\n\n# Set the working directory to be ready for use\nWORKDIR /app\n\n# Default command (can be overridden)\nCMD [\"bash\"]", "language": "python"}
{"number": 1165, "title": "[BUG] Cannot execute class method with LocalPythonExecutor", "created_at": "2025-04-09T15:38:30Z", "closed_at": "2025-04-11T07:33:38Z", "commit_id": "3a25900e199b09d69e0681f72bc764987f42c8d0", "labels": ["bug"], "url": "https://github.com/huggingface/smolagents/issues/1165", "body": "**Describe the bug**\nMy LLM write the code in OOP style, defined the class and class methods, when try to execute class method it fails\n\n**Code to reproduce the error**\n```python\nfrom smolagents import LocalPythonExecutor\n\nagent_name = \"Agent\"\nexecutor = LocalPythonExecutor(additional_authorized_imports=[\"json\", \"os\", \"urllib.parse\", \"bs4\", \"pprint\"])\nexecutor.send_variables({\"agent_name\": agent_name})\n\ncode = \"\"\"\nclass Hello: \n    def greetings(self, name: str):\n         return f\"Hello {name}\"\n\nobj = Hello()\nprint(obj.greetings(agent_name))\n\"\"\"\nexecutor(code)\n```\n\n**Error logs (if any)**\n```error\nInterpreterError: Code execution failed at line 'print(obj.greetings(agent_name))' due to: InterpreterError: It is not permitted to evaluate other functions than the provided tools or functions defined/imported in previous code (tried to execute print).\n```\n\n**Expected behavior**\nSuccessfully execute the code\n\n**Packages version:**\n`smolagents==1.12.0`\n\n**Additional context**\nAdd any other context about the problem here.\n", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/1165/comments", "author": "ZeusFSX", "comments": [{"user": "albertvillanova", "created_at": "2025-04-11T07:33:38Z", "body": "Thanks for reaching out.\n\nOnce you created your executor, you need to set the base static tools by calling `send_tools`:\n```python\nexecutor.send_tools({})\n\nexecutor(code)\n```\nOutput:\n```python\n(None, 'Hello Agent\\n', False)\n```"}, {"user": "ZeusFSX", "created_at": "2025-04-11T08:33:56Z", "body": "Thanks"}], "satisfaction_conditions": ["A solution that enables class methods to execute successfully with LocalPythonExecutor", "Clear instructions on the proper initialization sequence for LocalPythonExecutor", "A fix for the 'not permitted to evaluate other functions' error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:10:52"}, "dockerfile": null, "language": "python"}
{"number": 950, "title": "clean duplicate init func ,as it is called in __init__()", "created_at": "2025-03-11T22:25:11Z", "closed_at": "2025-03-12T01:17:51Z", "commit_id": "812c2d2e798701024d0259e3d46ab4f45a228185", "labels": [], "url": "https://github.com/huggingface/smolagents/pull/950", "body": null, "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/950/comments", "author": "zhanluxianshen", "comments": [{"user": "aymeric-roucher", "created_at": "2025-03-12T01:17:51Z", "body": "@zhanluxianshen thank you for raising this: however, since agent tools could have been modified since init, it's still necessary to have this method to re-bake tools description in the system prompt!"}, {"user": "zhanluxianshen", "created_at": "2025-03-12T02:27:12Z", "body": "yep, but i have not found where the ```system_prompt ``` will be changed,  between init and run."}, {"user": "aymeric-roucher", "created_at": "2025-03-12T04:31:31Z", "body": "It's not the system prompt that will be changed, it's the tools that could have been changed, thus the need to refresh sstem prompt!"}, {"user": "zhanluxianshen", "created_at": "2025-03-12T05:07:46Z", "body": "Thanks the explain.\r\n@aymeric-roucher "}], "satisfaction_conditions": ["An explanation of why duplicate code is necessary in this case", "Clarification about when/why components need to be refreshed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:09"}, "dockerfile": null, "language": "python"}
{"number": 791, "title": "[BUG] Rich markup causes crash during error logging", "created_at": "2025-02-25T15:10:32Z", "closed_at": "2025-02-25T15:43:40Z", "commit_id": "df617bb4c1c3956ed327212ed6b17f0df9efe6f3", "labels": ["bug"], "url": "https://github.com/huggingface/smolagents/issues/791", "body": "**Describe the bug**\nRich markup logging interferes with agent output errors when logging to the console.\n\n```python\nclass AgentError(Exception):\n    \"\"\"Base class for other agent-related exceptions\"\"\"\n\n    def __init__(self, message, logger: \"AgentLogger\"):\n        super().__init__(message)\n        self.message = message\n        logger.log(f\"[bold red]{message}[/bold red]\", level=\"ERROR\") # --- ISSUE HERE ---\n\n```\n\nThis is problematic because it causes continuous crashes and terminates execution.\nThe issue occurs when message contains Rich markup syntax, such as closing tags.\n\n**Code to reproduce the error**\n```python\nfrom smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel\n\nmodel = HfApiModel()\nagent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\nagent.run(\"You have to print this: ^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]\")\n```\n\n\n**Error logs (if any)**\n```bash\n  File \"/Users/someone/.pyenv/versions/3.11.10/envs/fancy-project/lib/python3.11/site-packages/rich/markup.py\", line 167, in render\n    raise MarkupError(\nrich.errors.MarkupError: closing tag '[/:]' at position 99 doesn't match any open tag\n```\n\n**Expected behavior**\nIt should log error as usual without crashing.\n\n**Packages version:**\n`smolagents==1.9.2`\n\n**Additional context**\n\nDebugging, this is an example of the content of `{message}` that is generating the issue.\n```bash\nExecution logs:\nError listing files in S3 bucket mybucket/source: Parameter validation failed:\nInvalid bucket name \"mybucket/source\": Bucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN matching the regex \n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:][a-zA-Z0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:][a-zA-Z0-9\\-]{1,63}[/:]accesspoint[/:][\na-zA-Z0-9\\-]{1,63}$\"\nError listing files in S3 bucket mybucket/source: Parameter validation failed:\nInvalid bucket name \"mybucket/source\": Bucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN matching the regex \n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:][a-zA-Z0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:][a-zA-Z0-9\\-]{1,63}[/:]accesspoint[/:][\na-zA-Z0-9\\-]{1,63}$\"\n```", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/791/comments", "author": "albertorb", "comments": [{"user": "albertorb", "created_at": "2025-02-25T15:12:28Z", "body": "If needed, I’m available to help investigate further, provide additional information, or even submit a PR to fix the issue. Since I’m not sure how you’d like to tackle the problem, I haven’t provided a specific solution, as there are multiple possible approaches."}, {"user": "albertvillanova", "created_at": "2025-02-25T15:43:40Z", "body": "Thanks for reporting.\n\nI think the issue was fixed in:\n- #753\n\nThe fix is not released yet, so you need to install smolagents from the main branch.\n\n```python\nIn [1]: from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel\n   ...: \n   ...: model = HfApiModel()\n   ...: agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\n   ...: agent.run(\"You have to print this: ^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]\")\n╭──────────────────────────────────────────────────────────────────────────────────────────────── New run ────────────────────────────────────────────────────────────────────────────────────────────────╮\n│                                                                                                                                                                                                         │\n│ You have to print this: ^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9\\]*:[0-9\\]{12}:accesspoint[/:\\]                                                                                                     │\n│                                                                                                                                                                                                         │\n╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n ─ Executing parsed code: ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  print(\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]\")                                                                                                                        \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nExecution logs:\n^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]\n\nOut: None\n[Step 1: Duration 16.09 seconds| Input tokens: 2,112 | Output tokens: 86]\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 2 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n ─ Executing parsed code: ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  final_answer(\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]\")                                                                                                                 \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── \nOut - Final answer: ^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]\n[Step 2: Duration 6.37 seconds| Input tokens: 4,449 | Output tokens: 164]\nOut[1]: '^arn:(aws).*:(s3|s3-object-lambda):[a-z\\\\-0-9]*:[0-9]{12}:accesspoint[/:]'\n```"}, {"user": "albertorb", "created_at": "2025-02-25T15:54:41Z", "body": "Thanks for the quick reply, @albertvillanova ! Apologies for the duplication."}], "satisfaction_conditions": ["A solution that prevents Rich markup in error messages from causing crashes", "Ability to log error messages containing special characters without application termination", "Information about when the fix will be available in a released version"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:16"}, "dockerfile": null, "language": "python"}
{"number": 773, "title": "Fix duplicate Tool Issue in Open Deep Research", "created_at": "2025-02-24T18:42:02Z", "closed_at": "2025-02-26T06:06:18Z", "commit_id": "44f4336fd9ca98e121bd8f7a86c0a09937160c29", "labels": [], "url": "https://github.com/huggingface/smolagents/pull/773", "body": "This PR fixes the following error when running Open Deep Research:\r\n`ValueError: Each tool or managed_agent should have a unique name! You passed these duplicate names: ['inspect_file_as_text', 'inspect_file_as_text']`\r\n", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/773/comments", "author": "keetrap", "comments": [{"user": "keetrap", "created_at": "2025-02-24T18:58:04Z", "body": "Currently, we are passing the `SERPAPI_API_KEY`  in the `BROWSER_CONFIG` as follows:\r\n\r\n```python\r\nBROWSER_CONFIG = {\r\n    \"viewport_size\": 1024 * 5,\r\n    \"downloads_folder\": \"downloads_folder\",\r\n    \"request_kwargs\": {\r\n        \"headers\": {\"User-Agent\": user_agent},\r\n        \"timeout\": 300,\r\n    },\r\n    \"serpapi_key\": os.getenv(\"SERPAPI_API_KEY\"),\r\n}\r\n```\r\nHowever, in the `WEB_TOOLS`, the` GoogleSearchTool` is using the `serper` provider, which requires the `SERPER_API_KEY`:\r\n```python \r\nGoogleSearchTool(provider=\"serper\")\r\n```\r\n\r\nThis requires users to provide two different API keys. Is this intentional, or should we be using the same provider throughout?\r\n\r\nSimple Fix:\r\n```python \r\nGoogleSearchTool(provider=\"serpapi\")\r\n```\r\n"}, {"user": "aymeric-roucher", "created_at": "2025-02-25T16:31:01Z", "body": "@keetrap initializing these agents should not create a duplicate: since one of the `inspect_file_as_text` belongs to the managed agent and the other belongs to the manager, there's no risk of confusion. We need to change this in the smolagents code."}, {"user": "keetrap", "created_at": "2025-02-25T18:29:56Z", "body": "> @keetrap initializing these agents should not create a duplicate: since one of the `inspect_file_as_text` belongs to the managed agent and the other belongs to the manager, there's no risk of confusion. We need to change this in the smolagents code.\r\n\r\nThanks for the review.\r\n\r\nIn PR #721, I've updated the name validation logic to properly handle cases where a managed agent and its tool can have the same name.\r\n```python\r\n#Updated in PR 721\r\ntool_and_managed_agent_names = [tool.name for tool in tools]\r\nif managed_agents is not None:\r\n    for agent in managed_agents:\r\n        tool_and_managed_agent_names.append(agent.name)\r\n        for tool in agent.tools.values():\r\n            if tool.name != \"final_answer\":\r\n                tool_and_managed_agent_names.append(tool.name)\r\n```\r\nNow we can improve this further\r\n```python\r\n    def _validate_tools_and_managed_agents(self, tools, managed_agents):\r\n        tool_and_managed_agent_names = [tool.name for tool in tools]\r\n        if managed_agents is not None:\r\n            for agent in managed_agents:\r\n                tool_and_managed_agent_names.append(agent.name)\r\n                agent_and_tools_names = [agent.name]\r\n                for tool in agent.tools.values():\r\n                    if tool.name != \"final_answer\":\r\n                        agent_and_tools_names.append(tool.name)\r\n                if len(agent_and_tools_names) != len(set(agent_and_tools_names)):\r\n                    raise ValueError(\r\n                        \"Each managed_agent and its tools should have a unique name! You passed these duplicate names: \"\r\n                        f\"{[name for name in agent_and_tools_names if agent_and_tools_names.count(name) > 1]}\"\r\n                    )        \r\n        if len(tool_and_managed_agent_names) != len(set(tool_and_managed_agent_names)):\r\n            raise ValueError(\r\n                \"Each tool or managed_agent should have a unique name! You passed these duplicate names: \"\r\n                f\"{[name for name in tool_and_managed_agent_names if tool_and_managed_agent_names.count(name) > 1]}\"\r\n            )\r\n ```\r\n "}, {"user": "keetrap", "created_at": "2025-02-26T06:06:18Z", "body": "Closing this due to #796 "}], "satisfaction_conditions": ["A solution that prevents duplicate tool name errors between managed agents and their tools", "Proper validation logic that distinguishes between global tool names and tool names within managed agents", "Clear error messages that identify the specific duplicate names causing validation failures"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:23"}, "dockerfile": null, "language": "python"}
{"number": 454, "title": "fix: support o1", "created_at": "2025-01-31T14:48:39Z", "closed_at": "2025-02-05T10:44:42Z", "commit_id": "e26aed68e819629299243db3f69b4e08eed33745", "labels": [], "url": "https://github.com/huggingface/smolagents/pull/454", "body": "Remove `max_tokens` for `o1` models", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/454/comments", "author": "ricklamers", "comments": [{"user": "ricklamers", "created_at": "2025-01-31T14:49:06Z", "body": "I couldn't get it working without these changes. Maybe this needs to be changed, but this is working for me."}, {"user": "aymeric-roucher", "created_at": "2025-02-05T10:44:42Z", "body": "This fix is not needed anymore! Now that we've removed the default parameter `max_tokens`, the model works out of the box for me. Tell us if you still have errors and we'll reopen!"}, {"user": "ricklamers", "created_at": "2025-02-05T13:39:50Z", "body": "Nice!"}], "satisfaction_conditions": ["A working solution for using o1 models without parameter conflicts", "Elimination of the need for manual parameter adjustments when using o1 models"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:29"}, "dockerfile": null, "language": "python"}
{"number": 108, "title": "Always getting the error: \"AssertionError exception: no description\" ", "created_at": "2025-01-07T21:09:23Z", "closed_at": "2025-01-20T16:21:39Z", "commit_id": "681758ae84a8075038dc676d8af7262077bd00c3", "labels": [], "url": "https://github.com/huggingface/smolagents/issues/108", "body": "No matter what I do to modify the docstring I always get the same error as mentioned in the title.\r\n\r\nHere is a tool that I have created.\r\n\r\nI would like to know what within my docstrings is causing this.\r\n\r\n```python\r\n\r\ncg = CoinGeckoAPI(demo_api_key=os.getenv('coingecko_api_key'))\r\n\r\n@tool\r\ndef get_coins_list(currency: str) -> list:\r\n    \"\"\"\r\n    This tool makes a query to the CoinGecko API to get a response of ALL of the supported coins with their price, market cap, volume and related market data in USD.\r\n\r\n    Args:\r\n        currency: The dollar value which the coin should be represented into\r\n    \"\"\"\r\n    return cg.get_coins_markets(vs_currency=currency)\r\n\r\n```", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/108/comments", "author": "jondoescoding", "comments": [{"user": "whoahaow", "created_at": "2025-01-07T21:34:16Z", "body": "does it fix it?\r\n\r\n```python\r\ncg = CoinGeckoAPI(api_key=os.getenv('coingecko_api_key'))\r\n\r\nclass GetCoinsListTool(Tool):\r\n    name = \"get_coins_list\"\r\n    description = \"\"\"\r\n    This tool makes a query to the CoinGecko API to get a response of ALL of the supported coins with their price, market cap, volume and related market data in USD.\r\n    \"\"\"\r\n    inputs = {\r\n        \"currency\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The currency in which the coin data should be represented (e.g., 'usd', 'eur').\"\r\n        }\r\n    }\r\n    output_type = \"list\"\r\n\r\n    def forward(self, currency: str) -> list:\r\n        return cg.get_coins_markets(vs_currency=currency)\r\n```"}, {"user": "jondoescoding", "created_at": "2025-01-07T21:45:48Z", "body": "Got the same error.\r\n\r\n```python\r\nException has occurred: AssertionError\r\nexception: no description\r\n\r\nException has occurred: AssertionError\r\nexception: no description\r\n  File \"...\\coingecko_agent\\agent.py\", line 7, in <module>\r\n    coin_list_tool = GetCoinsListTool()\r\n                     ^^^^^^^^^^^^^^^^^^\r\nAssertionError: \r\n\r\n```"}, {"user": "whoahaow", "created_at": "2025-01-07T22:25:44Z", "body": "I don't know if this is suitable for you, but here's what I did:\r\n```python\r\nfrom smolagents import CodeAgent, HfApiModel, Tool\r\nimport os\r\nfrom pycoingecko import CoinGeckoAPI\r\nimport json\r\n\r\n# Initialize CoinGecko API client\r\ncg = CoinGeckoAPI(api_key=os.getenv('coingecko_api_key'))\r\n\r\n# Define the GetCoinsListTool class\r\nclass GetCoinsListTool(Tool):\r\n    name = \"get_coins_list\"\r\n    description = \"\"\"\r\n    This tool makes a query to the CoinGecko API to get a response of ALL of the supported coins with their price, market cap, volume and related market data in USD.\r\n    You need to import json. The output is a JSON string. You should use the `json` module to parse this string into a Python list.\r\n    \"\"\"\r\n    inputs = {\r\n        \"currency\": {\r\n            \"type\": \"string\",\r\n            \"description\": \"The currency in which the coin data should be represented (e.g., 'usd', 'eur').\"\r\n        }\r\n    }\r\n    output_type = \"string\"  # Change to 'string'\r\n\r\n    def forward(self, currency: str) -> str:\r\n        coins_data = cg.get_coins_markets(vs_currency=currency)\r\n        return json.dumps(coins_data)  # Convert the list to a JSON string\r\n\r\n# Initialize the model\r\nmodel = HfApiModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\r\n\r\n# Initialize the agent with the tool\r\nagent = CodeAgent(\r\n    tools=[GetCoinsListTool()],\r\n    model=model,\r\n    add_base_tools=True,\r\n    additional_authorized_imports=[\"json\"]  # Authorize the json module\r\n)\r\n\r\n# Run the agent with a task\r\ntask = \"Get the list of coins in USD and print the first 5 entries. Then present it as usual text.\"\r\nresult = agent.run(task)\r\n\r\n# Print the result\r\nprint(\"Agent Output:\")\r\nprint(result)\r\n```"}, {"user": "jondoescoding", "created_at": "2025-01-07T23:24:32Z", "body": "Works like a charm. Thanks! But why does the the @tool decorator not work?"}, {"user": "aymeric-roucher", "created_at": "2025-01-09T10:24:13Z", "body": "@jondoescoding could you provide your full error trace and package versions? I tried to reproduce but for me your code snippet works"}], "satisfaction_conditions": ["A working solution that resolves the 'AssertionError: no description' error", "A functional way to create a CoinGecko API tool that can be used with their agent"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:34"}, "dockerfile": null, "language": "python"}
{"number": 101, "title": "Code Agent -> max_iterations", "created_at": "2025-01-07T10:20:29Z", "closed_at": "2025-01-07T14:53:36Z", "commit_id": "d09d4c3a545b96b99a4d285aacd608025d6d1a13", "labels": [], "url": "https://github.com/huggingface/smolagents/issues/101", "body": "CodeAgent stops after 5 iterations. Any way to explicitely increase this limit ?", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/101/comments", "author": "flaming-potato", "comments": [{"user": "paulmartrencharpro", "created_at": "2025-01-07T10:32:15Z", "body": "Yes, the CodeAgent class' parent MultiStepAgent has a max_steps parameter that you can change.\r\n\r\n`agent = CodeAgent(tools=[DuckDuckGoSearchTool(), PythonInterpreterTool()], model=HfApiModel(), max_steps=20)`"}, {"user": "aymeric-roucher", "created_at": "2025-01-07T14:53:36Z", "body": "Closing this since @paulmartrencharpro explained the resolution very well! 😄 "}, {"user": "flaming-potato", "created_at": "2025-01-07T15:46:06Z", "body": "Thanks a lot :)"}], "satisfaction_conditions": ["Information on how to increase the iteration limit for CodeAgent", "A practical, implementable solution that can be directly applied", "Clear explanation of which parameter controls the iteration limit"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:38"}, "dockerfile": null, "language": "python"}
{"number": 44, "title": "LLM using wrong function to send a request to an agent", "created_at": "2025-01-02T23:30:08Z", "closed_at": "2025-01-09T22:54:20Z", "commit_id": "e57f4f55ef506948d2e17b320ddc2a98b282eacf", "labels": [], "url": "https://github.com/huggingface/smolagents/issues/44", "body": "Notice in `Step 0`, it tried to call `home_automation.request`, gets an error, then calls the correct function `home_automation()`\r\n\r\n```bash\r\nroot# python demo.py \r\nYou: turn on the kitchen light plz\r\n╭───────────────────────────────── New run ─────────────────────────────────╮\r\n│                                                                           │\r\n│ turn on the kitchen light plz                                             │\r\n│                                                                           │\r\n╰─ LiteLLMModel - gpt-4o-mini ──────────────────────────────────────────────╯\r\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\r\n╭─ Executing this code: ────────────────────────────────────────────────────╮\r\n│   1 home_automation.request(\"Please turn on the kitchen light.\")          │\r\n╰───────────────────────────────────────────────────────────────────────────╯\r\nCode execution failed: Code execution failed at line \r\n'home_automation.request(\"Please turn on the kitchen light.\")' because of the\r\nfollowing error:\r\nObject <smolagents.agents.ManagedAgent object at 0x7d83aaf84ce0> has no \r\nattribute request\r\n[Step 0: Duration 1.71 seconds| Input tokens: 2,018 | Output tokens: 61]\r\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\r\n╭─ Executing this code: ────────────────────────────────────────────────────╮\r\n│   1 home_automation(\"turn on the kitchen light\")                          │\r\n╰───────────────────────────────────────────────────────────────────────────╯\r\n╭───────────────────────────────── New run ─────────────────────────────────╮\r\n│                                                                           │\r\n│ You're a helpful agent named 'home_automation'.                           │\r\n│ You have been submitted this task by your manager.                        │\r\n│ ---                                                                       │\r\n│ Task:                                                                     │\r\n│ turn on the kitchen light                                                 │\r\n│ ---                                                                       │\r\n│ You're helping your manager solve a wider task: so make sure to not       │\r\n│ provide a one-line answer, but give as much information as possible to    │\r\n│ give them a clear understanding of the answer.                            │\r\n│                                                                           │\r\n│ Your final_answer WILL HAVE to contain these parts:                       │\r\n│ ### 1. Task outcome (short version):                                      │\r\n│ ### 2. Task outcome (extremely detailed version):                         │\r\n│ ### 3. Additional context (if relevant):                                  │\r\n│                                                                           │\r\n│ Put all these in your final_answer tool, everything that you do not pass  │\r\n│ as an argument to final_answer will be lost.                              │\r\n│ And even if your task resolution is not successful, please return as much │\r\n│ context as possible, so that your manager can act upon this feedback.     │\r\n│ {additional_prompting}                                                    │\r\n│                                                                           │\r\n╰─ LiteLLMModel - gpt-4o-mini ──────────────────────────────────────────────╯\r\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 0 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\r\n╭───────────────────────────────────────────────────────────────────────────╮\r\n│ Calling tool: 'turn_on_light' with arguments: {'light': 'kitchen'}        │\r\n╰───────────────────────────────────────────────────────────────────────────╯\r\n```\r\n\r\n\r\nHere's my code:\r\n\r\n```python\r\nhome_automation_agent = ToolCallingAgent(\r\n    tools=[turn_on_light],\r\n    model=model,\r\n)\r\n\r\nmanaged_home_automation_agent = ManagedAgent( \r\n    agent=home_automation_agent,\r\n    name=\"home_automation\",\r\n    description=\"Controls the home automation system.\"\r\n)\r\n\r\nsms_agent = ToolCallingAgent(\r\n    tools=[send_sms, phone_number_lookup],\r\n    model=model,\r\n)\r\n\r\nmanaged_sms_agent = ManagedAgent( \r\n    agent=sms_agent,\r\n    name=\"sms\",\r\n    description=\"Sends text messages.\"\r\n)\r\n\r\nagent = CodeAgent(\r\n    model=model,\r\n    tools=[],\r\n    managed_agents=[managed_home_automation_agent, managed_sms_agent],\r\n)\r\n```", "comments_url": "https://api.github.com/repos/huggingface/smolagents/issues/44/comments", "author": "vqndev", "comments": [{"user": "aymeric-roucher", "created_at": "2025-01-09T22:54:20Z", "body": "Hi @vqndev, thank you for submitting!\r\nThis is a great example of self-healing in a multi-step agent!\r\nIn step 0, `gpt4o-mini` does a mistake by calling a tool incorrectly. Then it rectifies the code in the second step (because it saw the error message in its memory) and finally solves the task!\r\n\r\nAll in all it's not a framework issue, it's just the LLM being dumb! Which often happens with smaller LLMs, GPT-4o would be less likely to do this first mistake."}, {"user": "vqndev", "created_at": "2025-01-13T15:41:14Z", "body": "Thanks! "}], "satisfaction_conditions": ["Acknowledgment that the observed behavior is expected and not a framework bug", "Understanding of why the LLM initially used the wrong function call syntax", "Recognition of the self-healing behavior as a feature rather than a bug"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:47"}, "dockerfile": null, "language": "python"}
{"number": 95, "title": "Issue installing on ARM 64 MacOS 151.1", "created_at": "2024-12-07T01:06:13Z", "closed_at": "2024-12-24T19:58:19Z", "commit_id": "f93ff45936c055a7b1f8e15bc3ce16a7fa29868d", "labels": ["bug", "triaged"], "url": "https://github.com/thousandbrainsproject/tbp.monty/issues/95", "body": "### Describe the bug\n\nFollowing the install instructions for Anaconda and Apple Silicon Mac mini M2 I see the following error with the \r\n```conda env create```\r\nstep. I am getting the following error dump\r\n\r\n```(base) john@john-macmini tbp.monty % conda env create -f environment_arm64.yml  \r\nChannels:\r\n - aihabitat\r\n - pytorch\r\n - pyg\r\n - defaults\r\n - conda-forge\r\nPlatform: osx-arm64\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed\r\n\r\nLibMambaUnsatisfiableError: Encountered problems while solving:\r\n  - nothing provides requested aihabitat::habitat-sim =0.2.2*\r\n  - nothing provides requested mkl <2022\r\n  - nothing provides blas * mkl needed by pytorch-1.11.0.arm64-py3.8_0\r\n\r\nCould not solve for environment specs\r\nThe following packages are incompatible\r\n├─ habitat-sim 0.2.2*  does not exist (perhaps a typo or a missing channel);\r\n├─ mkl <2022  does not exist (perhaps a typo or a missing channel);\r\n└─ pytorch 1.11.0*  is not installable because it requires\r\n   └─ blas * mkl, which does not exist (perhaps a missing channel).\r\n```\r\n\r\nI am running the following Python version\r\n```(base) john@john-macmini tbp.monty % python --version\r\nPython 3.12.7\r\n```\r\n", "comments_url": "https://api.github.com/repos/thousandbrainsproject/tbp.monty/issues/95/comments", "author": "doonhammer", "comments": [{"user": "tristanls", "created_at": "2024-12-09T18:31:23Z", "body": "Hi @doonhammer, I'm sorry that the install instructions are not working.\r\n\r\nOn Apple's ARM 64, the Anaconda instructions require to specify the environment file explicitly:\r\n```\r\nconda env create -f environment_arm64.yml\r\n```\r\nIs the above command the one that generated the error?\r\n\r\nIf you still get the same error even when specifying `-f environment_arm64.yml`, this may be a similar problem described in #81. There, the solution seemed to be to use the Miniconda setup instructions (if you're using `zsh` shell):\r\n```\r\nconda env create -f environment_arm64.yml --subdir=osx-64\r\nconda init zsh\r\nconda activate tbp.monty\r\nconda config --env --set subdir osx-64\r\n```\r\nOr if you're using `bash` or some other shell:\r\n```\r\nconda env create -f environment_arm64.yml --subdir=osx-64\r\nconda init\r\nconda activate tbp.monty\r\nconda config --env --set subdir osx-64\r\n```"}, {"user": "doonhammer", "created_at": "2024-12-11T00:53:12Z", "body": "Adding --subdir=osx-64 did the trick with Anaconda install. Install succeeded and all tests passed. Thanks for the help"}], "satisfaction_conditions": ["A working installation method for ARM 64 MacOS that resolves package compatibility issues", "A solution that works with the user's existing Anaconda setup", "A complete installation process that allows tests to pass"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:12:05"}, "dockerfile": "FROM python:3.8-slim\n\n# Set environment variables\nENV PYTHONUNBUFFERED=1 \\\n    PYTHONDONTWRITEBYTECODE=1\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    wget \\\n    ca-certificates \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Miniconda\nWORKDIR /tmp\nRUN wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh && \\\n    bash miniconda.sh -b -p /opt/conda && \\\n    rm miniconda.sh\n\n# Add conda to PATH\nENV PATH=\"/opt/conda/bin:${PATH}\"\n\n# Create workspace directory\nWORKDIR /workspace\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/thousandbrainsproject/tbp.monty.git && \\\n    cd tbp.monty && \\\n    git checkout f93ff45936c055a7b1f8e15bc3ce16a7fa29868d\n\n# Set working directory to the cloned repository\nWORKDIR /workspace/tbp.monty\n\n# Create a modified environment file for compatibility\nRUN echo \"name: tbp.monty\" > environment_modified.yml && \\\n    echo \"channels:\" >> environment_modified.yml && \\\n    echo \"  - pytorch\" >> environment_modified.yml && \\\n    echo \"  - conda-forge\" >> environment_modified.yml && \\\n    echo \"  - defaults\" >> environment_modified.yml && \\\n    echo \"dependencies:\" >> environment_modified.yml && \\\n    echo \"  - python=3.8\" >> environment_modified.yml && \\\n    echo \"  - pytorch=1.13.0\" >> environment_modified.yml && \\\n    echo \"  - torchvision\" >> environment_modified.yml && \\\n    echo \"  - pip\" >> environment_modified.yml && \\\n    echo \"  - pip:\" >> environment_modified.yml && \\\n    echo \"    - -e .\" >> environment_modified.yml\n\n# Create and activate the conda environment\nRUN conda env create -f environment_modified.yml && \\\n    conda clean -afy\n\n# Add conda environment activation to bash startup\nSHELL [\"/bin/bash\", \"-c\"]\nRUN echo \"source activate tbp.monty\" > ~/.bashrc\n\n# Set the default command to activate the conda environment\nCMD [\"/bin/bash\", \"-c\", \"source activate tbp.monty && /bin/bash\"]", "language": "python"}
{"number": 479, "title": "Is there a way to force handoffs?", "created_at": "2025-04-11T01:13:36Z", "closed_at": "2025-04-11T02:49:10Z", "commit_id": "68c725f9425ab371e8774e50319e18da61ef2e80", "labels": ["question"], "url": "https://github.com/openai/openai-agents-python/issues/479", "body": "### Question\nIs there a way to force handoffs to other agents similar to how we can do it for tools by making model_setting `tool_choice` to `\"required\"`? Is the only current way to do this essentially to make the agent a tool and `tool_choice` to `\"required\"`?", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/479/comments", "author": "dylee9", "comments": [{"user": "rohan-mehta", "created_at": "2025-04-11T02:47:09Z", "body": "Set tool choice to the name of the handoff tool (which you can get from `Handoff.default_tool_name()` or `handoff.tool_name`)"}, {"user": "dylee9", "created_at": "2025-04-11T02:49:10Z", "body": "Perfect!"}], "satisfaction_conditions": ["A specific method to force agent handoffs similar to how tool usage can be forced", "A direct, concise approach that doesn't require converting agents to tools", "Information about specific parameter settings or configuration options to control handoff behavior"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:37"}, "dockerfile": null, "language": "python"}
{"number": 413, "title": "ImportError: cannot import name 'MCPServerSse' from 'agents.mcp'", "created_at": "2025-04-01T12:45:09Z", "closed_at": "2025-04-07T04:40:23Z", "commit_id": "9c53abe8c15ab2cf1c5591c1db1f61b52a1b24dc", "labels": ["bug", "needs-more-info"], "url": "https://github.com/openai/openai-agents-python/issues/413", "body": "Traceback (most recent call last):\nFile \"C:\\Users\\Lenovo\\Desktop\\Strats AI\\open ai sdk\\main.py\", line 4, in\nfrom agents.mcp import MCPServerSse, MCPServerStdio\nImportError: cannot import name 'MCPServerSse' from 'agents.mcp' (C:\\Users\\Lenovo\\Desktop\\Strats AI\\open ai sdk\\venv\\Lib\\site-packages\\agents\\mcp_init_.py)\n\nThis is the error i am facing despite creating the venv and installing the latest version of the open ai sdk", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/413/comments", "author": "oms0401", "comments": [{"user": "rm-openai", "created_at": "2025-04-01T15:45:25Z", "body": "Are you on Python 3.9? Can you post the full error/stack trace?"}, {"user": "smortezah", "created_at": "2025-04-02T08:50:01Z", "body": "Same for Python 3.12.\n\n```\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[2], line 1\n----> 1 from agents.mcp import MCPServerStdio\n      2 samples_dir='.'\n      4 async with MCPServerStdio(\n      5     params={\n      6         \"command\": \"npx\",\n      7         \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n      8     }\n      9 ) as server:\n\nImportError: cannot import name 'MCPServerStdio' from 'agents.mcp' (.venv/lib/python3.12/site-packages/agents/mcp/__init__.py)\n```"}, {"user": "limingyang325", "created_at": "2025-04-02T11:29:14Z", "body": "\n> Are you on Python 3.9? Can you post the full error/stack trace?\nI am using Python 3.9, and I encountered the same issue.\n"}, {"user": "rm-openai", "created_at": "2025-04-02T15:13:06Z", "body": "Can you try `from agents.mcp.server import MCPServerSse` and tell me what error you see?\n\nAlso this wont work on Python 3.9, as MCP support requires 3.10+"}, {"user": "smortezah", "created_at": "2025-04-02T15:34:07Z", "body": "> Can you try `from agents.mcp.server import MCPServerSse` and tell me what error you see?\n\nNot working.\n\n```\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 1\n----> 1 from agents.mcp.server import MCPServerSse\n\nFile ~/.venv/lib/python3.12/site-packages/agents/mcp/server.py:10\n      7 from typing import Any, Literal\n      9 from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n---> 10 from mcp import ClientSession, StdioServerParameters, Tool as MCPTool, stdio_client\n     11 from mcp.client.sse import sse_client\n     12 from mcp.types import CallToolResult, JSONRPCMessage\n\nFile ~/mcp.py:6\n      3 import shutil\n      5 from agents import Agent, Runner\n----> 6 from agents.mcp import MCPServer, MCPServerStdio\n      9 async def run(mcp_server: MCPServer):\n     10     agent = Agent(\n     11         name=\"Assistant\",\n     12         instructions=\"Use the tools to read the filesystem and answer questions based on those files.\",\n     13         mcp_servers=[mcp_server],\n     14     )\n\nImportError: cannot import name 'MCPServer' from 'agents.mcp' (.venv/lib/python3.12/site-packages/agents/mcp/__init__.py)\n```"}, {"user": "rm-openai", "created_at": "2025-04-02T15:36:54Z", "body": "@smortezah thanks for bearing with me - can you try running this and telling me what you see?\n```\nimport importlib.metadata\nimport os\nimport sys\n\nprint(sys.version)\ntry:\n    print(importlib.metadata.version(\"agents\"))\nexcept Exception:\n    print(\"-1\")\n\ntry:\n    import mcp\n\n    print(dir(mcp))\nexcept Exception:\n    print(\"mcp not found\")\n\nagents_dir = importlib.import_module(\"agents\").__path__[0]\nprint(str(agents_dir))\n\n\nmcp_file = os.path.join(str(agents_dir), \"mcp\", \"__init__.py\")\nwith open(mcp_file) as f:\n    print(f.read())\n```"}, {"user": "smortezah", "created_at": "2025-04-02T15:45:18Z", "body": "```\n3.12.9 (main, Feb  5 2025, 18:58:23) [Clang 19.1.6 ]\n-1\nmcp not found\n~/.venv/lib/python3.12/site-packages/agents\ntry:\n    from .server import (\n        MCPServer,\n        MCPServerSse,\n        MCPServerSseParams,\n        MCPServerStdio,\n        MCPServerStdioParams,\n    )\nexcept ImportError:\n    pass\n\nfrom .util import MCPUtil\n\n__all__ = [\n    \"MCPServer\",\n    \"MCPServerSse\",\n    \"MCPServerSseParams\",\n    \"MCPServerStdio\",\n    \"MCPServerStdioParams\",\n    \"MCPUtil\",\n]\n```"}, {"user": "rm-openai", "created_at": "2025-04-02T15:53:20Z", "body": "@smortezah How did you install the `openai-agents` package? Seems like somehow the MCP dep didnt get pulled in.\n\nCan you also try\n```\nimport importlib.metadata\nprint(importlib.metadata.version(\"openai-agents\")\n```\n\nand reinstalling the package via\n```\npip uninstall openai-agents\npip install openai-agents\n```"}, {"user": "smortezah", "created_at": "2025-04-02T16:01:50Z", "body": "@rm-openai I installed it with `uv add \"openai-agents[viz]\"`.\n\n```\nimport importlib.metadata\nprint(importlib.metadata.version(\"openai-agents\")\n```\n0.0.7\n\nAlso, none of the followings worked:\n```\nuv remove \"openai-agents[viz]\"\nuv add \"openai-agents[viz]\"\n```\nand\n```\nuv remove openai-agents\nuv add openai-agents\n```\n\nHOWEVER, it works when I use `pip` instead of `uv`:\n```\nbrew install python@3.12\npython3.12 -m venv venv3.12\nsource venv3.12/bin/activate\npip install openai-agents\n\npython -c \"from agents.mcp.server import MCPServerSse\"\n```"}, {"user": "rm-openai", "created_at": "2025-04-02T16:10:40Z", "body": "@smortezah it sounds like you might not be using `uv run` when you install via uv. This worked fine for me:\n\n```\nmkdir test_mcp && cd test_mcp && uv init .\n\nuv add \"openai-agents[viz]\" && uv run python -c \"from agents.mcp.server import MCPServerSse\"\n```"}, {"user": "smortezah", "created_at": "2025-04-02T16:19:05Z", "body": "@rm-openai I guess I found the source of issue. If I put a python file that only includes `from agents.mcp.server import MCPServerSse` in the root directory of my project, it works. However, if I put this file in a subdirectory, it stops working regardless of where I call this file from; that is, whether I run `python a.py` or `uv run a.py` from the root directory or from within the subdirectory, it throws the error."}, {"user": "smortezah", "created_at": "2025-04-04T15:52:27Z", "body": "@rm-openai @oms0401 Solved.\n\nI encountered an interesting situation where I had a file named `mcp.py` in my subdirectory. Attempting to import `from mcp` resulted in a circular import. Interestingly, I wasn’t importing from `mcp` in my Jupyter notebook or the Python file I was trying to execute. However, the presence of `mcp.py` in the directory led to the following error:\n`ImportError: cannot import name ‘MCPServer’ from ‘agents.mcp’ (.venv/lib/python3.12/site-packages/agents/mcp/__init__.py)`\n\nTo resolve this issue, I simply renamed `mcp.py`."}, {"user": "rm-openai", "created_at": "2025-04-04T19:17:51Z", "body": "Wow that is kinda crazy. Makes sense though."}, {"user": "oms0401", "created_at": "2025-04-07T04:40:23Z", "body": "Yes the issue is solved right now but the sdk is not stable in the current python 3.12 versio"}, {"user": "ycjcl868", "created_at": "2025-04-11T09:18:16Z", "body": "Same for Python 3.12.\n\n"}], "satisfaction_conditions": ["Identification of the root cause of the import error", "A practical solution to resolve the import error", "Clarification on Python version compatibility", "Understanding of package installation methods that work correctly"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:41"}, "dockerfile": null, "language": "python"}
{"number": 394, "title": "Wait to run the Agent first step until Input guardrails is complete", "created_at": "2025-03-29T22:39:44Z", "closed_at": "2025-04-01T15:47:57Z", "commit_id": "382500d841680e3ccd727001639394e6a6697a9a", "labels": [], "url": "https://github.com/openai/openai-agents-python/pull/394", "body": "The guardrail may trigger a Tripewire but the agent still run\r\n\r\nIn v0.0.7, the code runs the input guardrail task and the agent's first step together asynchronously. \r\n\r\nIf the guardrail is in place to avoid a side effect from the Agent run, the agent may still do something unexpected.\r\n\r\n\r\nLet's say that you have a tripwire guardrail: \"Don't allow to delete a file\", and the agent is an MCP file system agent", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/394/comments", "author": "rach", "comments": [{"user": "rm-openai", "created_at": "2025-04-01T15:47:57Z", "body": "the current behavior is the intended/documented behavior. If you want to run the guardrail _before_ the agent runs, you can just do that via python code:\r\n```\r\ntriggered = await do_something()\r\nif not triggered:\r\n  await Runner.run(...)\r\n```"}, {"user": "rach", "created_at": "2025-04-05T16:44:14Z", "body": "For my own understanding. What was the rational to have the agent start before the guardrail is done?\nI can work around it, now that I know the behavior. "}, {"user": "rm-openai", "created_at": "2025-04-08T18:02:16Z", "body": "@rach for latency. If the guardrail doesn't fail, then you will have made a bunch of progress on the actual agent. RUnning (guardrail, agent) in parallel is much faster than serially"}], "satisfaction_conditions": ["Explanation of the design rationale behind the current behavior", "Clarification of the current system behavior regarding guardrails and agent execution", "Practical workaround for implementing sequential guardrail-then-agent execution"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:45"}, "dockerfile": null, "language": "python"}
{"number": 373, "title": "Issue with incomplete pip installation - macOS", "created_at": "2025-03-27T17:21:20Z", "closed_at": "2025-03-27T17:42:44Z", "commit_id": "377ddbace60ea37f7651c16fc59667f717a44e03", "labels": ["question"], "url": "https://github.com/openai/openai-agents-python/issues/373", "body": "Description:\nI'm encountering an issue when trying to install the openai-agents package. After a successful installation via pip, the package seems to be incomplete — the openai_agents directory in site-packages only contains metadata files (INSTALLER, METADATA, RECORD, etc.), with no actual Python module files. As a result, I cannot import the module, and attempts to do so result in a ModuleNotFoundError.\n\nSteps to Reproduce:\nCreate a fresh Python environment:\n\nbash\nCopy\npython3 -m venv env\nActivate the environment:\n\nbash\nCopy\nsource env/bin/activate\nInstall openai-agents:\n\nbash\nCopy\npip install openai-agents\nAfter installation, run the following to verify the installation:\n\nbash\nCopy\npip show openai-agents\nThe output shows the installation is successful, but the site-packages/openai_agents directory contains only metadata files.\n\nAttempt to import the module in Python:\n\npython\nCopy\nfrom openai_agents import Agent\nThis results in a ModuleNotFoundError.\n\nExpected Behavior:\nThe openai-agents package should include the necessary Python modules under the site-packages/openai_agents directory. I should be able to import the module as follows:\n\npython\nCopy\nfrom openai_agents import Agent\nActual Behavior:\nAfter installation, the site-packages/openai_agents directory contains only metadata files (INSTALLER, METADATA, RECORD, etc.), and the Python modules (openai_agents/__init__.py, etc.) are missing. Consequently, I am unable to import the module.\n\nEnvironment:\nPython version: 3.9.6\n\nOperating System: macOS 11.0\n\nPackage version: openai-agents 0.0.7\n\nInstallation method: pip install openai-agents\n\nAdditional Information:\nI’ve tried reinstalling the package multiple times using --no-cache-dir and have also used a fresh virtual environment.\n\nI have also checked the site-packages/openai_agents directory, and it is missing the necessary Python files.\n\nI’ve checked the PyPI page, and there’s no indication of a known issue with the package installation.\n\nThe issue persists after upgrading pip to the latest version.\n\n", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/373/comments", "author": "william-e43", "comments": [{"user": "rm-openai", "created_at": "2025-03-27T17:38:49Z", "body": "I just tried this on a mac with python 3.9/3.11/3.12 and couldn't reproduce it. One note, your import is incorrect. The correct one is:\n```\nfrom agents import Agent\n```\n\ncan you try that?"}, {"user": "william-e43", "created_at": "2025-03-27T17:42:44Z", "body": "Yeah, working now with that import statement. Thanks"}], "satisfaction_conditions": ["Correct import statement syntax for the openai-agents package", "Guidance that allows successful module importing", "Clarification about the package's expected usage pattern"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:53"}, "dockerfile": null, "language": "python"}
{"number": 316, "title": "How to provide object as input to agent", "created_at": "2025-03-24T09:24:21Z", "closed_at": "2025-03-25T15:12:05Z", "commit_id": "623063b633bb0f4ed4da1d31f341f3ecee3ba25f", "labels": ["question"], "url": "https://github.com/openai/openai-agents-python/issues/316", "body": "Hi Team,\n\nI am trying to create an agent which will use function tool approach to call my python method to extract the text from a excel file. So I have to provide the file as an input. How can I do it?\n\n```\n\nclass ExtractionItem(BaseModel):\n    extractedText: str\n    \"\"\"The extracted text from the document..\"\"\"\n\n===== Agent Instantiation====\ntext_extractor_agent = Agent(\n        name=\"Text Extraction Agent\",\n        instructions=PROMPT,\n        model=\"gpt-4o\",\n        tools=[text_extraction],\n        output_type=ExtractionItem\n    )\n\n====== Function call =======\n\n@function_tool\nasync def text_extraction(input_excel_file):\n    try:\n        logic to extract the text\n    except Exception as e:\n        issues = str(e)\n\n    result = ExtractionItem(\n        extractedText=extracted text\n    )\n    return result\n\n```\nNow while calling the agent how can i provide file as an input? Please help me \n\n\n", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/316/comments", "author": "puneetsharma7445", "comments": [{"user": "DanieleMorotti", "created_at": "2025-03-24T10:52:41Z", "body": "Hi, there are several methods. I don't know what you need to do on the excel file, therefore I provide you with some generic examples.\n\nThe first method is to make the model pass the input file name (you have to pay attention to the actual path of the file):\n```python\nfrom agents import (\n    Agent,\n    ModelSettings,\n    RunContextWrapper,\n    Runner,\n    function_tool,\n)\nimport pandas as pd\nfrom pydantic import BaseModel\n\n@function_tool\ndef text_extraction(file_path: str):\n    \"\"\"\n    It returns the excel file data.\n    \"\"\"\n    try:\n        df = pd.read_excel(file_path)\n        return df.to_csv()\n    except Exception as exc:\n        return f\"Failed to read excel file, due to exception: {exc}.\"\n\nclass ExtractionItem(BaseModel):\n    \"\"\"The extracted text from the document..\"\"\"\n    extractedText: str\n\nasync def main():\n    agent = Agent(\n        name=\"Text extraction agent\",\n        model=\"gpt-4o-mini\",\n        instructions=\"You are a helpful agent, your task is to extract info from an excel file.\",\n        tools=[text_extraction],\n        model_settings=ModelSettings(\n            max_tokens=1024\n        ),\n        output_type=ExtractionItem\n    )\n    # Example of user input\n    user_input = \"I want to extract all the user names from the file 'excel_test.xlsx'\"\n    result = await Runner.run(agent, input=user_input)\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nWhile the second approach may be to read the file and put it in the context:\n```python\nfrom dataclasses import dataclass\nimport io\n\n@dataclass\nclass ContextInfo:\n    excel_file: io.BytesIO = None\n\n@function_tool\ndef text_extraction(context: RunContextWrapper[ContextInfo]):\n    \"\"\"\n    It returns the excel file data.\n    \"\"\"\n    try:\n        df = pd.read_excel(context.context.excel_file)\n        return df.to_csv()\n    except Exception as exc:\n        return f\"Failed to read excel file, due to exception: {exc}.\"\n\n\nasync def main():\n    agent = Agent(\n        name=\"Text extraction agent\",\n        model=\"gpt-4o-mini\",\n        instructions=\"You are a helpful agent, your task is to extract info from an excel file.\",\n        tools=[text_extraction],\n        model_settings=ModelSettings(\n            max_tokens=1024\n        ),\n        output_type=ExtractionItem\n    )\n\n    user_input = \"I want to extract all the user names from the excel file.\"\n    result = await Runner.run(\n        agent, input=user_input,\n        context=ContextInfo(excel_file=open(\"excel_test.xlsx\", \"rb\"))\n    )\n    print(result.final_output)\n```\n\nFor both methods, you need to install `pandas` and `openpyxl` to read the Excel file. I also suggest being cautious when passing an entire file, as it may contain a large number of tokens.\nHope this may be useful for your use case!"}, {"user": "yizhangliu", "created_at": "2025-03-24T11:08:01Z", "body": "Context is a good thing."}, {"user": "rm-openai", "created_at": "2025-03-24T17:49:26Z", "body": "Tool calling is via JSON inputs and string outputs. So the LLM is unable to pass a file directly in the tool call, but it can pass a reference to the file as @DanieleMorotti mentioned. For example, file name, path, etc."}, {"user": "puneetsharma7445", "created_at": "2025-03-25T05:44:27Z", "body": "Thanks for the help and support @DanieleMorotti .\n\nI was struggling with approach to pass input parameters to the Tool. I am able to proceed now."}], "satisfaction_conditions": ["A method to provide a file as input to an agent's function tool", "Practical examples showing how to pass file data to function tools", "Explanation of available options for file handling in agent tools"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:58"}, "dockerfile": null, "language": "python"}
{"number": 298, "title": "Guide on few-shot prompting", "created_at": "2025-03-22T04:54:38Z", "closed_at": "2025-03-22T05:39:14Z", "commit_id": "8d906f88f02d30b3cf6068e5de88a5f1e4bafd82", "labels": ["question"], "url": "https://github.com/openai/openai-agents-python/issues/298", "body": "Few-shot prompting is one of the best techniques for controlling model output. I used to implement this with the `openai` client:\n\n```python\n\nfew_shot_1 = [\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"\nTeach me about patience.\n\"\"\".strip(),\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\"\"\nThe river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n\"\"\".strip(),\n    },\n]\n\n# ...\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt},\n        *few_shot_1,\n        *few_shot_2,\n        {\"role\": \"user\", \"content\": user_input},\n    ],\n    stream=True,\n)\n```\n\nBut with the Agents SDK, there's no clear way to do this. Ideally, few-shot examples should be part of the prompt defined at agent declaration, but adding multiple messages appears to be impossible for an `Agent` right now.\n\nShould I include my few-shot examples in agent instructions or as runner input instead?", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/298/comments", "author": "nicognaW", "comments": [{"user": "rm-openai", "created_at": "2025-03-22T05:26:13Z", "body": "1. including examples in the system prompt (ie agent instructions) works pretty well, so that could work.\n2. You can pass it to the runner, as you described. Would only really work well for the first agent though (ie handoffs wouldn't have the right examples)\n\nWould those work for you? "}, {"user": "nicognaW", "created_at": "2025-03-22T05:39:15Z", "body": "That is insightful, thanks for the reply."}], "satisfaction_conditions": ["Practical approaches for implementing few-shot prompting with the Agents SDK", "Explanation of tradeoffs between different implementation approaches", "Clear alternatives that address the technical constraints of the Agents SDK"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:04"}, "dockerfile": null, "language": "python"}
{"number": 263, "title": "Fix potential infinite tool call loop by resetting tool_choice after …", "created_at": "2025-03-20T13:29:21Z", "closed_at": "2025-03-25T15:30:53Z", "commit_id": "9384a0fb3fd13151c010d3f45c89bfcb05172784", "labels": [], "url": "https://github.com/openai/openai-agents-python/pull/263", "body": "# Fix potential infinite tool call loop by resetting tool_choice after tool execution\r\n\r\n## Summary\r\n\r\nThis PR fixes an issue where setting `tool_choice` to \"required\" or a specific function name could cause models to get stuck in an infinite tool call loop.\r\n\r\nWhen `tool_choice` is set to force tool usage, this setting persists across model invocations. This PR automatically resets `tool_choice` to \"auto\" after tool execution, allowing the model to decide whether to make additional tool calls in subsequent turns.\r\n\r\nUnlike using `tool_use_behavior=\"stop_on_first_tool\"`, this approach lets the model continue processing tool results while preventing forced repeated tool calls.\r\n\r\n## Test plan\r\n\r\n- Added tests to verify tool_choice reset behavior for both agent and run_config settings\r\n- Added integration test to verify the solution prevents infinite loops\r\n- All tests pass\r\n\r\n## Checks\r\n\r\n- [x] I've added new tests for the fix\r\n- [x] I've updated the relevant documentation (added comment in code)\r\n- [x] I've run `make lint` and `make format`\r\n- [x] I've made sure tests pass\r\n", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/263/comments", "author": "mini-peanut", "comments": [{"user": "rm-openai", "created_at": "2025-03-20T14:43:59Z", "body": "This is a good idea! What do you think about making it a configurable param, default to `reset=True`?"}, {"user": "mini-peanut", "created_at": "2025-03-20T14:51:40Z", "body": "> This is a good idea! What do you think about making it a configurable param, default to `reset=True`?\r\n\r\n@rm-openai Thanks for the feedback! I considered adding a config parameter, but wonder if it might add complexity without clear use cases. Most users would want to prevent infinite loops by default, and those with specific needs could already implement custom behaviors through the existing API.\r\n\r\nUnless you have specific scenarios in mind where maintaining forced tool calls is beneficial, perhaps the simpler approach is better?"}, {"user": "rm-openai", "created_at": "2025-03-20T15:12:45Z", "body": "@mini-peanut, yeah one use case I had in mind was this:\r\n\r\nSetup:\r\n```\r\nagent = Agent(\r\n  instructions=\"Use the find_company tool to find the company info. Then use the search_directory tool to get the CEO's email.\",\r\n  tools=[find_company, search_directory],\r\n  tool_choice=\"required\",\r\n  tool_use_behavior={\"stop_at_tool_names\": \"search_directory\"},\r\n```\r\n\r\nIf we reset `tool_choice`, then we can't trust the Agent to reliably call the second tool.\r\n\r\nThoughts?"}, {"user": "mini-peanut", "created_at": "2025-03-20T16:16:50Z", "body": "> @mini-peanut, yeah one use case I had in mind was this:\r\n> \r\n> Setup:\r\n> \r\n> ```\r\n> agent = Agent(\r\n>   instructions=\"Use the find_company tool to find the company info. Then use the search_directory tool to get the CEO's email.\",\r\n>   tools=[find_company, search_directory],\r\n>   tool_choice=\"required\",\r\n>   tool_use_behavior={\"stop_at_tool_names\": \"search_directory\"},\r\n> ```\r\n> \r\n> If we reset `tool_choice`, then we can't trust the Agent to reliably call the second tool.\r\n> \r\n> Thoughts?\r\n\r\n@rm-openai Thanks for sharing that use case. I'd like to refine my approach to focus on the specific problem we're solving.\r\n\r\n**The Problem:** Setting `tool_choice` to \"required\" or a specific function name can inadvertently cause infinite loops.\r\n\r\n**Core Hypothesis:** When a user forces a single specific function call, they rarely intend for that same function to be repeatedly called in an infinite loop. This differs from intentional sequential calling of different functions.\r\n\r\n**Problem Scenario:** This issue typically manifests in two specific cases:\r\n1. When `tool_choice` is set to a specific function name, causing the same function to be called repeatedly\r\n2. When `tool_choice=\"required\"` with only one available tool, which functionally behaves the same way\r\n\r\n**Concerns with Adding a Configuration Parameter:**\r\nUsers with legitimate sequential tool usage would need to explicitly set `reset_tool_choice_after_use` to `False`.\r\n\r\n**Targeted Solution:** We can address these specific scenarios without disrupting legitimate use cases:\r\n```python\r\n# Only reset in the problematic scenarios where loops are likely unintentional\r\nif (isinstance(tool_choice, str) and tool_choice not in [\"auto\", \"required\", \"none\"]) or \r\n   (tool_choice == \"required\" and len(tools) == 1):\r\n    # Reset to \"auto\"\r\n```\r\n\r\nThis approach precisely targets the infinite loop problem without affecting the multi-tool sequential calling pattern you described, and without requiring additional configuration.\r\n"}, {"user": "rm-openai", "created_at": "2025-03-21T14:29:41Z", "body": "lgtm - but would you mind fixing lint/typechecking please? can't merge without that"}, {"user": "mini-peanut", "created_at": "2025-03-22T06:19:43Z", "body": "@rm-openai Fixed, and the code should pass the checks. Thanks for your patience\r\n"}, {"user": "rm-openai", "created_at": "2025-03-22T16:06:59Z", "body": "Unfortunately looks like typechecking is still not passing"}, {"user": "rm-openai", "created_at": "2025-03-25T15:30:54Z", "body": "I'm merging this because it's mostly great. I think it will need a couple of followups:\r\n1. Instead of copying the agent, we should do internal bookkeping of the resets\r\n2. I still think this should be configurable\r\n3. I'm not sure it makes sense to reset the RunConfig ModelSettings. \r\n\r\nI'll follow up with all of those!"}], "satisfaction_conditions": ["A solution that prevents infinite tool call loops when tool_choice is set to 'required' or a specific function name", "A targeted approach that addresses problematic scenarios without disrupting legitimate sequential tool usage", "A solution that doesn't require additional configuration parameters unless absolutely necessary", "Code that passes all required checks (lint, typechecking, tests)", "Proper test coverage to verify the solution works as intended"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:11"}, "dockerfile": null, "language": "python"}
{"number": 123, "title": "Handoff to multiple agents in parallel", "created_at": "2025-03-13T07:22:42Z", "closed_at": "2025-03-15T10:27:00Z", "commit_id": "3ef5f4712aa2c2dcd2cd04520fa2589faadf4eb3", "labels": ["question"], "url": "https://github.com/openai/openai-agents-python/issues/123", "body": "Does the SDK support delegate to multiple sub-agents at once? \nIf the triage agent wants to delegate tasks to 3 best-capable agents as once and then gather and evaluate all of the results, how do I implement this logic? \nIn the examples, the parallelization seems to have to be hard coded rather than an intelligent hand-off.", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/123/comments", "author": "WSQsGithub", "comments": [{"user": "rm-openai", "created_at": "2025-03-13T22:43:23Z", "body": "No, it doesn't. Handoffs are meant for scenarios where you transfer control of the entire conversation to a new agent - so it's not possible to hand off to multiple agents.\n\nDepending on your scenario, it might make sense to either:\n1. Have mutliple agents and expose them as tools e.g.:\n```\nagent1, agent_2, agent_3, agent_4, ... = ...;\n\nmain_agent = Agent(\n  name=\"Triage\",\n  instructions=\"Call all the relevant agent tools in parallel, then synthesize a good response\",\n  model_settings=ModelSettings(parallel_tool_calls=True), # Enable parallel tool calling\n  tools=[agent_1.as_tool(...), agent_2.as_tool(...), agent_3.as_tool(...), ...]\n)\n```\n\nor \n\n2. If it's deterministic, do it in code:\n```\nagent1, agent_2, agent_3 = ...;\n\nresult_1, result_2, result_3 = await asyncio.gather(\n  Runner.run(agent_1, ...),\n  Runner.run(agent_2, ...),\n  Runner.run(agent_3, ...),\n)\n\nnew_input = f\"Synthesize a good response: {result_1.final_output} \\n {result_2.final_output} ...\"\n\nmain_agent = Agent(...)\nfinal_result = await Runner.run(main_agent, new_input)\n```\n\nWould these options work?\n"}, {"user": "huangbhan", "created_at": "2025-03-14T07:55:46Z", "body": "Same issue,Solution 1 is a good design concept, it works for me.\nBut I have a question.\n\nOption 1:\nagent -> multiple tools\n\nOption 2:\nagent -> multiple agents as tools (each agent has a tool that it can call)\n\nWhich of these two options is better? What are the differences?\n\n\n> No, it doesn't. Handoffs are meant for scenarios where you transfer control of the entire conversation to a new agent - so it's not possible to hand off to multiple agents.不，它不是。交接是为了将整个对话的控制权转移给一个新的代理，因此不可能交接给多个代理。\n> \n> Depending on your scenario, it might make sense to either:根据您的情况，您可能会觉得以下两种选择中的一种更合适：\n> \n> 1. Have mutliple agents and expose them as tools e.g.:拥有多个代理并将其作为工具公开，例如：\n> \n> ```\n> agent1, agent_2, agent_3, agent_4, ... = ...;\n> \n> main_agent = Agent(\n>   name=\"Triage\",\n>   instructions=\"Call all the relevant agent tools in parallel, then synthesize a good response\",\n>   model_settings=ModelSettings(parallel_tool_calls=True), # Enable parallel tool calling\n>   tools=[agent_1.as_tool(...), agent_2.as_tool(...), agent_3.as_tool(...), ...]\n> )\n> ```\n> \n> or  或\n> \n> 2. If it's deterministic, do it in code:如果是确定性的，就用代码实现：\n> \n> ```\n> agent1, agent_2, agent_3 = ...;\n> \n> result_1, result_2, result_3 = await asyncio.gather(\n>   Runner.run(agent_1, ...),\n>   Runner.run(agent_2, ...),\n>   Runner.run(agent_3, ...),\n> )\n> \n> new_input = f\"Synthesize a good response: {result_1.final_output} \\n {result_2.final_output} ...\"\n> \n> main_agent = Agent(...)\n> final_result = await Runner.run(main_agent, new_input)\n> ```\n> \n> Would these options work?这些选项可行吗？\n\n"}, {"user": "WSQsGithub", "created_at": "2025-03-14T10:24:23Z", "body": "Thank you for making things clear with handoffs. But it would be neat if agent can dynamically call multiple tools concurrently. "}, {"user": "rm-openai", "created_at": "2025-03-14T19:00:53Z", "body": "> But it would be neat if agent can dynamically call multiple tools concurrently.\n\nIn the first example I gave, that's indeed what is happening. Is there some use case that doesn't work there?"}, {"user": "WSQsGithub", "created_at": "2025-03-15T10:27:00Z", "body": "> > But it would be neat if agent can dynamically call multiple tools concurrently.\n> \n> In the first example I gave, that's indeed what is happening. Is there some use case that doesn't work there?\n\nMy bad. I didn't notice this modification of `parallel_tool_calls=True`. Thank you for your clarification!"}], "satisfaction_conditions": ["A way to delegate tasks to multiple agents in parallel", "Clarification on whether handoffs support multiple parallel agents", "A solution that allows dynamic concurrent tool calling", "A programmatic approach rather than hard-coded parallelization"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:28"}, "dockerfile": null, "language": "python"}
{"number": 71, "title": "max_tokens is not an accepted parameter", "created_at": "2025-03-12T09:22:15Z", "closed_at": "2025-03-12T23:29:36Z", "commit_id": "5aba0b5b19645d4f77dcc43a10e23ed4c8d6ff9e", "labels": ["bug"], "url": "https://github.com/openai/openai-agents-python/issues/71", "body": "Out of the Documentation, there is no reference of max_tokens. the ModelSettings does not accept the max_tokens parameter..\n\nThis becomes a problem especially when using anthropic models as they dont assume a max tokens value and need one to get passed.\n", "comments_url": "https://api.github.com/repos/openai/openai-agents-python/issues/71/comments", "author": "s44002", "comments": [{"user": "s44002", "created_at": "2025-03-12T09:22:56Z", "body": "I am fixing the issue"}, {"user": "rm-openai", "created_at": "2025-03-12T23:29:36Z", "body": "Apologies, I didn't see this issue/PR in time and implemented it myself via #105"}, {"user": "s44002", "created_at": "2025-03-13T03:42:26Z", "body": "No worries, getting that fixed was the whole point. "}], "satisfaction_conditions": ["Support for the max_tokens parameter in ModelSettings", "Compatibility with anthropic models that require max_tokens specification", "Alignment between documentation and actual functionality"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:33"}, "dockerfile": null, "language": "python"}
{"number": 51, "title": "安装包", "created_at": "2025-02-26T07:11:41Z", "closed_at": "2025-02-26T09:42:18Z", "commit_id": "485ebbacc242ecd8e73c8d71ad61a012c9696917", "labels": [], "url": "https://github.com/icip-cas/PPTAgent/issues/51", "body": "requirements.txt里面的marker-pdf==1.1.0是找不到的，没有这个版本\n\n\n\n报错如下：ERROR: Could not find a version that satisfies the requirement marker-pdf==1.1.0 (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.2.1, 0.2.2, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.8, 0.2.9, 0.2.10, 0.2.11, 0.2.12, 0.2.13, 0.2.14, 0.2.15, 0.2.16, 0.2.17)\nERROR: No matching distribution found for marker-pdf==1.1.0\n", "comments_url": "https://api.github.com/repos/icip-cas/PPTAgent/issues/51/comments", "author": "Xwxhwy", "comments": [{"user": "Force1ess", "created_at": "2025-02-26T09:20:43Z", "body": "可能因为你的python版本不对吧"}, {"user": "Xwxhwy", "created_at": "2025-02-26T09:42:15Z", "body": "谢谢，我安装了3.10.22成功了"}], "satisfaction_conditions": ["Guidance on resolving the package version compatibility issue", "Information about potential Python version dependencies", "A solution that allows successful installation of the required packages"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:18:57"}, "dockerfile": null, "language": "python"}
{"number": 43, "title": "GPU image does not use GPU", "created_at": "2025-01-13T12:12:31Z", "closed_at": "2025-01-13T12:53:46Z", "commit_id": "1e45a3107f6f23df3f8a824f72639d8ff7711519", "labels": [], "url": "https://github.com/remsky/Kokoro-FastAPI/issues/43", "body": "I am using following image: ghcr.io/remsky/kokoro-fastapi-gpu:v0.0.5post1 with unmodified `docker-compose.yml` from repo.\r\n```\r\nkokoro-tts-1     | INFO:     Started server process [1]\r\nkokoro-tts-1     | INFO:     Waiting for application startup.\r\nkokoro-tts-1     | 11:30:33 AM | INFO     | Loading TTS model and voice packs...\r\nkokoro-tts-1     | 11:30:33 AM | INFO     | CUDA available: False\r\nkokoro-tts-1     | 11:30:33 AM | INFO     | Initializing model on cpu\r\n```\r\n\r\nImage runs as `appuser` instead of root and on linux you need to be in proper group to use NVIDIA driver (even when using nvidia-container-toolkit).\r\n```\r\ndevilan@darkstar:~/git/Kokoro-FastAPI (master) $ docker exec -it b5ed10b82d7f /bin/bash\r\nappuser@b5ed10b82d7f:/app$ nvidia-smi \r\nFailed to initialize NVML: Insufficient Permissions\r\n```\r\n\r\nYou should at least point this out clearly in README if this app should not work as root user.\r\nThere are some methods to resolve this problem, but each of them compromises system a bit.\r\nYou can chmod 666 /dev/nvidia* so no access restriction for this device (but not everyone would like to do so).", "comments_url": "https://api.github.com/repos/remsky/Kokoro-FastAPI/issues/43/comments", "author": "DevilaN", "comments": [{"user": "remsky", "created_at": "2025-01-13T12:53:46Z", "body": "Hey @DevilaN. Thanks for the info. I'm not seeing that issue, though as mentioned in the readme, it's only been tested on Windows 11 w/WSL2 and the HuggingFace A100's for CUDA support + a Mac M3 for CPU (also may require a build from source for arm compatibility, though looking to add the multiplatform image shortly)\r\n\r\nAdded a note in the readme for linux users with this issue, feel free to modify it with a PR with any brief specifics I miss, so they can build from source with the config they'd want. \r\n\r\nbfe2dd522f5eef196973c2c95a0416bc314f17e4"}, {"user": "DevilaN", "created_at": "2025-01-13T13:21:06Z", "body": "Nice!\r\nThere might be a little catch with `add_group:` for users.\r\nIf numerical group id exists in container it is not added, so for me `video` group is the proper one on Gentoo. But it has id: 27 which is also id of `sudo` group inside image `/etc/groups`. Only when I specify id of group instead of it's name it is working great.\r\n\r\nOf course different systems / distros have different permissions / id's so there is no one proper way of doing this.\r\n\r\nIt might be worth mentioning that instead of tinkering with `docker-compose.yml` file it might be better to create `docker-compose.override.yml` and place changes there, so it will not conflict with further repo pulling.\r\n\r\nThank you for your support!"}], "satisfaction_conditions": ["Documentation about Linux permission requirements for GPU access", "Information about group permission configuration for GPU access", "Guidance on maintaining custom configurations across repository updates", "Acknowledgment of platform-specific differences in GPU access configuration"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:19:03"}, "dockerfile": null, "language": "python"}
{"number": 39, "title": "How to add static objects?", "created_at": "2025-04-08T00:51:22Z", "closed_at": "2025-04-08T05:59:15Z", "commit_id": "ae0875d4af06360129588a693aba7cc4feb7d058", "labels": [], "url": "https://github.com/RoboVerseOrg/RoboVerse/issues/39", "body": "How to add an object with fixed pose, but still participate in collision checking?", "comments_url": "https://api.github.com/repos/RoboVerseOrg/RoboVerse/issues/39/comments", "author": "dementrock", "comments": [{"user": "Fisher-Wang", "created_at": "2025-04-08T05:12:22Z", "body": "Hi Rocky!\n\nYou can configurate the object with `fix_base_link=True` and `collision_enabled=True`. Alternatively, you can use `physics=PhysicStateType.GEOM` which is an alias the configuration above, following IsaacSim's convension."}, {"user": "dementrock", "created_at": "2025-04-08T05:59:15Z", "body": "Thx! fix_base_link=True worked. Just setting physics=PhysicStateType.GEOM did not work (using mujoco)"}], "satisfaction_conditions": ["A method to create static objects that still participate in collision detection", "A solution compatible with the Mujoco physics engine", "Configuration parameters that can be applied to objects to achieve the desired behavior"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:58"}, "dockerfile": null, "language": "python"}
{"number": 60, "title": "Error on Custom Video Motion Processing  No module named 'mmcv.parallel'", "created_at": "2025-03-29T11:24:10Z", "closed_at": "2025-03-31T07:35:43Z", "commit_id": "5e2ed8b1283c0aac10bd18759d9dc0154cd848f0", "labels": [], "url": "https://github.com/aigc3d/LHM/issues/60", "body": "\nHello There, \nI am testing the 'Custom Video Motion Processing' part and installed \n\ncd ./engine/pose_estimation\npip install -v -e third-party/ViTPose\npip install ultralytics\n\nI am able to run inference pipeline -\nbash ./inference.sh ./configs/inference/human-lrm-500M.yaml LHM-500M ./train_data/example_imgs/ ./train_data/motion_video/mimo1/smplx_params\n\n\nBut when I'm running this line of code-\npython ./engine/pose_estimation/video2motion.py --video_path ./train_data/demo.mp4 --output_path ./train_data/custom_motion\n\nIt is always throwing error on mmpose,  I tried to install different version of mmpose using mim install,  no luck.\nCould you let me know what am I missing, or the correct compatible libraries.\nERROR-\n\nLHM$ python ./engine/pose_estimation/video2motion.py --video_path ./train_data/demo.mp4 --output_path ./train_data/custom_motion\nTraceback (most recent call last):\n  File \"/workspace/ComfyUI/custom_nodes/LHM/./engine/pose_estimation/video2motion.py\", line 28, in <module>\n    from blocks.detector import DetectionModel\n  File \"/workspace/ComfyUI/custom_nodes/LHM/engine/pose_estimation/blocks/detector.py\", line 7, in <module>\n    from mmpose.apis.inference import batch_inference_pose_model\n  File \"/venv/main/lib/python3.10/site-packages/mmpose/apis/__init__.py\", line 2, in <module>\n    from .inference import (inference_bottom_up_pose_model,\n  File \"/venv/main/lib/python3.10/site-packages/mmpose/apis/inference.py\", line 9, in <module>\n    from mmcv.parallel import collate, scatter\nModuleNotFoundError: No module named 'mmcv.parallel'\n\n\n\n----------------\n\n\nLHM$ python ./engine/pose_estimation/video2motion.py --video_path ./train_data/demo.mp4 --output_path ./train_data/custom_motion\n/venv/main/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n/venv/main/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\nTraceback (most recent call last):\n  File \"/workspace/ComfyUI/custom_nodes/LHM/./engine/pose_estimation/video2motion.py\", line 28, in <module>\n    from blocks.detector import DetectionModel\n  File \"/workspace/ComfyUI/custom_nodes/LHM/engine/pose_estimation/blocks/detector.py\", line 7, in <module>\n    from mmpose.apis.inference import batch_inference_pose_model\nImportError: cannot import name 'batch_inference_pose_model' from 'mmpose.apis.inference' (/venv/main/lib/python3.10/site-packages/mmpose/apis/inference.py)\n\n--------------------------------\n\nLHM$ python ./engine/pose_estimation/video2motion.py --video_path ./train_data/demo.mp4 --output_path ./train_data/custom_motion\n/venv/main/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n/venv/main/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\nTraceback (most recent call last):\n  File \"/workspace/ComfyUI/custom_nodes/LHM/./engine/pose_estimation/video2motion.py\", line 28, in <module>\n    from blocks.detector import DetectionModel\n  File \"/workspace/ComfyUI/custom_nodes/LHM/engine/pose_estimation/blocks/detector.py\", line 7, in <module>\n    from mmpose.apis.inference import batch_inference_pose_model\nImportError: cannot import name 'batch_inference_pose_model' from 'mmpose.apis.inference' (/venv/main/lib/python3.10/site-packages/mmpose/apis/inference.py)\n\n", "comments_url": "https://api.github.com/repos/aigc3d/LHM/issues/60/comments", "author": "AIExplorer25", "comments": [{"user": "rencosmo", "created_at": "2025-03-29T16:01:23Z", "body": "pip install mmcv==1.7.2"}, {"user": "AIExplorer25", "created_at": "2025-03-29T16:16:09Z", "body": "Yes, found it, the new version has moved multiple modules to mmengine."}], "satisfaction_conditions": ["Identification of the correct dependency version needed to resolve the import error", "Understanding of why the import error occurred", "A solution that resolves the 'No module named mmcv.parallel' error"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:07:38"}, "dockerfile": "FROM python:3.10-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    wget \\\n    git \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    build-essential \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/aigc3d/LHM.git . && \\\n    git checkout 5e2ed8b1283c0aac10bd18759d9dc0154cd848f0\n\n# Install PyTorch with CUDA 11.8\nRUN pip install --no-cache-dir torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n\n# Install main dependencies\nRUN pip install --no-cache-dir \\\n    numpy==1.24.3 \\\n    scipy \\\n    scikit-image \\\n    matplotlib \\\n    opencv-python \\\n    trimesh \\\n    pyrender \\\n    lpips \\\n    imageio \\\n    imageio-ffmpeg \\\n    tqdm \\\n    open3d \\\n    gdown \\\n    accelerate \\\n    transformers \\\n    diffusers \\\n    safetensors \\\n    einops \\\n    kornia \\\n    xformers==0.0.20 \\\n    omegaconf \\\n    wandb \\\n    pytorch-lightning \\\n    ninja \\\n    moviepy \\\n    chumpy \\\n    smplx \\\n    hydra-core \\\n    fastapi \\\n    uvicorn \\\n    gradio==3.32.0\n\n# Install mmcv and mmpose with specific versions to fix the import error\nRUN pip install --no-cache-dir openmim && \\\n    mim install mmcv-full==1.7.0 && \\\n    pip install mmdet==2.28.2 && \\\n    pip install mmpose==0.28.1\n\n# Install ViTPose\nRUN cd ./engine/pose_estimation && \\\n    git clone https://github.com/ViTAE-Transformer/ViTPose.git third-party/ViTPose && \\\n    cd third-party/ViTPose && \\\n    pip install -v -e .\n\n# Install ultralytics\nRUN pip install ultralytics\n\n# Create directories for model weights\nRUN mkdir -p pretrained_models/human_model_files \\\n    pretrained_models/sam2 \\\n    pretrained_models/voxel_grid \\\n    pretrained_models/dense_sample_points \\\n    pretrained_models/gagatracker \\\n    pretrained_models/sapiens \\\n    exps/releases/video_human_benchmark/human-lrm-500M/step_060000 \\\n    exps/releases/video_human_benchmark/human-lrm-1B/step_060000 \\\n    train_data/example_imgs \\\n    train_data/motion_video \\\n    train_data/custom_motion\n\n# Set environment variables\nENV PYTHONPATH=/app\n\n# Make the inference script executable\nRUN chmod +x inference.sh\n\n# Default command\nCMD [\"/bin/bash\"]", "language": "python"}
{"number": 69, "title": "Error with Gradio: TypeError: argument of type 'bool' is not iterable", "created_at": "2025-04-02T20:12:32Z", "closed_at": "2025-04-03T20:14:13Z", "commit_id": "5e2ed8b1283c0aac10bd18759d9dc0154cd848f0", "labels": [], "url": "https://github.com/aigc3d/LHM/issues/69", "body": "Hey, I have been getting this error. Tried fixing it but couldn't. Do you guys want me to share complete error logs? \nPlease let me know a fix. thanks and if possible create a docker file which allows the setup to be easy. \n\n```\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/workspace/venv/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n  File \"/workspace/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/applications.py\", line 113, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 760, in __call__\n    await self.app(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n    raise exc\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n    raise exc\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/workspace/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/workspace/venv/lib/python3.10/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\n    return await run_in_threadpool(dependant.call, **values)\n  File \"/workspace/venv/lib/python3.10/site-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\n    return await anyio.to_thread.run_sync(func, *args)\n  File \"/workspace/venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n  File \"/workspace/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n    return await future\n  File \"/workspace/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n    result = context.run(func, *args)\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio/routes.py\", line 427, in main\n    gradio_api_info = api_info(False)\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio/routes.py\", line 456, in api_info\n    app.api_info = app.get_blocks().get_api_info()\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2782, in get_api_info\n    python_type = client_utils.json_schema_to_python_type(info)\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio_client/utils.py\", line 893, in json_schema_to_python_type\n    type_ = _json_schema_to_python_type(schema, schema.get(\"$defs\"))\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio_client/utils.py\", line 947, in _json_schema_to_python_type\n    des = [\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio_client/utils.py\", line 948, in <listcomp>\n    f\"{n}: {_json_schema_to_python_type(v, defs)}{get_desc(v)}\"\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio_client/utils.py\", line 955, in _json_schema_to_python_type\n    f\"str, {_json_schema_to_python_type(schema['additionalProperties'], defs)}\"\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio_client/utils.py\", line 901, in _json_schema_to_python_type\n    type_ = get_type(schema)\n  File \"/workspace/venv/lib/python3.10/site-packages/gradio_client/utils.py\", line 863, in get_type\n    if \"const\" in schema:\nTypeError: argument of type 'bool' is not iterable\n```", "comments_url": "https://api.github.com/repos/aigc3d/LHM/issues/69/comments", "author": "notaibin", "comments": [{"user": "hitsz-zuoqi", "created_at": "2025-04-03T01:14:43Z", "body": "this is due to the update of gradio，try install pydantic==2.8.0"}, {"user": "notaibin", "created_at": "2025-04-03T08:01:46Z", "body": "> this is due to the update of gradio，try install pydantic==2.8.0\n\nHey thanks, that solved it. but ran into another issue:\n  File \"/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1159, in convert\n    return t.to(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n\nI actually have two 16 GB T4s, the process only acknowledges one of them.\n "}, {"user": "hitsz-zuoqi", "created_at": "2025-04-03T09:46:24Z", "body": "> > this is due to the update of gradio，try install pydantic==2.8.0\n> \n> Hey thanks, that solved it. but ran into another issue:\n>   File \"/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1159, in convert\n>     return t.to(\n> torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n> \n> I actually have two 16 GB T4s, the process only acknowledges one of them.\n>  \n\nemmm，currently 24gb is able for lhm，we will update a light version which can running on 16gb"}, {"user": "notaibin", "created_at": "2025-04-03T12:35:07Z", "body": "> > > this is due to the update of gradio，try install pydantic==2.8.0\n> > \n> > \n> > Hey thanks, that solved it. but ran into another issue:\n> > File \"/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1159, in convert\n> > return t.to(\n> > torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU\n> > I actually have two 16 GB T4s, the process only acknowledges one of them.\n> \n> emmm，currently 24gb is able for lhm，we will update a light version which can running on 16gb\n\nhey thanks for the amazing work. I think you didn't acknowledge that I have 2x16 GB T4s. So, is it ncessary to have a GPU with at least 24 GB VRAM because 2x16 should also get the job done? but it only acknowledges 1 during the inference."}, {"user": "lingtengqiu", "created_at": "2025-04-03T17:05:06Z", "body": "> > > > this is due to the update of gradio，try install pydantic==2.8.0\n> > > \n> > > \n> > > Hey thanks, that solved it. but ran into another issue:\n> > > File \"/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1159, in convert\n> > > return t.to(\n> > > torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU\n> > > I actually have two 16 GB T4s, the process only acknowledges one of them.\n> > \n> > \n> > emmm，currently 24gb is able for lhm，we will update a light version which can running on 16gb\n> \n> hey thanks for the amazing work. I think you didn't acknowledge that I have 2x16 GB T4s. So, is it ncessary to have a GPU with at least 24 GB VRAM because 2x16 should also get the job done? but it only acknowledges 1 during the inference.\n\nYes you are right! we currently have trained LHM-mini, which can be run on single 16G card."}], "satisfaction_conditions": ["A solution to the TypeError related to Gradio and pydantic compatibility", "Guidance on GPU memory requirements for running the model", "Information about model variants that can run on lower VRAM GPUs"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:07:32"}, "dockerfile": "FROM python:3.10\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    wget \\\n    git \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/aigc3d/LHM.git . && \\\n    git checkout 5e2ed8b1283c0aac10bd18759d9dc0154cd848f0\n\n# Create and activate a virtual environment\nRUN python -m venv /app/venv\nENV PATH=\"/app/venv/bin:$PATH\"\n\n# Install PyTorch and dependencies for CUDA 12.1\nRUN pip install --no-cache-dir torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install dependencies with specific versions to avoid compatibility issues\n# Specifically pin gradio to a version that fixes the TypeError issue\nRUN pip install --no-cache-dir \\\n    numpy==1.24.4 \\\n    scipy \\\n    scikit-image \\\n    matplotlib \\\n    opencv-python \\\n    trimesh \\\n    pyrender \\\n    lpips \\\n    imageio \\\n    imageio-ffmpeg \\\n    tqdm \\\n    open3d \\\n    gdown \\\n    accelerate \\\n    transformers \\\n    diffusers \\\n    safetensors \\\n    einops \\\n    kornia \\\n    xformers \\\n    omegaconf \\\n    wandb \\\n    pytorch-lightning \\\n    ninja \\\n    moviepy \\\n    chumpy \\\n    smplx \\\n    hydra-core \\\n    fastapi==0.95.2 \\\n    uvicorn==0.22.0 \\\n    gradio==3.32.0\n\n# Create directories for model weights\nRUN mkdir -p pretrained_models/human_model_files \\\n    pretrained_models/sam2 \\\n    pretrained_models/voxel_grid \\\n    pretrained_models/dense_sample_points \\\n    pretrained_models/gagatracker \\\n    pretrained_models/sapiens \\\n    exps/releases/video_human_benchmark/human-lrm-500M/step_060000 \\\n    exps/releases/video_human_benchmark/human-lrm-1B/step_060000 \\\n    train_data/example_imgs \\\n    train_data/motion_video\n\n# Set environment variables\nENV PYTHONPATH=/app\n\n# Make the inference script executable\nRUN chmod +x inference.sh\n\n# Set the default command to show help information\nCMD [\"echo\", \"LHM Docker container is ready. Use the following command to run inference:\\ndocker run --gpus all -v /path/to/your/data:/app/data -it <image_name> ./inference.sh <config> <model_name> <image_path> <motion_seq>\"]", "language": "python"}
{"number": 51, "title": "replacing github.com's \"hub\" by \"injest\" does not work", "created_at": "2024-12-24T14:16:14Z", "closed_at": "2024-12-24T17:05:10Z", "commit_id": "dafe508fb846f40029a166fa9c7b02e2f74345ab", "labels": [], "url": "https://github.com/cyclotruc/gitingest/issues/51", "body": "All my browsers said \"Cann't find the Server\". ", "comments_url": "https://api.github.com/repos/cyclotruc/gitingest/issues/51/comments", "author": "kalufinnle", "comments": [{"user": "DevloperAmanSingh", "created_at": "2024-12-24T14:26:21Z", "body": "its `ingest`\r\n@kalufinnle "}, {"user": "cyclotruc", "created_at": "2024-12-24T17:05:10Z", "body": "Yes it's ingest with a G\r\nthank you @DevloperAmanSingh \r\nclosing this\r\n"}], "satisfaction_conditions": ["Correction of the spelling error in the URL", "Clear identification of why the server couldn't be found"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:08:07"}, "dockerfile": null, "language": "python"}
{"number": 63, "title": "AttributeError: 'Tensor' object has no attribute 'full_tensor'", "created_at": "2025-03-05T09:26:21Z", "closed_at": "2025-03-06T01:01:18Z", "commit_id": "deb3e6cd0b8cc364fd7912d74b55f4efe79a55e3", "labels": [], "url": "https://github.com/hiyouga/EasyR1/issues/63", "body": "我用docker file制作镜像，尝试运行，遇到这个报错：\n/EasyR1/verl/workers/rollout/vllm_rollout/dtensor_weight_loaders.py\", line 284, in redistribute_dtensor\n    local_loaded_weights = loaded_weights.full_tensor()\nAttributeError: 'Tensor' object has no attribute 'full_tensor'\n请问这可能是啥原因？看起来是某些依赖的版本不对？", "comments_url": "https://api.github.com/repos/hiyouga/EasyR1/issues/63/comments", "author": "h7878778h", "comments": [{"user": "hiyouga", "created_at": "2025-03-05T09:37:37Z", "body": "At least 2 GPU is needed to run EasyR1"}, {"user": "h7878778h", "created_at": "2025-03-06T01:01:14Z", "body": "> At least 2 GPU is needed to run EasyR1\n\nthanks"}], "satisfaction_conditions": ["Information about the minimum hardware requirements to run EasyR1", "Explanation of why the 'full_tensor' attribute error occurs", "A concise, direct answer that identifies the root cause of the error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:12"}, "dockerfile": null, "language": "python"}
{"number": 28, "title": "is_multimodal_model", "created_at": "2025-02-25T09:30:38Z", "closed_at": "2025-02-25T11:18:20Z", "commit_id": "918dc96adfaa2ba10362991f8bdc63db3b9fb529", "labels": [], "url": "https://github.com/hiyouga/EasyR1/issues/28", "body": "Did anybody faced this problem. when I tried to train Qwen2.5-3B-Instruct model, and i get  an error:\n\n **\"Your model does not support multi-modal inputs\".**\n\ntried on 4*A6000 40GB.\n\nMy trainning shell:\n\npython3 -m verl.trainer.main \\\n    config=examples/grpo_example.yaml \\\n    data.train_files=hiyouga/geometry3k@train \\\n    data.val_files=hiyouga/geometry3k@test \\\n    data.max_prompt_length=4096 \\\n    worker.actor.model.model_path=${MODEL_PATH} \\\n    worker.rollout.tensor_parallel_size=1 \\\n    worker.rollout.enable_chunked_prefill=false \\\n    trainer.experiment_name=qwen2_5_vl_3b_geo \\\n    trainer.n_gpus_per_node=4", "comments_url": "https://api.github.com/repos/hiyouga/EasyR1/issues/28/comments", "author": "RogersSteve", "comments": [{"user": "hiyouga", "created_at": "2025-02-25T10:46:27Z", "body": "geometry3k is a multimodal dataset, you should use Qwen2.5 VL model to fit this dataset"}, {"user": "RogersSteve", "created_at": "2025-02-25T11:18:17Z", "body": "> geometry3k is a multimodal dataset, you should use Qwen2.5 VL model to fit this dataset\n\nThanks for your reply, it worked!"}], "satisfaction_conditions": ["Identification of the correct model type needed for multimodal datasets", "A solution that resolves the 'Your model does not support multi-modal inputs' error", "Guidance on model selection appropriate for the geometry3k dataset"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:19"}, "dockerfile": null, "language": "python"}
{"number": 158, "title": "How can use my own local model when my when local environment cannot connect to the Internet.", "created_at": "2025-03-19T06:24:45Z", "closed_at": "2025-03-24T08:28:38Z", "commit_id": "0ed6fa19fdb49f32b75e6bf04cbe31c0c46e15cd", "labels": [], "url": "https://github.com/zilliztech/deep-searcher/issues/158", "body": "This is really an excellent project！Thank you for your contributions！ I would like to ask if it's possible to download the model from Hugging Face to use locally instead of accessing it through the API?", "comments_url": "https://api.github.com/repos/zilliztech/deep-searcher/issues/158/comments", "author": "CALVINhzy1", "comments": [{"user": "SimFG", "created_at": "2025-03-19T09:41:11Z", "body": "you can try to use:\nLLM, Ollama; (before using, you should run the qwq llm according ollama)\n```\nconfig.set_provider_config(\"llm\", \"Ollama\", {\"model\": \"qwq\"})\n```\nEmbedding, pymilvus-model;\n```\nconfig.set_provider_config(\"embedding\", \"MilvusEmbedding\", {\"model\": \"BAAI/bge-base-en-v1.5\"})\n```"}, {"user": "CALVINhzy1", "created_at": "2025-03-24T08:09:29Z", "body": "Thanks for your reply, the problem has solved! If we need to use local embedding model, we can download the model we need from huggingface offline, copy the model folder and specify the path of the model folder. "}, {"user": "SimFG", "created_at": "2025-03-24T08:25:57Z", "body": "If the issue has been solved, please help me close the issue. Thanks a lot"}], "satisfaction_conditions": ["Instructions for using local models without internet connection", "Guidance on how to specify local model paths", "Information about downloading models from Hugging Face for offline use", "Configuration instructions for local model integration"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:04:58"}, "dockerfile": null, "language": "python"}
{"number": 106, "title": "Various dimension issues", "created_at": "2025-03-02T14:51:46Z", "closed_at": "2025-03-02T23:02:10Z", "commit_id": "5b9ee2bc7e36dfddccf6d8e279c47eaa81ed0053", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/106", "body": "I can get things running well at certain dimensions, e.g. 848 x 480, or 1280 x 720, however there are other dimensions which cause errors to kick up. \n\n640 x 360 gives this error\n\n```\n# ComfyUI Error Report\n## Error Details\n- **Node ID:** 27\n- **Node Type:** WanVideoSampler\n- **Exception Type:** RuntimeError\n- **Exception Message:** The size of tensor a (45) must match the size of tensor b (44) at non-singleton dimension 3\n## Stack Trace\n```\n  File \"/notebooks/ComfyUI/execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/notebooks/ComfyUI/execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/notebooks/ComfyUI/execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n\n  File \"/notebooks/ComfyUI/execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/notebooks/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper/nodes.py\", line 1249, in process\n    temp_x0 = sample_scheduler.step(\n              ^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/notebooks/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper/wanvideo/utils/fm_solvers.py\", line 754, in step\n    model_output = self.convert_model_output(model_output, sample=sample)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/notebooks/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper/wanvideo/utils/fm_solvers.py\", line 383, in convert_model_output\n    x0_pred = sample - sigma_t * model_output\n              ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\n\n```\n\nNearby dimensions such as 648x360 kick up a slightly different error, purporting to be from EnhanceAVideo. My online instance just ran out of time though, so I'll update this thread once I've managed to get back on it!\n\n", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/106/comments", "author": "SRagy", "comments": [{"user": "kijai", "created_at": "2025-03-02T15:12:49Z", "body": "The model seems to require the dimensions to be divisible by 16 at least, not sure of the exact requirement, but I bet that error is because 360 / 16 = 22.5"}, {"user": "SRagy", "created_at": "2025-03-02T23:02:10Z", "body": "Yes, that was it, thanks."}], "satisfaction_conditions": ["Explanation of dimension requirements for the model", "Identification of the root cause of the dimension-related errors"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:03:59"}, "dockerfile": "FROM python:3.10-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install git and other dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the ComfyUI repository (needed as the custom node is built for ComfyUI)\nRUN git clone https://github.com/comfyanonymous/ComfyUI.git\n\n# Create the custom_nodes directory\nRUN mkdir -p ComfyUI/custom_nodes\n\n# Clone the WanVideoWrapper repository into custom_nodes\nWORKDIR /app/ComfyUI/custom_nodes\nRUN git clone https://github.com/kijai/ComfyUI-WanVideoWrapper.git\n\n# Checkout the specific commit mentioned in the issue\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN git checkout 5b9ee2bc7e36dfddccf6d8e279c47eaa81ed0053\n\n# Install ComfyUI dependencies\nWORKDIR /app/ComfyUI\nRUN pip install -r requirements.txt\n\n# Install WanVideoWrapper dependencies\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN pip install -r requirements.txt\n\n# Create directories for models that would be needed\nRUN mkdir -p /app/ComfyUI/models/text_encoders \\\n    /app/ComfyUI/models/diffusion_models \\\n    /app/ComfyUI/models/vae\n\n# Set the working directory back to the ComfyUI root\nWORKDIR /app/ComfyUI\n\n# The container is now ready for use\n# Note to users: You'll need to mount or copy the model files into the appropriate directories\n# - Text encoders to /app/ComfyUI/models/text_encoders\n# - Transformer to /app/ComfyUI/models/diffusion_models\n# - Vae to /app/ComfyUI/models/vae\n# Models can be downloaded from https://huggingface.co/Kijai/WanVideo_comfy/tree/main\n\n# Default command - can be overridden when running the container\nCMD [\"python\", \"-m\", \"main\"]", "language": "python"}
{"number": 318, "title": "Wan fun control reference image ignored", "created_at": "2025-03-27T10:36:05Z", "closed_at": "2025-03-27T11:04:56Z", "commit_id": "ac3f2859931513181f1c3ceaaacd8bce6c4f4889", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/318", "body": "Testing the example control workflow. Connecting the reference image to the wan empty embeds does nothing.\n\nIn other words the output is the same with the default connections or with reference image, like the reference image is completely ignored.", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/318/comments", "author": "luxdelux7", "comments": [{"user": "kijai", "created_at": "2025-03-27T10:57:01Z", "body": "> Testing the example control workflow. Connecting the reference image to the wan empty embeds does nothing.\n> \n> In other words the output is the same with the default connections or with reference image, like the reference image is completely ignored.\n\nYou don't connect it to the empty, that's just for T2V, you connect it straight to the sampler when using reference image."}, {"user": "luxdelux7", "created_at": "2025-03-27T11:04:56Z", "body": "Ah thank you. I was stupidly just connecting to the empty embeds as the note was above it.\n\nIt indeed works! Awesome. Now to test further."}], "satisfaction_conditions": ["Clear instructions on the correct connection point for the reference image in the control workflow", "Explanation of the distinction between Text-to-Video (T2V) connections and reference image connections", "Simple, direct guidance that corrects the user's workflow configuration"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:03:19"}, "dockerfile": "FROM python:3.10-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install git and other dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the ComfyUI repository (needed as the custom node is built for ComfyUI)\nRUN git clone https://github.com/comfyanonymous/ComfyUI.git\n\n# Create the custom_nodes directory\nRUN mkdir -p ComfyUI/custom_nodes\n\n# Clone the WanVideoWrapper repository into custom_nodes\nWORKDIR /app/ComfyUI/custom_nodes\nRUN git clone https://github.com/kijai/ComfyUI-WanVideoWrapper.git\n\n# Checkout the specific commit mentioned in the issue\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN git checkout ac3f2859931513181f1c3ceaaacd8bce6c4f4889\n\n# Install ComfyUI dependencies\nWORKDIR /app/ComfyUI\nRUN pip install -r requirements.txt\n\n# Install WanVideoWrapper dependencies\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN pip install -r requirements.txt\n\n# Create directories for models that would be needed\nRUN mkdir -p /app/ComfyUI/models/text_encoders \\\n    /app/ComfyUI/models/diffusion_models \\\n    /app/ComfyUI/models/vae\n\n# Set the working directory back to the ComfyUI root\nWORKDIR /app/ComfyUI\n\n# The container is now ready for use\n# Note to users: You'll need to mount or copy the model files into the appropriate directories\n# - Text encoders to /app/ComfyUI/models/text_encoders\n# - Transformer to /app/ComfyUI/models/diffusion_models\n# - Vae to /app/ComfyUI/models/vae\n# Models can be downloaded from https://huggingface.co/Kijai/WanVideo_comfy/tree/main\n\n# Default command - can be overridden when running the container\nCMD [\"python\", \"-m\", \"main\"]", "language": "python"}
{"number": 334, "title": "Error when excuting 1.3B control lora example workflow", "created_at": "2025-03-28T09:28:14Z", "closed_at": "2025-03-28T12:14:20Z", "commit_id": "d9b1f4d1a5aea91d101ae97a54714a5861af3f50", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/334", "body": "Both the nodes and workflow json file are latest.\n```\ngot prompt\nencoded latents shape torch.Size([1, 16, 13, 96, 64])\nFETCH ComfyRegistry Data: 5/80\nFETCH ComfyRegistry Data: 10/80\nin_channels:  16\nModel type: t2v, num_heads: 12, num_layers: 30\nModel variant detected: 1_3B\nTeaCache: Using cache device: cpu\nmodel_type FLOW\nUsing accelerate to load and assign model weights to device...\nLoading transformer parameters to cpu: 100%|██████████████████████████████████████| 825/825 [00:00<00:00, 14996.67it/s]\nLoading LoRA: wan\\wan2 with strength: 1.0\nControl-LoRA detected, patching model...\nLoading model and applying LoRA weights:: 100%|█████████████████████████████████████| 458/458 [00:00<00:00, 586.94it/s]\nMoving diffusion model from cuda:0 to cpu\n!!! Exception during processing !!! Empty image embeds must be provided for T2V (Text to Video\nTraceback (most recent call last):\n  File \"D:\\AI\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\AI\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\AI\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"D:\\AI\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\AI\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 1801, in process\n    raise ValueError(\"Empty image embeds must be provided for T2V (Text to Video\")\nValueError: Empty image embeds must be provided for T2V (Text to Video\n```", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/334/comments", "author": "cheezecrisp", "comments": [{"user": "kijai", "created_at": "2025-03-28T09:41:29Z", "body": "Wrong model selected, it needs to be the new Fun-Control model, link is in the workflow, based on your log you have a normal 1.3B model selected."}, {"user": "cheezecrisp", "created_at": "2025-03-28T09:49:51Z", "body": "> Wrong model selected, it needs to be the new Fun-Control model, link is in the workflow, based on your log you have a normal 1.3B model selected.\n\nTried the fun model and got another error:\n```\nRuntimeError: The size of tensor a (32) must match the size of tensor b (36) at non-singleton dimension 1\n```\n\nI used this workflow several days ago (before the fun models came out) and it worked well at that time, so I think it's nothing to do with the fun models."}, {"user": "kijai", "created_at": "2025-03-28T11:17:05Z", "body": "Right sorry, misread... forgot about whole control lora, should work now."}, {"user": "cheezecrisp", "created_at": "2025-03-28T12:14:21Z", "body": "> Right sorry, misread... forgot about whole control lora, should work now.\n\nGreat! It's fixed now"}], "satisfaction_conditions": ["A fix that resolves the error when executing the 1.3B control lora example workflow", "Compatibility with the existing workflow setup rather than requiring new models", "A solution that addresses the specific error about empty image embeds for T2V"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:03:12"}, "dockerfile": "FROM python:3.10-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install git and other dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the ComfyUI repository (needed as the custom node is built for ComfyUI)\nRUN git clone https://github.com/comfyanonymous/ComfyUI.git\n\n# Create the custom_nodes directory\nRUN mkdir -p ComfyUI/custom_nodes\n\n# Clone the WanVideoWrapper repository into custom_nodes\nWORKDIR /app/ComfyUI/custom_nodes\nRUN git clone https://github.com/kijai/ComfyUI-WanVideoWrapper.git\n\n# Checkout the specific commit mentioned in the issue\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN git checkout d9b1f4d1a5aea91d101ae97a54714a5861af3f50\n\n# Install ComfyUI dependencies\nWORKDIR /app/ComfyUI\nRUN pip install -r requirements.txt\n\n# Install WanVideoWrapper dependencies\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN pip install -r requirements.txt\n\n# Create directories for models that would be needed\nRUN mkdir -p /app/ComfyUI/models/text_encoders \\\n    /app/ComfyUI/models/diffusion_models \\\n    /app/ComfyUI/models/vae\n\n# Set the working directory back to the ComfyUI root\nWORKDIR /app/ComfyUI\n\n# The container is now ready for use\n# Note to users: You'll need to mount or copy the model files into the appropriate directories\n# - Text encoders to /app/ComfyUI/models/text_encoders\n# - Transformer to /app/ComfyUI/models/diffusion_models\n# - Vae to /app/ComfyUI/models/vae\n# Models can be downloaded from https://huggingface.co/Kijai/WanVideo_comfy/tree/main\n\n# Default command - can be overridden when running the container\nCMD [\"python\", \"-m\", \"main\"]", "language": "python"}
{"number": 200, "title": "wanvideo control workflow error", "created_at": "2025-03-11T17:16:37Z", "closed_at": "2025-03-12T06:10:32Z", "commit_id": "a4b1a4be7ac69ba705cd9cc43450751cd2e6f25c", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/200", "body": " File \"E:\\ComfyUI_py312\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_py312\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"E:\\ComfyUI_py312\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_py312\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 1717, in process\n    noise_pred, self.teacache_state = predict_with_cfg(\n                                      ^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_py312\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 1486, in predict_with_cfg\n    noise_pred_cond, teacache_state_cond = transformer(\n                                           ^^^^^^^^^^^^\n  File \"E:\\ComfyUI_py312\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_py312\\python_embeded\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_py312\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\wanvideo\\modules\\model.py\", line 746, in forward\n    x = self.unpatchify(x, grid_sizes)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\ComfyUI_py312\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\wanvideo\\modules\\model.py\", line 767, in unpatchify\n    x = x[:math.prod(v)].view(*v, *self.patch_size, c)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[14, 120, 67, 1, 2, 2, 16]' is invalid for input of size 7257600\n\nPrompt executed in 68.14 seconds", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/200/comments", "author": "NeilWang079", "comments": [{"user": "dvschultz", "created_at": "2025-03-12T05:48:39Z", "body": "I also got this"}, {"user": "dvschultz", "created_at": "2025-03-12T05:56:20Z", "body": "video input h/w needs to be divisible by 16"}, {"user": "NeilWang079", "created_at": "2025-03-12T06:10:32Z", "body": "Thanks"}], "satisfaction_conditions": ["An explanation of the cause of the runtime error", "A clear, concise solution to resolve the video dimension error"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:03:37"}, "dockerfile": "FROM python:3.10-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install git and other dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the ComfyUI repository (needed as the custom node is built for ComfyUI)\nRUN git clone https://github.com/comfyanonymous/ComfyUI.git\n\n# Create the custom_nodes directory\nRUN mkdir -p ComfyUI/custom_nodes\n\n# Clone the WanVideoWrapper repository into custom_nodes\nWORKDIR /app/ComfyUI/custom_nodes\nRUN git clone https://github.com/kijai/ComfyUI-WanVideoWrapper.git\n\n# Checkout the specific commit mentioned in the issue\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN git checkout a4b1a4be7ac69ba705cd9cc43450751cd2e6f25c\n\n# Install ComfyUI dependencies\nWORKDIR /app/ComfyUI\nRUN pip install -r requirements.txt\n\n# Install WanVideoWrapper dependencies\nWORKDIR /app/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\nRUN pip install -r requirements.txt\n\n# Create directories for models that would be needed\nRUN mkdir -p /app/ComfyUI/models/text_encoders \\\n    /app/ComfyUI/models/diffusion_models \\\n    /app/ComfyUI/models/vae\n\n# Set the working directory back to the ComfyUI root\nWORKDIR /app/ComfyUI\n\n# The container is now ready for use\n# Note to users: You'll need to mount or copy the model files into the appropriate directories\n# - Text encoders to /app/ComfyUI/models/text_encoders\n# - Transformer to /app/ComfyUI/models/diffusion_models\n# - Vae to /app/ComfyUI/models/vae\n# Models can be downloaded from https://huggingface.co/Kijai/WanVideo_comfy/tree/main\n\n# Default command - can be overridden when running the container\nCMD [\"python\", \"-m\", \"main\"]", "language": "python"}
{"number": 402, "title": "[I2V] Is there any way to increase the strength of the image embed vs prompt?", "created_at": "2025-04-07T21:53:48Z", "closed_at": "2025-04-10T14:12:58Z", "commit_id": "a39e4e2d0a85030655e8f3c196a4099938cb4e94", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/402", "body": "Is there any way to increase the strength of the image embed vs prompt?\nI mean that the image has a larger influence over the video than the prompt?\nHaven't noticed this issue before, but my output video is more different than I would like from the input image.\nThe change happens in the first split second of the video.", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/402/comments", "author": "fintarn", "comments": [{"user": "cheezecrisp", "created_at": "2025-04-09T16:59:25Z", "body": "Are you using loras? Some loras really change the face of the input image."}, {"user": "fintarn", "created_at": "2025-04-10T14:12:58Z", "body": "Yeah I think it was the Lora, lol. Thanks."}], "satisfaction_conditions": ["Identification of factors that can cause differences between the input image and output video", "Recognition of specific components that might affect the image-to-video transformation fidelity"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:59"}, "dockerfile": null, "language": "python"}
{"number": 371, "title": "Memory(RAM) problem after sampling with 32GB RAM", "created_at": "2025-04-02T10:06:19Z", "closed_at": "2025-04-02T12:25:30Z", "commit_id": "ce6522c2279c30ed6ac157cbfacd8e15cb1cfae2", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/371", "body": "After the sampler the model offloads to RAM which seems to cause my RAM to be 100% and freeze the entire computer for a minute or two, after which I assume the clipvision and/or TE is booted from RAM because after that my RAM is used like 50%. Stays on 50% if I don't change the prompt on subsequent runs. Is there any way to unload clipvision and TE from RAM before the sampler so that I can avoid this?", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/371/comments", "author": "fintarn", "comments": [{"user": "kijai", "created_at": "2025-04-02T10:22:38Z", "body": "In general in ComfyUI you can't free the RAM because that's how it all works, the node outputs are cached so that entire workflow doesn't always need to be ran from the beginning. I don't know of a way at least."}, {"user": "fintarn", "created_at": "2025-04-02T10:33:50Z", "body": "I'll try to use the comfy TE nodes that you included in the workflow and see if that solves it"}, {"user": "kijai", "created_at": "2025-04-02T11:11:48Z", "body": "> I'll try to use the comfy TE nodes that you included in the workflow and see if that solves it\n\nIt wouldn't change anything if you used same model, but there are smaller models to use that would then reserve less RAM, like the fp8 scaled."}, {"user": "fintarn", "created_at": "2025-04-02T12:25:30Z", "body": "Yeah, it seems the only thing that will prevent this is using the fp8 scaled model. Thanks."}], "satisfaction_conditions": ["A solution that prevents RAM from reaching 100% and freezing the computer after sampling", "A method to reduce memory usage of CLIP vision and TE models", "A workaround compatible with ComfyUI's caching architecture", "A practical approach that works with their existing 32GB RAM setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:04"}, "dockerfile": null, "language": "python"}
{"number": 226, "title": "The size of tensor a (32) must match the size of tensor b (36) at non-singleton dimension 1", "created_at": "2025-03-14T22:42:15Z", "closed_at": "2025-03-14T23:01:51Z", "commit_id": "84a26d30f9f96c72e481ae7688f09b0ccea6d9da", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/226", "body": "I was playing with the 1.3B Control Example, and tried to load wan2_1-I2V-14B-480_fp8_e4m3fn model, but whether I leave quantization disabled or pick fp8_e4m3fn I get this exception:\n\n```\nTraceback (most recent call last):\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 483, in loadmodel\n    new_in.weight[:, :old_in_dim].copy_(transformer.patch_embedding.weight)\nRuntimeError: The size of tensor a (32) must match the size of tensor b (36) at non-singleton dimension 1\n```\n\nShould I be not using the I2V model, after all the workflow's input is a video?", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/226/comments", "author": "3dluvr", "comments": [{"user": "3dluvr", "created_at": "2025-03-14T22:45:18Z", "body": "Actually, I tried the Wan2_1-T2V-14B-480p_fp8_e4m3fn model as well, and got this:\n\n```\nTraceback (most recent call last):\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 494, in loadmodel\n    patcher = apply_lora(patcher, device, transformer_load_device, params_to_keep=params_to_keep, dtype=dtype, base_dtype=base_dtype, state_dict=sd, low_mem_load=lora_low_mem_load)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\utils.py\", line 64, in apply_lora\n    model.patch_weight_to_device(\"{}.{}\".format(name, param), device_to=device_to)\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\comfy\\model_patcher.py\", line 561, in patch_weight_to_device\n    out_weight = comfy.lora.calculate_weight(self.patches[key], temp_weight, key)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\comfy\\lora.py\", line 518, in calculate_weight\n    weight = pad_tensor_to_shape(weight, reshape)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\ComfyUI_windows_portable\\ComfyUI\\comfy\\lora.py\", line 447, in pad_tensor_to_shape\n    raise ValueError(\"The new shape must be larger than the original tensor in all dimensions\")\nValueError: The new shape must be larger than the original tensor in all dimensions\n```\n\nIt appears it only works with the Wan2_1-T2V-1_3B-bf16 model..."}, {"user": "kijai", "created_at": "2025-03-14T22:54:57Z", "body": "> It appears it only works with the Wan2_1-T2V-1_3B-bf16 model...\n\nYeah, there's no control loras for other models than 1.3B yet."}, {"user": "3dluvr", "created_at": "2025-03-14T23:01:51Z", "body": "Ah, that would explain it...so many little nuances.\n\nThanks!!"}], "satisfaction_conditions": ["Explanation of model compatibility limitations with the Control Example workflow", "Clarification about which specific model works with the Control Example", "Acknowledgment that the user's technical errors were due to expected limitations rather than user error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:31"}, "dockerfile": null, "language": "python"}
{"number": 159, "title": "Teacache node not working or reducing video quality.", "created_at": "2025-03-05T23:39:45Z", "closed_at": "2025-03-09T00:23:32Z", "commit_id": "f4e706156f00c0a6a99cb0929a5d19c757a8c0cb", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/159", "body": "14b i2v 720p\nrel_l1_thresh     0.004\nEnable the use_comfficients option will not speed up video generation.\nDisable the use_comfficients option will accelerate the video, but the video quality will significantly decrease.", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/159/comments", "author": "L020304", "comments": [{"user": "kijai", "created_at": "2025-03-05T23:50:58Z", "body": "Use much higher threshold when using the coefficients, something like 0.2"}, {"user": "L020304", "created_at": "2025-03-09T00:23:32Z", "body": "teacache work now."}], "satisfaction_conditions": ["Guidance on appropriate threshold settings when using coefficients", "A solution that enables teacache to function properly", "A solution that maintains video quality while using coefficients"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:45"}, "dockerfile": null, "language": "python"}
{"number": 137, "title": "OOM issue using 3090 24G VRAM", "created_at": "2025-03-04T15:20:27Z", "closed_at": "2025-03-05T09:38:06Z", "commit_id": "721cd65e7b5224c70a3d20446d9d561f1732216b", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/137", "body": "Even using max swap 40 I still got this issue..... 24G isn't enought to run I2V?\n\ngot prompt                                                                                                                         \n!!! Exception during processing !!! Allocation on device                                                                           \nTraceback (most recent call last):                                                                                                 \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\ComfyUI\\execution.py\", line 327, in execute                                            \n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)                                                                                                                 \n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                 \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\ComfyUI\\execution.py\", line 202, in get_output_data                                    \n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)                                                                                                \n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\ComfyUI\\execution.py\", line 174, in _map_node_over_list                                \n    process_inputs(input_dict, i)                                                                                                  \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\ComfyUI\\execution.py\", line 163, in process_inputs                                     \n    results.append(getattr(obj, func)(**inputs))                                                                                   \n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                    \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 307, in loadmodel         \n    sd = load_torch_file(model_path, device=transformer_load_device, safe_load=True)                                               \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                               \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\ComfyUI\\comfy\\utils.py\", line 62, in load_torch_file                                   \n    raise e                                                                                                                        \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\ComfyUI\\comfy\\utils.py\", line 54, in load_torch_file                                   \n    sd = safetensors.torch.load_file(ckpt, device=device.type)                                                                     \n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                     \n  File \"D:\\StableDiffusion\\ComfyUI-aki-v1.6\\python\\Lib\\site-packages\\safetensors\\torch.py\", line 315, in load_file                 \n    result[k] = f.get_tensor(k)                                                                                                    \n                ^^^^^^^^^^^^^^^                                                                                                    \ntorch.OutOfMemoryError: Allocation on device                                                                                       \n                                                                                                                                   \nGot an OOM, unloading all loaded models.                                                                                           \nPrompt executed in 8.58 seconds ", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/137/comments", "author": "Jasonzhangf", "comments": [{"user": "kijai", "created_at": "2025-03-04T15:50:29Z", "body": "Already when loading the model? How much RAM do you have?"}, {"user": "Jasonzhangf", "created_at": "2025-03-05T00:42:36Z", "body": "yes, I tried many times and even tried reinstall the node and restart the computer.\r\n\r\n\r\nI've got 24G Vram.\r\n\r\n\r\n\r\n---Original---\r\nFrom: \"Jukka ***@***.***&gt;\r\nDate: Tue, Mar 4, 2025 23:50 PM\r\nTo: ***@***.***&gt;;\r\nCc: ***@***.******@***.***&gt;;\r\nSubject: Re: [kijai/ComfyUI-WanVideoWrapper] OOM issue using 3090 24G VRAM(Issue #137)\r\n\r\n\r\n   \r\nAlready when loading the model? How much RAM do you have?\r\n\r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***&gt;\r\n  kijai left a comment (kijai/ComfyUI-WanVideoWrapper#137)\r\n \r\nAlready when loading the model? How much RAM do you have?\r\n \r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***&gt;"}, {"user": "kijai", "created_at": "2025-03-05T00:45:45Z", "body": "I meant RAM, system memory, at that point the model is being loaded there based on that log."}, {"user": "kijai", "created_at": "2025-03-05T00:46:31Z", "body": "Or it should be if you have offload_device selected on the loader node that is..."}, {"user": "Jasonzhangf", "created_at": "2025-03-05T03:22:53Z", "body": "I've got 32G RAM. Is that too less for this application? I acctually can run wan2.1 720p with GGUF model."}, {"user": "Jasonzhangf", "created_at": "2025-03-05T05:56:03Z", "body": "Actually I tried to play with the offload_device with nearly all the combinations(clip textencoder/T5 text encoder/main model) but still failed:\n1. All offload or All main devices: there's slight change on RAM usage like from 30% to 4x%, but VRAM will go to 99% quickly when loading main models and then OOM.\n2. Textencoders and T5 text encoders offload, main model main devices, the same.\n3. main models->main device, Text encoders/T5 text encoders offload, the same.\n4. with swap and without, the same.\n\n-------------------------\nOr it should be if you have offload_device selected on the loader node that is..."}, {"user": "kijai", "created_at": "2025-03-05T06:28:09Z", "body": "> Actually I tried to play with the offload_device with nearly all the combinations(clip textencoder/T5 text encoder/main model) but still failed:\n> 1. All offload or All main devices: there's slight change on RAM usage like from 30% to 4x%, but VRAM will go to 99% quickly when loading main models and then OOM.\n> 2. Textencoders and T5 text encoders offload, main model main devices, the same.\n> 3. main models->main device, Text encoders/T5 text encoders offload, the same.\n> 4. with swap and without, the same.\n> \n> -------------------------\n> Or it should be if you have offload_device selected on the loader node that is...\n\nYou don't happen to be using --high-vram mode? "}, {"user": "Jasonzhangf", "created_at": "2025-03-05T09:37:35Z", "body": "> > Actually I tried to play with the offload_device with nearly all the combinations(clip textencoder/T5 text encoder/main model) but still failed:\n> > \n> > 1. All offload or All main devices: there's slight change on RAM usage like from 30% to 4x%, but VRAM will go to 99% quickly when loading main models and then OOM.\n> > 2. Textencoders and T5 text encoders offload, main model main devices, the same.\n> > 3. main models->main device, Text encoders/T5 text encoders offload, the same.\n> > 4. with swap and without, the same.\n> > \n> > \n> > Or it should be if you have offload_device selected on the loader node that is...\n> \n> You don't happen to be using --high-vram mode?\n\nOh, shit. That's the reason. Actually I'm using a package from aaaki and there's setting of graphic card with a caption whether it's over 8G or not. I found If I set it to be over 8Gb it will use --high-vram mode. Thanks buddy, you are my hero!"}], "satisfaction_conditions": ["Identification of the root cause of the OOM error when running I2V on a 24GB VRAM GPU", "A specific configuration change that allows the model to load successfully on the user's hardware", "Guidance on appropriate memory management settings for the user's specific hardware configuration"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:03:52"}, "dockerfile": null, "language": "python"}
{"number": 68, "title": "Unknown attribute allow_fp16_accumulation", "created_at": "2025-02-28T11:40:45Z", "closed_at": "2025-02-28T23:21:44Z", "commit_id": "a2bb63d546642ef52a03dcb54726efa35b26b29f", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/68", "body": "After updating to the latest version I receive the following error. It worked fine before I updated Comfy and the Wan nodes.\n\n```\n!!! Exception during processing !!! Unknown attribute allow_fp16_accumulation\nTraceback (most recent call last):\n  File \"D:\\SD\\ComfyUI-Test\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\SD\\ComfyUI-Test\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\SD\\ComfyUI-Test\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"D:\\SD\\ComfyUI-Test\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\SD\\ComfyUI-Test\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 290, in loadmodel\n    torch.backends.cuda.matmul.allow_fp16_accumulation = True\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\SD\\ComfyUI-Test\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\torch\\backends\\cuda\\__init__.py\", line 144, in __setattr__\n    raise AttributeError(\"Unknown attribute \" + name)\nAttributeError: Unknown attribute allow_fp16_accumulation\n\nPrompt executed in 109.53 seconds\n```\n\nMy environment:\n- Windows 11\n- Python version: 3.12.8\n- Latest Comfy\n- CUDA 12.6\n- CuDNN 8.9.7\n- PyTorch version: 2.6.0+cu126\n- SageAttention\n\n", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/68/comments", "author": "andypotato", "comments": [{"user": "coddz", "created_at": "2025-02-28T11:44:56Z", "body": "me too\n\nWindows 11\nPython version: 3.12.7\nLatest Comfy\nCUDA 12.4\nPyTorch version: 2.5.1+cu124\nSageAttention"}, {"user": "kijai", "created_at": "2025-02-28T11:47:18Z", "body": "Uhhh sorry stupid mistake, that should've been optional but I missed something silly. Update now and it should work.\n\nThe reason for this update is that in torch 2.7.0 nightly there this feature:\n\n```\nFull FP16 Accmumulation in FP16 GEMMs\n-------------------------------------\n\nCertain GPUs have increased performance when doing _all_ FP16 GEMM accumulation\nin FP16, at the cost of numerical precision and greater likelihood of overflow.\nNote that this setting only has an effect on GPUs of compute capability 7.0 (Volta)\nor newer.\n\nThis behavior can be enabled via:\n\n  torch.backends.cuda.matmul.allow_fp16_accumulation = True\n```\n\nWhich makes using fp16 as the base_precision run the model lot faster, even if you use fp8 quantization."}, {"user": "andypotato", "created_at": "2025-02-28T23:21:40Z", "body": "I can confirm this is fixed - Thank you!"}, {"user": "colorant", "created_at": "2025-03-02T11:08:59Z", "body": "Does this one been merged already? still encounter this issue even that I have update the code to 2025/3/2 's main branch."}, {"user": "willmurdoch", "created_at": "2025-03-03T18:36:05Z", "body": "Same problem here!"}, {"user": "drphero", "created_at": "2025-03-12T14:04:26Z", "body": "> Which makes using fp16 as the base_precision run the model lot faster, even if you use fp8 quantization.\n\nIs base_precision the weight_type or the compute_type?"}, {"user": "kijai", "created_at": "2025-03-12T14:06:46Z", "body": "> > Which makes using fp16 as the base_precision run the model lot faster, even if you use fp8 quantization.\n> \n> Is base_precision the weight_type or the compute_type?\n\nCompute."}], "satisfaction_conditions": ["A fix for the 'Unknown attribute allow_fp16_accumulation' error", "Compatibility with the user's PyTorch version", "Ability to continue using the Wan nodes with the latest ComfyUI version", "Clear explanation of why the error occurred"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:04:07"}, "dockerfile": null, "language": "python"}
{"number": 44, "title": "crop error", "created_at": "2025-02-26T19:01:40Z", "closed_at": "2025-02-26T20:53:06Z", "commit_id": "23ef23826bd4ae532d47088397570932aa2cc334", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/44", "body": "i get this error when trying to run: clip_preprocess() got an unexpected keyword argument 'crop'\n\nany ideas?", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/44/comments", "author": "fuzzyfazzy", "comments": [{"user": "kijai", "created_at": "2025-02-26T19:49:51Z", "body": "It's a core ComfyUI function, so all I can think of is that your ComfyUI itself is too old."}, {"user": "fuzzyfazzy", "created_at": "2025-02-26T20:52:54Z", "body": "Perfect thankyou! I updated comfy and works! "}], "satisfaction_conditions": ["Identification of the root cause of the 'crop' keyword argument error", "A practical solution that resolves the 'crop' error", "Guidance related to ComfyUI version compatibility"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:04:13"}, "dockerfile": null, "language": "python"}
{"number": 29, "title": "720p I2V model produces poor results", "created_at": "2025-02-26T11:22:49Z", "closed_at": "2025-02-26T12:41:30Z", "commit_id": "b0fa8123c638200de08bb48c7a39f7cd39836e00", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/29", "body": "The 480p I2V model produces excellent results 🥰👍; \nHowever, the 720p I2V model produces poor results 🥲, full of flashes and meaningless interference.\n\nAre there any parameters that need to be adjusted to make the 720p model work properly?\n(I have tried increasing the steps, but the results are still poor.)", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/29/comments", "author": "zyd232", "comments": [{"user": "kijai", "created_at": "2025-02-26T12:12:39Z", "body": "What resolution are you using? I haven't tested it much, but I assume for it to work properly you'd need to use much higher resolutions."}, {"user": "jpgallegoar", "created_at": "2025-02-26T12:21:21Z", "body": "try this:\n1280x720x81f, 30 steps 6cfg 5 shift, dpm++_sde, negative prompt = 色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走，相机移动"}, {"user": "zyd232", "created_at": "2025-02-26T12:41:30Z", "body": "> try this: 1280x720x81f, 30 steps 6cfg 5 shift, dpm++_sde, negative prompt = 色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走，相机移动\n\nIt works! Thx 🫡"}], "satisfaction_conditions": ["Specific parameter settings that improve the 720p I2V model output quality", "A solution that eliminates visual artifacts like flashes and interference in the 720p model output", "Configuration guidance that works specifically for the higher resolution (720p) model"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:04:29"}, "dockerfile": null, "language": "python"}
{"number": 7, "title": "umt5-xxl-enc-bf16 OOM with 12GB VRAM", "created_at": "2025-02-25T23:47:38Z", "closed_at": "2025-02-26T16:28:12Z", "commit_id": "b81ea1d0f7dfc4e13c619be061f8a692eccaa7f9", "labels": [], "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/7", "body": "The clip do not load with 12GB Vram (3090). Is it possible to get a FP8?\n\ngot prompt\n!!! Exception during processing !!! Allocation on device\nTraceback (most recent call last):\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 327, in execute\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 202, in get_output_data\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 174, in _map_node_over_list\n    process_inputs(input_dict, i)\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 163, in process_inputs\n    results.append(getattr(obj, func)(**inputs))\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\nodes.py\", line 460, in loadmodel\n    T5_text_encoder = T5EncoderModel(\n                      ^^^^^^^^^^^^^^^\n  File \"G:\\SD\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-WanVideoWrapper\\wanvideo\\modules\\t5.py\", line 499, in __init__\n    set_module_tensor_to_device(model, name, device=device, dtype=dtype, value=state_dict[name])\n  File \"G:\\SD\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 330, in set_module_tensor_to_device\n    new_value = value.to(device)\n                ^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: Allocation on device\n\nGot an OOM, unloading all loaded models.\nPrompt executed in 3.95 seconds", "comments_url": "https://api.github.com/repos/kijai/ComfyUI-WanVideoWrapper/issues/7/comments", "author": "narikm", "comments": [{"user": "Skol600ml", "created_at": "2025-02-26T00:02:28Z", "body": "You need to reduce the tile sizes "}, {"user": "narikm", "created_at": "2025-02-26T00:15:00Z", "body": "> You need to reduce the tile sizes\n\nIt OOM before that point, as it simply load it."}, {"user": "itswhateverman", "created_at": "2025-02-26T00:21:55Z", "body": "i had to switch from the fp32 to the bf16 vae on 12gb for the text encoder not to OOM. seemed vae loads first and is just enough to make the difference. once the text encoder output is cached i can switch back, until i adjust the prompt (using the t2v example) "}, {"user": "narikm", "created_at": "2025-02-26T00:32:26Z", "body": "> i had to switch from the fp32 to the bf16 vae on 12gb for the text encoder not to OOM. seemed vae loads first and is just enough to make the difference. once the text encoder output is cached i can switch back, until i adjust the prompt (using the t2v example)\n\nStill OOM, but at the \"Wan text encode\" node."}, {"user": "JoeAu", "created_at": "2025-02-26T08:49:09Z", "body": "Is it possible to use FP8 or bnb4 quantization for T5-XXL, or run it on a CPU?"}, {"user": "Foul-Tarnished", "created_at": "2025-02-26T09:25:23Z", "body": "A rtx3090 has 24gb ??"}, {"user": "kijai", "created_at": "2025-02-26T09:36:19Z", "body": "> Is it possible to use FP8 or bnb4 quantization for T5-XXL, or run it on a CPU?\n\nAdded that now, seems to use ~4GB less VRAM for encoding, got past that stage under 10GB VRAM used when I tested now."}, {"user": "kijai", "created_at": "2025-02-26T09:58:12Z", "body": "> I'm having the same issue since i2v has clip + t5 loaded before it starts sampling which needs more than 12GB VRAM. It only occurs once in t2v.\n> \n> However I'm assuming you would still get an OOM when it tries to load the model since I can use t2v 1.3B but still got an OOM at t2v 14B (40% swap + compile) which is about the same size as i2v model.\n> \n> The fp8 clips are a necessity though.\n\nClip and T5 are not in VRAM at same time at any point as long as you have the force_offload enabled in the node (default)."}, {"user": "narikm", "created_at": "2025-02-26T16:28:12Z", "body": "Resolved by pulling new version."}], "satisfaction_conditions": ["A solution that reduces VRAM usage enough to run the T5-XXL encoder on a 12GB GPU", "A quantization approach that maintains model functionality while reducing memory requirements", "A solution that addresses the specific OOM error during model loading phase"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:04:44"}, "dockerfile": null, "language": "python"}
{"number": 11, "title": "Python API (developer mode) - kiss_slam_pipeline throws errors", "created_at": "2025-03-25T11:06:04Z", "closed_at": "2025-03-25T13:44:30Z", "commit_id": "45e0989ec64a3c1fc0605a915e30531e0aa97190", "labels": [], "url": "https://github.com/PRBonn/kiss-slam/issues/11", "body": "Thank you for publishing your work!\n\nAfter your latest commit, I am able to install the package as an editable. However, \n`$ kiss_slam_pipeline -v <dataset>`\nis giving the following error:\n\n> Running cmake --build & --install in /home/kiss-slam/build/cp312-cp312-linux_x86_64\n> [0/1] Re-running CMake...\n> /bin/sh: 1: /tmp/pip-build-env-aoqahp9b/normal/lib/python3.12/site-packages/cmake/data/bin/cmake: not found\n> FAILED: build.ninja \n> /tmp/pip-build-env-aoqahp9b/normal/lib/python3.12/site-packages/cmake/data/bin/cmake --regenerate-during-build -S/home/kiss-slam -B/home/kiss-slam/build/cp312-cp312-linux_x86_64\n> ninja: error: rebuilding 'build.ninja': subcommand failed\n> ERROR: None\n\nPlease let me know if I am doing something wrong.", "comments_url": "https://api.github.com/repos/PRBonn/kiss-slam/issues/11/comments", "author": "shin-ka", "comments": [{"user": "benemer", "created_at": "2025-03-25T11:42:05Z", "body": "Please try installing with\n\n```\npip install --no-build-isolation -ve .\n```"}, {"user": "shin-ka", "created_at": "2025-03-25T13:44:30Z", "body": "Thank you this works"}], "satisfaction_conditions": ["A working installation method for the kiss_slam package that resolves the build errors", "A command-line solution that allows the user to successfully use the kiss_slam_pipeline tool", "Clear instructions that address the specific pip installation configuration needed for this package"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:08:02"}, "dockerfile": "FROM ubuntu:24.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PYTHONUNBUFFERED=1\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    python3-full \\\n    python3-pip \\\n    python3-venv \\\n    libeigen3-dev \\\n    libsuitesparse-dev \\\n    cmake \\\n    build-essential \\\n    ninja-build \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/PRBonn/kiss-slam.git . && \\\n    git checkout 45e0989ec64a3c1fc0605a915e30531e0aa97190\n\n# Create virtual environment and install in editable mode\nRUN python3 -m venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\nRUN pip3 install --upgrade pip && \\\n    pip3 install --no-cache-dir -e .\n\n# Set default command\nCMD [\"/bin/bash\"]", "language": "python"}
{"number": 45, "title": "[WinError 2] The system cannot find the file specified", "created_at": "2025-03-08T10:25:11Z", "closed_at": "2025-03-25T23:57:18Z", "commit_id": "7d1bf783b1d591aefb09b2dbbdd967e2c732aedb", "labels": ["bug"], "url": "https://github.com/lastmile-ai/mcp-agent/issues/45", "body": "I'm using the example code available on the README.md, I just made some changes to add new servers (todoist and brave-search).\n\n_mcp_agents.config.yaml_\n```yaml\nmcp:\n  servers:\n    todoist:\n      command: \"npx\"\n      args: [ \"@abhiz123/todoist-mcp-server\", \"-y\"]\n    brave-search:\n      command: \"npx\"\n      args: [\"@modelcontextprotocol/server-brave-search\", \"-y\"]\n    fetch:\n      command: \"uvx\"\n      args: [\"mcp-server-fetch\"]\n\nopenai:\n  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored\n  default_model: gpt-4o\n```\n\n_main.py_\n```python\nimport asyncio\n\nfrom mcp_agent.agents.agent import Agent\nfrom mcp_agent.app import MCPApp\nfrom mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n\napp = MCPApp(name=\"mcp_basic_agent\")\n\n\nasync def example_usage():\n    async with app.run() as agent_app:\n        logger = agent_app.logger\n        personal_assistant = Agent(\n            name=\"personal-assistant\",\n            instruction=\"\"\"You are a personal assistant. You are able to help the user with their queries.\"\"\",\n            server_names=[\"fetch\", \"todoist\"],\n        )\n\n        async with personal_assistant:\n            logger.info(\"personal-assistant: Connected to server, calling list_tools...\")\n            result = await personal_assistant.list_tools()\n            logger.info(\"Tools available:\", data=result.model_dump())\n\n            llm = await personal_assistant.attach_llm(OpenAIAugmentedLLM)\n            result = await llm.generate_str(\n                message=\"Show my tasks due today\",\n            )\n            logger.info(f\"Result: {result}\")\n\n\nif __name__ == \"__main__\":\n    import time\n\n    start = time.time()\n    asyncio.run(example_usage())\n    end = time.time()\n    t = end - start\n\n    print(f\"Total run time: {t:.2f}s\")\n```\n\nI had this code running on macOS and it worked. But when I try to run the same code on Windows 11, I get:\n`[ERROR] 2025-03-08T07:09:11 mcp_agent.mcp.mcp_connection_manager - todoist: Lifecycle task encountered an error: [WinError 2] The system cannot find the file specified`\n\nThe full stacktrace:\n```\nTraceback (most recent call last):\n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\app.py\", line 172, in run                                                                                                                          \n    yield self                                                                                                                                                                                                                  \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\main.py\", line 19, in example_usage                                                                                                                                                  \n    async with personal_assistant:                                                                                                                                                                                              \n               ^^^^^^^^^^^^^^^^^^                                                                                                                                                                                               \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\mcp\\mcp_aggregator.py\", line 70, in __aenter__                                                                                                     \n    await self.load_servers()                                                                                                                                                                                                   \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\mcp\\mcp_aggregator.py\", line 179, in load_servers                                                                                                  \n    await self._persistent_connection_manager.get_server(                                                                                                                                                                       \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\mcp\\mcp_connection_manager.py\", line 278, in get_server\n    raise RuntimeError(                                                                                                                                                                                                         \nRuntimeError: todoist: Failed to initialize server; check logs for errors.                                                                                                                                                      \n                                                                                                                                                                                                                                \nDuring handling of the above exception, another exception occurred:                                                                                                                                                             \n                                                                                                                                                                                                                                \nTraceback (most recent call last):                                                                                                                                                                                              \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\main.py\", line 42, in <module>                                                                                                                                                       \n    asyncio.run(example_usage())                                                                                                                                                                                                \n  File \"C:\\Users\\Acer\\AppData\\Roaming\\uv\\python\\cpython-3.12.8-windows-x86_64-none\\Lib\\asyncio\\runners.py\", line 194, in run                                                                                                    \n    return runner.run(main)                                                                                                                                                                                                     \n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Acer\\AppData\\Roaming\\uv\\python\\cpython-3.12.8-windows-x86_64-none\\Lib\\asyncio\\runners.py\", line 118, in run                                                                                                    \n    return self._loop.run_until_complete(task)                                                                                                                                                                                  \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                  \n  File \"C:\\Users\\Acer\\AppData\\Roaming\\uv\\python\\cpython-3.12.8-windows-x86_64-none\\Lib\\asyncio\\base_events.py\", line 686, in run_until_complete                                                                                 \n    return future.result()                                                                                                                                                                                                      \n           ^^^^^^^^^^^^^^^                                                                                                                                                                                                      \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\main.py\", line 11, in example_usage                                                                                                                                                  \n    async with app.run() as agent_app:                                                                                                                                                                                          \n               ^^^^^^^^^\n  File \"C:\\Users\\Acer\\AppData\\Roaming\\uv\\python\\cpython-3.12.8-windows-x86_64-none\\Lib\\contextlib.py\", line 231, in __aexit__                                                                                                   \n    await self.gen.athrow(value)                                                                                                                                                                                                \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\app.py\", line 174, in run                                                                                                                          \n    await self.cleanup()                                                                                                                                                                                                        \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\app.py\", line 156, in cleanup                                                                                                                      \n    await cleanup_context()                                                                                                                                                                                                     \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\context.py\", line 215, in cleanup_context                                                                                                          \n    await LoggingConfig.shutdown()                                                                                                                                                                                              \n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\logging\\logger.py\", line 240, in shutdown                                                                                                          \n    await bus.stop()\n  File \"D:\\code-projects\\ai\\mcp-agent-test\\.venv\\Lib\\site-packages\\mcp_agent\\logging\\transport.py\", line 322, in stop                                                                                                           \n    await asyncio.wait_for(self._queue.join(), timeout=5.0)                                                                                                                                                                     \n  File \"C:\\Users\\Acer\\AppData\\Roaming\\uv\\python\\cpython-3.12.8-windows-x86_64-none\\Lib\\asyncio\\tasks.py\", line 520, in wait_for                                                                                                 \n    return await fut                                                                                                                                                                                                            \n           ^^^^^^^^^                                                                                                                                                                                                            \n  File \"C:\\Users\\Acer\\AppData\\Roaming\\uv\\python\\cpython-3.12.8-windows-x86_64-none\\Lib\\asyncio\\queues.py\", line 215, in join                                                                                                    \n    await self._finished.wait()                                                                                                                                                                                                 \n  File \"C:\\Users\\Acer\\AppData\\Roaming\\uv\\python\\cpython-3.12.8-windows-x86_64-none\\Lib\\asyncio\\locks.py\", line 212, in wait\n    await fut                                                                                                                                                                                                                   \nasyncio.exceptions.CancelledError: Cancelled by cancel scope 28e9c879130\n```\nI tried only using the `fetch` MCP server, and it works. It only breaks when I add any of the others.\n\n\nI thought it was an issue with my node / npm installation, but I tried to run these MCP servers on Claude-Desktop and it worked. \n**Node version**: v23.9.0\n**npx version**:  10.9.2\n**mcp_agent**: >=0.0.8\n**python version**: 3.12.8\n\nI would appreciate any help you can give me.", "comments_url": "https://api.github.com/repos/lastmile-ai/mcp-agent/issues/45/comments", "author": "DaviRolim", "comments": [{"user": "saqadri", "created_at": "2025-03-08T20:26:45Z", "body": "@DaviRolim thank you for the detailed repro steps and the diligence you went through to investigate the issue yourself! I will look into this, I haven't done a ton of testing on Windows so it's possible I missed something. I'll investigate and get back to you!"}, {"user": "saqadri", "created_at": "2025-03-10T02:41:08Z", "body": "@DaviRolim I think I know what is happening. I will test this out but I think the issue is with \"npx\". \n\nThere likely needs to be a fix in `transport_context_factory` in `MCPConnectionManager` and `ServerRegistry` classes, but can you try the following for me and let me know:\n\n1. Run `where npx` in your terminal (I think it should be C:\\Program Files\\nodejs\\npx.cmd or\nC:\\Users\\YourUsername\\AppData\\Roaming\\npm\\npx.cmd)\n2. Take the path from 1 and replace the `npx` instances in `mcp_agent.config.yaml` with the full path to npx instead\n3. Retry and see if that works. Also try just `npx.cmd`"}, {"user": "DaviRolim", "created_at": "2025-03-10T11:38:40Z", "body": "Thank you @saqadri. Using `npx.cmd` instead of `npx` works."}, {"user": "saqadri", "created_at": "2025-03-10T12:41:01Z", "body": "> Thank you @saqadri. Using `npx.cmd` instead of `npx` works.\n\nGreat to know! I will add some special handling in the connection manager so this can be done automatically."}, {"user": "saqadri", "created_at": "2025-03-25T23:57:18Z", "body": "@DaviRolim @yeshan333 this has been fixed in v0.0.13. Please let me know if you run into any other issues in Windows."}], "satisfaction_conditions": ["A working solution for running MCP servers on Windows", "A fix for the 'system cannot find the file specified' error when using npx on Windows", "Compatibility with Windows command execution conventions", "A solution that maintains the same functionality as on macOS"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:06:13"}, "dockerfile": "FROM python:3.12-slim\n\n# Add labels\nLABEL maintainer=\"MCP Agent Team\"\nLABEL description=\"Environment for validating mcp-agent with Node.js MCP servers\"\n\n# Set environment variables\nENV PYTHONUNBUFFERED=1 \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_NO_CACHE_DIR=1\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    git \\\n    gnupg \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Node.js and npm\nRUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \\\n    && apt-get install -y --no-install-recommends nodejs \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install uv (Python package manager)\nRUN pip install uv\n\n# Create app directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/lastmile-ai/mcp-agent.git . \\\n    && git checkout 7d1bf783b1d591aefb09b2dbbdd967e2c732aedb\n\n# Create a virtual environment and install project dependencies with uv\nRUN uv venv && uv pip install -e .\n\n# Install Node.js MCP servers globally\nRUN npm install -g @abhiz123/todoist-mcp-server @modelcontextprotocol/server-brave-search\n\n# Install uvx for the fetch server\nRUN pip install uvicorn mcp-server-fetch\n\n# Create directory for user files\nRUN mkdir -p /app/user_files\n\n# Set working directory for user files\nWORKDIR /app/user_files\n\n# Copy example config file from the cloned repository\nRUN cp /app/examples/mcp_basic_agent/mcp_agent.config.yaml /app/user_files/mcp_agent.config.yaml\n\n# Create a placeholder secrets file (user will need to provide their own)\nRUN echo \"openai:\\n  api_key: your_api_key_here\" > /app/user_files/mcp_agent.secrets.yaml\n\n# Verify Node.js and npm versions\nRUN node --version && npm --version\n\n# Verify Python and uv versions\nRUN python --version && uv --version\n\n# The user will need to mount their code to this directory\nVOLUME [\"/app/user_files\"]", "language": "python"}
{"number": 51, "title": "Downgrading the minimum numpy version to 2.1.3.", "created_at": "2025-03-11T10:16:35Z", "closed_at": "2025-03-11T11:04:52Z", "commit_id": "d03e54efc0c401ce60b1598b5ecc97699d6b4aaa", "labels": [], "url": "https://github.com/lastmile-ai/mcp-agent/issues/51", "body": "Is it possible to downgrade the minimum numpy version to 2.1.3? I am having issues integrating mcp-agent into our project because of our dependency on numba, which hasn't yet upgraded to numpy 2.2.\n\nI have no idea if this is even a valid request to make.", "comments_url": "https://api.github.com/repos/lastmile-ai/mcp-agent/issues/51/comments", "author": "Nicba1010", "comments": [{"user": "saqadri", "created_at": "2025-03-11T11:00:16Z", "body": "> Is it possible to downgrade the minimum numpy version to 2.1.3? I am having issues integrating mcp-agent into our project because of our dependency on numba, which hasn't yet upgraded to numpy 2.2.\n> \n> I have no idea if this is even a valid request to make.\n\n@Nicba1010 let me try, I think that should be ok!"}, {"user": "saqadri", "created_at": "2025-03-11T11:04:38Z", "body": "Thanks for submitting the fix! I'll push a patch update today to pypi "}, {"user": "Nicba1010", "created_at": "2025-03-11T11:46:58Z", "body": "Thank you for merging it that quickly!"}], "satisfaction_conditions": ["Compatibility with numpy version 2.1.3 instead of requiring numpy 2.2", "Timely implementation of the version requirement change", "Official release of the updated version requirements to package repositories"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:06:03"}, "dockerfile": null, "language": "python"}
{"number": 106, "title": "Timestep \"starvation at tail\"", "created_at": "2025-01-21T11:36:59Z", "closed_at": "2025-01-21T14:57:32Z", "commit_id": "caf9f0c4670b462b51c845abff7f5731ed138364", "labels": [], "url": "https://github.com/Lightricks/LTX-Video/issues/106", "body": "Hi @yoavhacohen, thanks for the repo and your hard work.\n\nCan you please clarify a thing in your paper:\n\n> \"To prevent starvation at the tail of the resolution we clamp the pdf at percentiles 0.5 and 99.9.\"\n\nYou say that you don't want zero probabilities for the tails during training. Why does this matter if the minimum timestep you use during inference is 100 (due to the `shift_terminal` = 0.1)? Or is the the `shift_terminal` also applied during training?", "comments_url": "https://api.github.com/repos/Lightricks/LTX-Video/issues/106/comments", "author": "donthomasitos", "comments": [{"user": "yoavhacohen", "created_at": "2025-01-21T13:43:29Z", "body": "Hi @donthomasitos, thanks for your interest in our work!\n\nEven at t=0.1￼, there weren’t enough samples when the sequence was long.\nAdditionally, due to generalization from the contiguous timestep range, adding margins around the time steps in use helps make the distribution of samples contributing to each timestep more even."}, {"user": "donthomasitos", "created_at": "2025-01-21T14:57:32Z", "body": "Thank you!"}], "satisfaction_conditions": ["An explanation of why preventing timestep starvation at the tails is important despite minimum inference timestep constraints", "Clarification on how training and inference timestep handling differs", "Technical rationale for the PDF clamping approach mentioned in the paper"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:19"}, "dockerfile": null, "language": "python"}
{"number": 36, "title": "v3 overwriting .wav files creating incomplete short book", "created_at": "2025-01-29T23:44:23Z", "closed_at": "2025-02-01T23:22:03Z", "commit_id": "12fbf89fccfe5cf0b0a2eadfb462f0238a9acfe1", "labels": ["v3"], "url": "https://github.com/santinic/audiblez/issues/36", "body": "After updating to v3 `pip install --upgrade audiblez` and creating a new book `audiblez Durarara\\ Vol\\ 4.epub  -v af_bella  -s 1.0` short chapters are created and if looking directly at the folder in File Explorer, you notice the file always changing in size and often becoming much shorter. Tested on new virtual env as well.", "comments_url": "https://api.github.com/repos/santinic/audiblez/issues/36/comments", "author": "erictbar", "comments": [{"user": "santinic", "created_at": "2025-01-30T08:30:42Z", "body": "Yes, thanks, I rolled back to 0.2.2. v3 will need more work"}, {"user": "sameh0", "created_at": "2025-01-31T11:34:22Z", "body": "@erictbar could you please checkout if the fix works for you ?"}, {"user": "erictbar", "created_at": "2025-01-31T14:13:51Z", "body": "Yes, branch `v3` is working for me."}, {"user": "santinic", "created_at": "2025-01-31T16:33:03Z", "body": "@erictbar fix chunks up the text file basically at random, so the pronunciation is unnatural. I'm moving v3 to use spacy for sentence splitting"}, {"user": "santinic", "created_at": "2025-02-01T12:05:28Z", "body": "Please, update and try again. v3.1 comes with a lot of changes"}], "satisfaction_conditions": ["A fix for the issue of incomplete/shortened .wav files in v3", "Proper handling of text-to-speech processing that doesn't cut audio files short", "A stable version that processes complete audiobooks correctly", "Compatibility with the command line interface pattern they were using"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:18:05"}, "dockerfile": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies including ffmpeg\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n    git \\\n    ffmpeg \\\n    wget \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/santinic/audiblez.git . && \\\n    git checkout 12fbf89fccfe5cf0b0a2eadfb462f0238a9acfe1\n\n# Install poetry\nRUN pip install --no-cache-dir poetry\n\n# Configure poetry to not create a virtual environment\nRUN poetry config virtualenvs.create false\n\n# Install dependencies and build the project\nRUN poetry install\n\n# Download required model files\nRUN wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files/kokoro-v0_19.onnx && \\\n    wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files/voices.json\n\n# Set the entrypoint to the audiblez command\nENTRYPOINT [\"audiblez\"]\n\n# Default command shows help\nCMD [\"--help\"]", "language": "python"}
{"number": 75, "title": "markdown and tabulate packages not installed", "created_at": "2025-02-21T13:01:47Z", "closed_at": "2025-02-21T14:52:38Z", "commit_id": "31b721d8b963db39bb40d68f99fdade7fece55a0", "labels": ["bug"], "url": "https://github.com/santinic/audiblez/issues/75", "body": "Debian 12, Python 3.11.2\nAfter running the recommended setup steps:\n```\nsudo apt install ffmpeg espeak-ng                   # on Ubuntu/Debian 🐧\npip install audiblez\n```\nerrors occur requiring that I install `markdown` and `tabulate`", "comments_url": "https://api.github.com/repos/santinic/audiblez/issues/75/comments", "author": "WiFlag", "comments": [{"user": "santinic", "created_at": "2025-02-21T14:28:10Z", "body": "fixed in latest v0.4.7, please update and check"}, {"user": "WiFlag", "created_at": "2025-02-21T14:49:53Z", "body": "👍 fixed"}], "satisfaction_conditions": ["A solution that resolves the missing dependencies issue with markdown and tabulate packages", "A solution that requires minimal additional effort from the user", "Clear instructions on how to implement the solution"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:42"}, "dockerfile": null, "language": "python"}
{"number": 70, "title": "Error in the create_index_file step with non-ascii characters in author name (e.g. \"é\" in French)", "created_at": "2025-02-18T16:51:39Z", "closed_at": "2025-02-21T14:28:18Z", "commit_id": "f2ffaf7daaccc4be76ae32824c5a63b74b1158c7", "labels": ["bug"], "url": "https://github.com/santinic/audiblez/issues/70", "body": "At the end of generating the `.wav` files, the conversion process failed to conclude with the following exception:\n\n```\nEstimated time remaining: 00d 00h 00m 07s\nProgress: 99%\n\nChapter written to /redacted/audiblez/Barjavel, Rene - Ravage_chapter_5_ff_siwis_Ops_013.html.wav\nChapter 5 read in 29.37 seconds (977 characters per second)\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/zertrin/.local/pipx/venvs/audiblez/lib/python3.10/site-packages/audiblez/ui.py\", line 572, in run\n    core.main(**self.params, post_event=self.post_event)\n  File \"/home/zertrin/.local/pipx/venvs/audiblez/lib/python3.10/site-packages/audiblez/core.py\", line 121, in main\n    create_index_file(title, creator, chapter_wav_files, output_folder)\n  File \"/home/zertrin/.local/pipx/venvs/audiblez/lib/python3.10/site-packages/audiblez/core.py\", line 297, in create_index_file\n    f.write(f\";FFMETADATA1\\ntitle={title}\\nartist={creator}\\n\\n\")\nUnicodeEncodeError: 'ascii' codec can't encode character '\\xe9' in position 36: ordinal not in range(128)\n```\n\nCharacter `\\xe9` is indeed `é`.\n\nI would expect the software to handle UTF-8 characters.", "comments_url": "https://api.github.com/repos/santinic/audiblez/issues/70/comments", "author": "zertrin", "comments": [{"user": "santinic", "created_at": "2025-02-19T13:28:06Z", "body": "I think your system default is not utf8. I'll force that\n"}, {"user": "santinic", "created_at": "2025-02-19T14:04:58Z", "body": "this should be fixed in latest release v0.4.6"}, {"user": "zertrin", "created_at": "2025-02-21T17:00:52Z", "body": "Confirmed using v0.4.7, thanks! 💟 "}], "satisfaction_conditions": ["Support for non-ASCII characters in author names", "Proper encoding handling when creating index files", "Ability to process French language content without errors"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:49"}, "dockerfile": null, "language": "python"}
{"number": 67, "title": "Is it possible to use a LangGraph Agent w/ tools as a MCP server tool itself for a ReAct Agent?", "created_at": "2025-04-03T20:08:40Z", "closed_at": "2025-04-03T22:15:17Z", "commit_id": "2178e7e50f33b3e2dc5d96a1a9646bbaa1857745", "labels": ["question"], "url": "https://github.com/langchain-ai/langchain-mcp-adapters/issues/67", "body": "I wanted to see if it was possible to have some sort of MCP server-client architecture for supervisor agents and their atomic agents. I ended up figuring out a way to make a MCP tool a chained llm that acts as a singular \"agent\" which a LangGraph ReAct agent can then delegate to in the client file. \n\nHowever, when trying to make a new MCP tool that itself had a tool and was a LangGraph agent I came across an error. I then attempted to keep the tool in its chained llm format and add the tool as a chain and that constituted a response, but it seems like the llm chain doesn't detect, or use, the tool. \n\nI was wondering if there was any workaround for this or a way to input a ReAct agent as a tool into another ReAct agent?\n\nUnrelated, I believe that this would work with the create_supervisor() class, but for some reason whenever I upgrade LangGraph from version 0.2.76 to version 0.3.24 to use the supervisor class I can't seem to use langgraph.prebuilt for the create_react_agents. \n\nEx of code: \n\nin idlookup_server.py:\n\n`...\n@mcp.tool()\nasync def create_idlookup_agent(query:str):\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            ('system', \"You are a helpful assistant that can look up IDs in a database.\"),\n            ('human', \"Search for the member and get their credentials using the idlookup tool based on this query: {query}\"),\n        ]\n    )\n    #idk if bind_tools() is deprecated or not but regardless it doesn't work + i've tried with just adding the tool to the chain and that doesn't work either\n    llm = model.bind_tools(tools) \n    chain = prompt | llm | StrOutputParser()\n    result = await chain.ainvoke({\"query\": query})\n    return result\n\nif __name__ == \"__main__\":\n    mcp.run(transport='stdio')`\n\nin client.py:\n\n`...\n# this is a multi server mcp client \nas client:\nagent = create_react_agnet(model, client.get_tools())\nmembersearch_response = await agent.ainvoke({'message': 'Find me a member with the name John Doe and id 284372.'})\nprint(membersearch_response.content)`\n\nLet me know if this issue makes sense.", "comments_url": "https://api.github.com/repos/langchain-ai/langchain-mcp-adapters/issues/67/comments", "author": "tmehrish", "comments": [{"user": "vbarda", "created_at": "2025-04-03T20:18:06Z", "body": "What is the error? It's not clear from the code you provided\n\nIf you want to put another agent inside the tool, it should definitely be possible, it would simply be\n\n```python\nmy_agent = create_react_agent(...)\n\n@mcp.tool()\ndef my_agent_tool(query: str):\n    response = my_agent.invoke({\"messages\": query})\n    return response[\"messages\"][-1].content\n```\n\n> #idk if bind_tools() is deprecated or not but regardless it doesn't work + i've tried with just adding the tool to the chain and that doesn't work either\n\nyou might be using some outdated version of your libraries, `bind_tools` is supported and should work for all major model providers\n\n\n> Unrelated, I believe that this would work with the create_supervisor() class, but for some reason whenever I upgrade LangGraph from version 0.2.76 to version 0.3.24 to use the supervisor class I can't seem to use langgraph.prebuilt for the create_react_agents.\n\njust create a new virtualenv and reinstall langgraph"}, {"user": "tmehrish", "created_at": "2025-04-03T20:40:56Z", "body": "My error was that I was getting a ToolMessage with a ToolException saying that my 'Input should be a valid dictionary or instance of my function...'\n\nI tried implementing the agent similar to how you did and then got another ToolException saying 'Error executing tool: object is not subscriptable'\n\nFor both these errors my messages input to the supervisor agent was the same as shown above"}, {"user": "vbarda", "created_at": "2025-04-03T22:15:17Z", "body": "not sure about the first error, the second one seems to be just incorrectly unpacking the results from the agent?\n\neither way, this is not an issue with this library: i would recommend testing without MCP first -- creating a simple tool function that wraps the agent (how i suggested above), and then test both the tool and react agent with that tool to make sure it's working as expected\n\n```\nmy_agent = create_react_agent(...)\n\ndef my_agent_tool(query: str):\n    response = my_agent.invoke({\"messages\": query})\n    return response[\"messages\"][-1].content\n\nnew_agent = create_react_agent(model, [my_agent_tool])\n```"}, {"user": "tmehrish", "created_at": "2025-04-04T13:55:32Z", "body": "Yeah you're right, this works. I think my problem stemmed from the use of ChatPromptTemplate because the variable i specified in the prompt would have to be the same key when invoking the agent which would lead to an error. So to fix it I had to get rid of the prompt and change the key to \"messages\" like how you have it. "}], "satisfaction_conditions": ["A working approach to use a LangGraph agent as a tool within another agent", "Proper input/output handling between nested agents", "A solution that works without requiring MCP (Multi-Call Protocol) architecture", "Guidance on correct parameter naming and prompt structure"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:08:45"}, "dockerfile": null, "language": "python"}
{"number": 35, "title": "Error when only predicting ligand", "created_at": "2024-12-08T15:34:57Z", "closed_at": "2024-12-17T12:52:40Z", "commit_id": "a7803a8f137d256285b5b83f3338a0ee17f2e91d", "labels": [], "url": "https://github.com/bytedance/Protenix/issues/35", "body": "Hi! I wanna only predict the ligand structure, but proteinix raised errors:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/workspace/runner/inference.py\", line 213, in main\r\n    runner.dumper.dump(\r\n  File \"/workspace/runner/dumper.py\", line 74, in dump\r\n    self.dump_predictions(\r\n  File \"/workspace/runner/dumper.py\", line 107, in dump_predictions\r\n    self._save_structure(\r\n  File \"/workspace/runner/dumper.py\", line 135, in _save_structure\r\n    save_structure_cif(\r\n  File \"/workspace/protenix/data/utils.py\", line 181, in save_structure_cif\r\n    save_atoms_to_cif(\r\n  File \"/workspace/protenix/data/utils.py\", line 154, in save_atoms_to_cif\r\n    cifwriter.save_to_cif(\r\n  File \"/workspace/protenix/data/utils.py\", line 288, in save_to_cif\r\n    block_dict.update(self._get_entity_poly_and_entity_poly_seq_block())\r\n  File \"/workspace/protenix/data/utils.py\", line 260, in _get_entity_poly_and_entity_poly_seq_block\r\n    \"entity_poly\": pdbx.CIFCategory(entity_poly),\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 327, in __init__\r\n    columns = {\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 328, in <dictcomp>\r\n    key: CIFColumn(col) if not isinstance(col, CIFColumn) else col\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 138, in __init__\r\n    data = CIFData(data, str)\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 66, in __init__\r\n    self._array = _arrayfy(array)\r\n  File \"/opt/conda/lib/python3.10/site-packages/biotite/structure/io/pdbx/cif.py\", line 1061, in _arrayfy\r\n    raise ValueError(\"Array must contain at least one element\")\r\nValueError: Array must contain at least one element\r\n```\r\n\r\nThe json file is:\r\n```\r\n[\r\n    {\r\n        \"sequences\": [\r\n            {\r\n                \"ligand\": {\r\n                    \"ligand\": \"COc1cc(OC)ccc1/C=C/N(C(=O)C)C\",\r\n                    \"count\": 1\r\n                }\r\n            }\r\n        ],\r\n        \"modelSeeds\": [],\r\n        \"assembly_id\": \"1\",\r\n        \"name\": \"LIG_1\"\r\n    }\r\n]\r\n\r\n```\r\n", "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/35/comments", "author": "v-shaoningli", "comments": [{"user": "cloverzizi", "created_at": "2024-12-12T03:05:45Z", "body": "Hi Shaoning, \r\n\r\nThis issue has been resolved in the recent code update. The task results without polymer can now be saved normally. \r\nThank you for the feedback :D\r\n"}, {"user": "v-shaoningli", "created_at": "2024-12-12T08:12:44Z", "body": "Thanks for the update!"}], "satisfaction_conditions": ["A solution that allows predicting only ligand structures without errors", "Proper handling of cases where only ligand data is provided in the input JSON", "Successful saving/output of prediction results for ligand-only tasks"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:14:41"}, "dockerfile": "FROM ai4s-cn-beijing.cr.volces.com/pytorch-mirror/pytorch:2.3.1-cuda12.1-cudnn8-devel\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=Asia/Shanghai\n\n# Install system dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n    wget \\\n    g++ \\\n    gcc \\\n    libc6-dev \\\n    make zlib1g zlib1g-dev \\\n    git git-lfs expect zsh vim wget curl unzip zip cmake cmake-curses-gui libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev \\\n    libxrender1 libxext6 iproute2 \\\n    postgresql \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install HHsuite\nRUN DEBIAN_FRONTEND=noninteractive apt-get update && \\\n    apt-get install --no-install-recommends -y hmmer cmake cmake-curses-gui && \\\n    git clone --branch v3.3.0 https://github.com/soedinglab/hh-suite.git /tmp/hh-suite && \\\n    mkdir /tmp/hh-suite/build && \\\n    cd /tmp/hh-suite/build && \\\n    cmake -DCMAKE_INSTALL_PREFIX=/opt/hhsuite .. && \\\n    make -j 32 && make install && \\\n    ln -s /opt/hhsuite/bin/* /usr/bin && \\\n    cd - && \\\n    rm -rf /tmp/hh-suite && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nRUN pip3 --no-cache-dir install \\\n    scipy \\\n    ml_collections \\\n    tqdm \\\n    pandas \\\n    dm-tree==0.1.6 \\\n    rdkit==\"2023.03.01\" \\\n    biopython==1.83 \\\n    modelcif==0.7 \\\n    biotite==1.0.1 \\\n    gemmi==0.6.5 \\\n    pdbeccdutils==0.8.5 \\\n    scikit-learn==1.2.2 \\\n    scikit-learn-extra \\\n    deepspeed==0.14.4 \\\n    protobuf==3.20.2 \\\n    tos icecream ipdb wandb numpy==1.26.3 matplotlib==3.9.2 ipywidgets py3Dmol\n\n# For H20 compatibility\nRUN pip3 install --no-cache-dir nvidia-cublas-cu12==12.4.5.8 --no-deps\n\n# Clone CUTLASS for DeepSpeed DS4Sci_EvoformerAttention kernel\nRUN git clone -b v3.5.1 https://github.com/NVIDIA/cutlass.git /opt/cutlass\nENV CUTLASS_PATH=/opt/cutlass\n\n# Clone the repository and checkout the specific commit\nWORKDIR /workspace\nRUN git clone https://github.com/bytedance/Protenix.git && \\\n    cd Protenix && \\\n    git checkout a7803a8f137d256285b5b83f3338a0ee17f2e91d\n\n# Install the package in development mode\nWORKDIR /workspace/Protenix\nRUN pip install -e .\n\n# Create data directories that might be needed for inference\nRUN mkdir -p /af3-dev/release_data /af3-dev/release_model\n\n# Add information about downloading model and data files\nRUN echo \"To download model files run:\" > /workspace/README_FIRST.txt && \\\n    echo \"wget -P /af3-dev/release_model/ https://af3-dev.tos-cn-beijing.volces.com/release_model/model_v1.pt\" >> /workspace/README_FIRST.txt && \\\n    echo \"\" >> /workspace/README_FIRST.txt && \\\n    echo \"To download minimal data files needed for inference:\" >> /workspace/README_FIRST.txt && \\\n    echo \"wget -P /af3-dev/release_data/ https://af3-dev.tos-cn-beijing.volces.com/release_data/components.v20240608.cif\" >> /workspace/README_FIRST.txt && \\\n    echo \"wget -P /af3-dev/release_data/ https://af3-dev.tos-cn-beijing.volces.com/release_data/components.v20240608.cif.rdkit_mol.pkl\" >> /workspace/README_FIRST.txt\n\n# Set the working directory\nWORKDIR /workspace", "language": "python"}
{"number": 75, "title": "Weights for the constraint model", "created_at": "2025-02-27T02:58:34Z", "closed_at": "2025-03-04T03:13:07Z", "commit_id": "9765426532a467d6fdf57eb1a3eca8db29442b04", "labels": [], "url": "https://github.com/bytedance/Protenix/issues/75", "body": "Hi, Protenix Team\n\nI noticed that w/ and w/o contact constrain are two models. I have a question, is the weight of the two models exactly the same? Or is it just that the weights are different in the ConstraintEmbedder block, and all the other modules have the same weights?\nThen, I would also like to ask, is the w/ constrain model fine-tuned on the basis of the w/o constraint model? Or a brand new one that training from scratch and keeps input with constrain feature?\n\nLooking forward to your reply. Thank you very much.", "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/75/comments", "author": "fuxuliu", "comments": [{"user": "zhangyuxuann", "created_at": "2025-02-27T07:32:01Z", "body": "@fuxuliu  The w/ constrain model is **fine-tuned**  on the basis of the w/o constraint model. The weight of the two models are different. We haven't tried to train from scratch the constraint model yet. Finetuning is a relatively cheap method to adapt to new features like constraint and esm embedding."}, {"user": "fuxuliu", "created_at": "2025-02-27T07:43:39Z", "body": "@zhangyuxuann Hi, thank you very much for your reply.\n\nI have a few more questions, which may be a little more technical, I hope you don't mind.\n\nYou said the w/ constrain model is fine-tuned on the basis of the w/o constraint model, then in the process of fine-tuned, Do you train only one layer of constraint embedder? Or the whole model is unfreezed state?\n\nAnother question, I noticed that the contact-constraint feature is actually the contact max distance threshold of the pair of residue-residue (or residue-ligand atom) to be constrained. So how do you do contact-constraint sampling during training, because a bio complex actually has multiple contact interfaces, At the same time, the max distance assigned during training is the real contact distance obtained from the pdb?"}, {"user": "zhangyuxuann", "created_at": "2025-02-27T07:48:02Z", "body": "@fuxuliu the whole model is unfreezed state, but the added constraint part and the remaining part(with small learning rate) will have different lr schedule.  @Anfankus  can you explain more detail for the another question?"}, {"user": "fuxuliu", "created_at": "2025-02-27T08:06:46Z", "body": "@zhangyuxuann Thanks you reply.\n@Anfankus Could you please explain more detail for the another question? \nThank you."}, {"user": "Anfankus", "created_at": "2025-02-27T08:14:17Z", "body": "Hi @fuxuliu, for your question:\n\n> ...how do you do contact-constraint sampling during training\n\nDuring training, we first sample a `max_distance` and `num_contacts` from an uniform distribution and a geometric distribution respectively. The distributions vary according to the interface type. And then sample `num_contacts` contact pairs within `max_distance` from the ground truth structure. "}, {"user": "fuxuliu", "created_at": "2025-02-27T08:19:00Z", "body": "@Anfankus @zhangyuxuann \nokay.\nI think I understand a lot. Thank you for your answers"}, {"user": "fuxuliu", "created_at": "2025-02-27T09:05:38Z", "body": "@Anfankus Hi, \nI'm sorry to bother again.\n\nAnd it occurred to me that protein-protein interface (residue-residue), protein-ligand interface(protein-ligand atom), when training, it is generally considered that **distance threshold is** less than how much is it considered that he has contact?\nIf it is greater than a **distance threshold**, the pairs will not be sampled?"}, {"user": "Anfankus", "created_at": "2025-02-28T07:17:45Z", "body": "@fuxuliu \nThe distance threshold is less than 30A for protein-protein and is less than 10A for protein-ligand in our default training setting. Token pairs with spacing greater than the threshold will not be sampled.\n"}, {"user": "fuxuliu", "created_at": "2025-03-13T16:54:32Z", "body": "@zhangyuxuann Hi, Sorry to bother you again.\n\n> the whole model is unfreezed state, but the added constraint part and the remaining part(with small learning rate) will have different lr schedule.\n\nI would like to ask what is the learning rate and the number of steps to fine tune the added constraint part?\nIf the learning rate is 0.0018, I feel very big? Did the batchsize change from the pretrained phas?\n"}, {"user": "zhangyuxuann", "created_at": "2025-03-15T00:52:26Z", "body": "@fuxuliu The learning rate can be set as 5e-4, we finetune with batch size 64. The steps is about 15-20K."}], "satisfaction_conditions": ["Clarification about the relationship between the constrained and unconstrained models", "Information about which parts of the model are trained during fine-tuning", "Explanation of the constraint sampling methodology during training", "Details about distance thresholds used for different types of interfaces", "Technical parameters used for fine-tuning the constrained model"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:27"}, "dockerfile": null, "language": "python"}
{"number": 73, "title": "About contact_feature of ContactFeaturizer", "created_at": "2025-02-20T11:44:04Z", "closed_at": "2025-02-24T03:15:12Z", "commit_id": "9d4d8585bef7647fffafdf66a1b3a95f017ed6eb", "labels": [], "url": "https://github.com/bytedance/Protenix/issues/73", "body": "Hi, I have a question?\n\n```\ncontact_feature[token_id_1, token_id_2, 1] = max_distance\ncontact_feature[token_id_2, token_id_1, 1] = max_distance\ncontact_feature[token_id_1, token_id_2, 0] = 0\ncontact_feature[token_id_2, token_id_1, 0] = 0\n```\n\n\n```\ncontact_feature[token_id_1, token_id_2, 0] = 0\ncontact_feature[token_id_2, token_id_1, 0] = 0\n```\n\nMy understanding is that this should be assigned to 1, Because this itself is initialized to 0, and then contact should be 1, right?\n", "comments_url": "https://api.github.com/repos/bytedance/Protenix/issues/73/comments", "author": "fuxuliu", "comments": [{"user": "Anfankus", "created_at": "2025-02-24T03:01:50Z", "body": "Sorry for the confusion. The first dimension of our contact feature was designed for `minimal distance`. However it is not activated currently and just be filled with 0 to act as a placeholder."}, {"user": "fuxuliu", "created_at": "2025-02-24T03:15:12Z", "body": "ok, I see what you mean."}], "satisfaction_conditions": ["Clarification about the purpose of the first dimension (index 0) in the contact_feature array", "Explanation of the design intention behind the contact_feature structure"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:33"}, "dockerfile": null, "language": "python"}
{"number": 43, "title": "Is there a bug in lines 428-431 of sa2va_4b.py where the three datasets of recfcoco are added four times?", "created_at": "2025-02-25T09:24:06Z", "closed_at": "2025-03-13T10:09:06Z", "commit_id": "c490564bde93d44d787a56267c61f103d493f8af", "labels": [], "url": "https://github.com/magic-research/Sa2VA/issues/43", "body": null, "comments_url": "https://api.github.com/repos/magic-research/Sa2VA/issues/43/comments", "author": "bill-hx-liu", "comments": [{"user": "zhang-tao-whu", "created_at": "2025-03-13T10:08:56Z", "body": "Everything is normal. Since each image in the RES dataset will randomly sample several objects to form the conversation, there will be a large number of objects that are not sampled. We mitigate this issue by manually repeating the dataset four times."}, {"user": "bill-hx-liu", "created_at": "2025-03-17T12:21:34Z", "body": "ok，i got it，thank you!"}], "satisfaction_conditions": ["An explanation of the purpose behind repeating the dataset multiple times", "Clarification that the code behavior is intentional, not a bug", "An explanation of the sampling methodology for objects in the dataset"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:00"}, "dockerfile": null, "language": "python"}
{"number": 38, "title": "SAM2 MASK Decoder freeze or not?", "created_at": "2025-02-23T22:09:32Z", "closed_at": "2025-02-25T07:44:01Z", "commit_id": "06c61e0270a9f5a8bd89e6fa3862cfdb1840649f", "labels": [], "url": "https://github.com/magic-research/Sa2VA/issues/38", "body": "Hi Authors,\n\nThanks for your amazing work! I would like to clarify a detail for Sa2VA model training: In you introduction section, there is a statement saying that \"Moreover, we adopt a decoupled design in which SAM-2’s decoder and memory module are frozen, allowing us to retain the perception and tracking capabilities of SAM-2\", but in your architecture figure, there is a \"fire\" icon of SAM2 decoder, which is supposed to indicate that the module is trainable. I want to clarify that did you freeze the SAM2 decoder during training?\n\nThanks,\nRuining", "comments_url": "https://api.github.com/repos/magic-research/Sa2VA/issues/38/comments", "author": "Ruining0916", "comments": [{"user": "HarborYuan", "created_at": "2025-02-25T01:12:01Z", "body": "Hi @Ruining0916 ,\n\nThank you very much for the interest in our work. Here want to I clarify that the SAM-2's decoder is trainable during instruction-tuning as in the figure and the code. \n\nIt is a typo in the manuscript. We wanted to emphasize that Sa2VA does not require memory module training in the paper."}, {"user": "Ruining0916", "created_at": "2025-02-25T02:05:50Z", "body": "Thanks for your clarification! So the only module you used from SAM-2 is mask decoder right?"}, {"user": "HarborYuan", "created_at": "2025-02-25T07:43:58Z", "body": "Exactly. The other parts are fixed."}], "satisfaction_conditions": ["Clear clarification about which SAM-2 components are trainable versus frozen during the Sa2VA model training", "Identification of which specific SAM-2 modules are used in the Sa2VA architecture", "Correction of inconsistencies between the paper text and the architecture figure"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:08"}, "dockerfile": null, "language": "python"}
{"number": 23, "title": "Sa2VA-4b model performance on image segmentation benchmark", "created_at": "2025-02-14T20:35:32Z", "closed_at": "2025-02-25T02:28:02Z", "commit_id": "6f59e55da541fad1ffc370419265eab6f511569f", "labels": [], "url": "https://github.com/magic-research/Sa2VA/issues/23", "body": "Hi authors,\n\nThanks to your excellent work! I have a question regarding your performance report with Sa2VA-4b model on refcoco/+/g datasets due to the inconsistency cIou scores from table 2,4,6 which supposed to use the same Sa2VA-4b model and perform the same image segmentation benchmark. More specially,  Sa2VA-4b on refcoco/+/g datasets are **77.4/69.9/72.3** from table 2, **80.4/74.3/76.7** from table 4; and **80.4/74.3/75.7** from table 6. I wonder are these inconsistent scores come from any of the below reasons:\n\n- Does the Sa2VA-4b in table 2 and table4/6 not the same model (the one from table 2 does not involve co-training and the the one from table 4/6 involves co-training)? but from my understanding, they should both be the finalized model?\n- If they are different Sa2VA-4b models, then does co-training image chat/segmentation even improve image segmentation performance?\n- If they are the same Sa2VA-4b model, do it perform different benchmarks for table2 and table 4/6?\n- Are these numbers are just fluctuations from different runs?\n\nThanks a lot for your help and clarification!\n", "comments_url": "https://api.github.com/repos/magic-research/Sa2VA/issues/23/comments", "author": "Ruining0916", "comments": [{"user": "zhang-tao-whu", "created_at": "2025-02-16T06:42:44Z", "body": "In Table 2, Sa2VA employs InternVL2 as the base MLLM, while in Tables 4 and 6, Sa2VA utilizes InternVL 2.5 as the base MLLM. Table 6 showcases the outcomes of additional fine-tuning on the RES datasets using the co-trained model from Table 4. We did not witness an improvement in QA performance through the incorporation of segmentation data; nonetheless, we detected a modest enhancement in QA performance with the integration of visual prompt understanding data."}, {"user": "Ruining0916", "created_at": "2025-02-17T20:56:01Z", "body": "Thanks for your clarification. Th Sa2VA-4b from huggingface and in the training script utilize InternVL 2.5 right?"}, {"user": "zhang-tao-whu", "created_at": "2025-02-18T11:30:58Z", "body": "Yes."}], "satisfaction_conditions": ["Clarification about which version of the base MLLM (InternVL) is used in different tables of the paper", "Explanation for the performance differences in the reported benchmark scores across tables", "Confirmation about which model version is available in the public repository", "Information about the effects of co-training on model performance"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:14"}, "dockerfile": null, "language": "python"}
{"number": 16, "title": "[SEG] Hidden State usage", "created_at": "2025-02-07T17:01:36Z", "closed_at": "2025-02-10T07:08:58Z", "commit_id": "62279033294f1c5b17be011a2d5aaecf2435761c", "labels": [], "url": "https://github.com/magic-research/Sa2VA/issues/16", "body": "Thank you for the great work. As I read the code, if I understand correctly, it seems that If the model predicts the next token is [SEG], it will take the final layer hidden state and feed through an MLP connector and feed it to SAM2 for decoding.\n\nHowever, if the language model predicts [SEG] as the next token, it means the embedding has the highest (or very high if sampling is used) similarity with [SEG] embedding. Doesn't that mean this embedding fed to SAM2 is always very close to [SEG] no matter what the rest of the sentence/context is? How does the model utilize the context with very similar embeddings for segmenting different objects?\n\nPlease let me know if I misunderstood. Thank you!", "comments_url": "https://api.github.com/repos/magic-research/Sa2VA/issues/16/comments", "author": "seermer", "comments": [{"user": "HarborYuan", "created_at": "2025-02-10T06:31:53Z", "body": "Your understanding is correct. But I would like to point out that [SEG]'s embedding has two different MLP/projection layers, outputing to the classifier or visual prompt respectively. When considering an embedding, for text classifiers, they may be similar, but for prompt encoder, these differences are used to segment different objects."}, {"user": "seermer", "created_at": "2025-02-10T07:08:58Z", "body": "Thank you so much for your help! "}], "satisfaction_conditions": ["Clarification on how the model differentiates between different objects despite similar [SEG] token embeddings", "Confirmation or correction of the user's understanding of the model architecture", "Technical explanation of how context information is preserved in the segmentation process"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:21"}, "dockerfile": null, "language": "python"}
{"number": 13, "title": "SAV dataset inquiry", "created_at": "2025-02-05T23:47:42Z", "closed_at": "2025-02-06T19:31:17Z", "commit_id": "62279033294f1c5b17be011a2d5aaecf2435761c", "labels": [], "url": "https://github.com/magic-research/Sa2VA/issues/13", "body": "Congrats on your excellent work! I would like to clarify the SAV dataset details:\n\nYou mentioned about sam_v_full should be downloaded from meta/SA-V dataset page, which provides the link of downloading sav_000.tar - sav_055.tar; should we unzip them and put these files under sam_v_full directory as sub directories?\n\nThanks for your clarification!", "comments_url": "https://api.github.com/repos/magic-research/Sa2VA/issues/13/comments", "author": "Ruining0916", "comments": [{"user": "HarborYuan", "created_at": "2025-02-05T23:48:58Z", "body": "Hi @Ruining0916 ,\n\nExactly."}, {"user": "Ruining0916", "created_at": "2025-02-05T23:49:44Z", "body": "Thanks for your swift response!\n\n"}], "satisfaction_conditions": ["Confirmation of the correct directory structure for the SAV dataset", "Clear and direct confirmation of their understanding"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:28"}, "dockerfile": null, "language": "python"}
{"number": 522, "title": "Set pdf pages range to ask", "created_at": "2025-03-17T23:08:16Z", "closed_at": "2025-03-24T08:04:37Z", "commit_id": "f335107152512832580d718ae290c884db7166f6", "labels": ["type: feature request", "type: question", "priority: p3"], "url": "https://github.com/googleapis/python-genai/issues/522", "body": "Is there a way to ask a pdf only for 1 page o range of pages ? \n\nRight now:\n\n```python\nfrom pathlib import Path\n\nfrom google import genai\nimport os\n\n\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\nclient = genai.Client(api_key=GEMINI_API_KEY)\n\npdf_file = Path(\"my-local-file.pdf\")\nsample_file = client.files.upload(file=pdf_file)\nprompt = f\"Ask this question only foy 1 page: YOUR QUESTION\"\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[sample_file, prompt])\nprint(response.text)\n```\n\nbut If my pdf has 2000 pages, I spend too much tokens. Exist a way to set the page or range of pages to ask ? \n\n\nsomething like: \n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[sample_file, prompt]\n    pdf_pages = (1,1)\n\n)\n\n\n```\n", "comments_url": "https://api.github.com/repos/googleapis/python-genai/issues/522/comments", "author": "lexiconlp", "comments": [{"user": "wanlin31", "created_at": "2025-03-20T18:32:25Z", "body": "Unfortunately, only look at a particular page or page range is not supported. Can you try to create a new file that only contain the desirable portion of the original file? "}, {"user": "MarkDaoust", "created_at": "2025-03-21T01:02:47Z", "body": "> Can you try to create a new file that only contain the desirable portion of the original file?\n\nSeems like something that could be pretty easy using pypdf. Something like this seems to work:\n\n```\nfrom pypdf import PdfReader, PdfWriter\n\ndef create_new_pdf_from_range(input_pdf_path, output_pdf_path, start_page, end_page):\n    \"\"\"\n    Creates a new PDF from a range of pages in an existing PDF.\n\n    Args:\n        input_pdf_path: Path to the input PDF file.\n        output_pdf_path: Path to the output PDF file.\n        start_page: The starting page number (inclusive, 0-indexed).\n        end_page: The ending page number (exclusive, 0-indexed).\n    \"\"\"\n    reader = PdfReader(input_pdf_path)\n    writer = PdfWriter()\n\n    for page_num in range(start_page, end_page):\n        page = reader.pages[page_num]\n        writer.add_page(page)\n\n    with open(output_pdf_path, \"wb\") as output_pdf:\n        writer.write(output_pdf)\n\n# Example usage:\ninput_pdf = \"test.pdf\"  # Replace with your input PDF file path\noutput_pdf = \"output.pdf\" # Replace with your desired output PDF file path\nstart_page_index = 2     # Start page index (0 for the first page)\nend_page_index = 3       # End page index (exclusive, so this will be up to page 2)\n\ncreate_new_pdf_from_range(input_pdf, output_pdf, start_page_index, end_page_index)\n```\n\nNote that `client.files.upload` can take an open file object as input, so you can skip even writing it to disk\n\n@lexiconlp Does that solve your problem?"}, {"user": "lexiconlp", "created_at": "2025-03-24T08:04:37Z", "body": "I'd thought about doing that too @MarkDaoust , but I thought there was an option.\n\nThanks anyway @MarkDaoust : ) . I'm closing this issue.\n\n"}], "satisfaction_conditions": ["A method to process only specific pages from a PDF when using the Gemini API", "A solution that reduces token consumption when working with large PDFs", "A programmatic approach that integrates with their existing Python workflow"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:35"}, "dockerfile": null, "language": "python"}
{"number": 280, "title": "Models are not able to reference file names when producing their outputs", "created_at": "2025-02-07T17:13:34Z", "closed_at": "2025-02-20T20:31:21Z", "commit_id": "fcf88881698eabfa7d808df0f1353aa6bcc54cb8", "labels": ["type: feature request", "type: question", "priority: p3"], "url": "https://github.com/googleapis/python-genai/issues/280", "body": "I'd like the model to be able to reference the file source when answering questions that were preceded with file inputs.\n\nConsider this example:\n\n```python\nimport io, google.genai\n\nGOOGLE_API_KEY = \"--API_KEY--\"\nclient = google.genai.Client(api_key=GOOGLE_API_KEY)\n\ndef upload_file(file_contents, display_name, mime_type=\"text/plain\"):\n    file_contents = io.BytesIO(file_contents.encode(\"utf-8\"))\n    return client.files.upload(path=file_contents, config={\"mime_type\": mime_type, \"display_name\": display_name})\n\nfc_1 = \"\"\"Simplicity is the ultimate sophistication.\n— Leonardo da Vinci\n\"\"\"\nfc_2 = \"\"\"It always seems impossible until it’s done.\n- Nelson Mandela\n\"\"\"\n\nfiles = [upload_file(fc_1, \"file1.md\"), upload_file(fc_2, \"file2.md\")]\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=[files[0], files[1],\n        \"For every file, output the name of the file and the quote inside.\",\n    ])\nprint(response.candidates[0].content.parts[0].text)\n```\n\nHere is one of the responses I got:\n\n```\nOkay, I understand. Here's how I will respond, given a file name:\n\n**Input:**\n\n*   `[Filename]`\n\n**Output:**\n\n*   `[Filename]: [Author]`\n\n**Examples:**\n\n*   `quote1.txt`\n*   `quote1.txt: Leonardo da Vinci`\n\n*   `quote2.txt`\n*   `quote2.txt: Nelson Mandela`\n```\n\nNotice that the model is not aware of the file names and can't reference them in its answer.\n\nIf I invoke the model from the Google AI studio, I get the result I'd like:\n```\nfile1.md: Simplicity is the ultimate sophistication.\nfile2.md: It always seems impossible until it’s done.\n```\n\nIs this something we can expect to be ironed out in this library, or should I consider switching to google-generativeai lib?\n\nThe ability to reference files is absolutely crucial for our use case.", "comments_url": "https://api.github.com/repos/googleapis/python-genai/issues/280/comments", "author": "gapeslape", "comments": [{"user": "nurgel", "created_at": "2025-02-08T03:42:33Z", "body": "you could prepend all the metadata you like in text before the file, that is what they probably do on AI Studio."}, {"user": "gapeslape", "created_at": "2025-02-08T17:30:18Z", "body": "@nurgel that works great. Thanks!"}, {"user": "pamorgan", "created_at": "2025-02-20T20:31:21Z", "body": "Thank you - Please let us know if there are more follow ups needed."}], "satisfaction_conditions": ["A method to make file names accessible to the model when processing file content", "A solution that works with their existing code structure using the google.genai library", "A straightforward implementation that doesn't require complex changes"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:42"}, "dockerfile": null, "language": "python"}
{"number": 74, "title": "fix: FunctionCallCancellation ids type.", "created_at": "2025-01-03T17:44:30Z", "closed_at": "2025-01-03T18:13:13Z", "commit_id": "f494432900d60e64cbef69424918904ddbe255b1", "labels": ["google-contributor"], "url": "https://github.com/googleapis/python-genai/pull/74", "body": "fix: FunctionCallCancellation ids type.\n\nCo-authored-by: jayesh <jayeshparmar9829@gmail.com>\n", "comments_url": "https://api.github.com/repos/googleapis/python-genai/issues/74/comments", "author": "copybara-service[bot]", "comments": [{"user": "happy-qiao", "created_at": "2025-01-03T18:00:44Z", "body": "Fixes #50 \r\n"}, {"user": "happy-qiao", "created_at": "2025-01-03T18:08:38Z", "body": "@jayeshp19 Thanks for reporting and fixing the issue #50 . We haven't supported external PR yet. I've applied your changes in a new pull request and will merge it shortly."}, {"user": "jayeshp19", "created_at": "2025-01-03T18:17:35Z", "body": "Great!! Thanks :) Are you releasing new version any soon? "}, {"user": "happy-qiao", "created_at": "2025-01-03T18:23:44Z", "body": "We will release a new version early next week."}], "satisfaction_conditions": ["Confirmation that the type issue in FunctionCallCancellation ids will be fixed", "Information about when the fix will be available in a released version"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:48"}, "dockerfile": null, "language": "python"}
{"number": 129, "title": "910b4", "created_at": "2025-02-21T02:22:02Z", "closed_at": "2025-02-22T07:24:24Z", "commit_id": "3a4ce2aa15a9c7d442b49ba87c14591b41be3aae", "labels": ["question"], "url": "https://github.com/vllm-project/vllm-ascend/issues/129", "body": "请问支不支持910b4", "comments_url": "https://api.github.com/repos/vllm-project/vllm-ascend/issues/129/comments", "author": "w1051868626", "comments": [{"user": "wangxiyuan", "created_at": "2025-02-21T03:03:58Z", "body": "910b4 is supported. The only difference is that HBM of 910b4 is 32G."}, {"user": "w1051868626", "created_at": "2025-02-22T07:17:53Z", "body": "OK, I succeed"}, {"user": "wangxiyuan", "created_at": "2025-02-22T07:24:24Z", "body": "OK, I'll close this issue. Feel free to create new issue if you hit any problem. Thanks."}], "satisfaction_conditions": ["Confirmation of 910b4 hardware compatibility with the system", "Information about any hardware-specific limitations or differences"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:11:53"}, "dockerfile": null, "language": "python"}
{"number": 17, "title": "[Platform] add dispatch key", "created_at": "2025-02-07T02:51:08Z", "closed_at": "2025-02-21T09:10:31Z", "commit_id": "7d9ae22ecb6dc3ea4e720e5109cf46e1ae7da730", "labels": [], "url": "https://github.com/vllm-project/vllm-ascend/pull/17", "body": "### What this PR does / why we need it?\r\nAdd dispatch key for NPU, so that the log could be print correctly.\r\n\r\nNow\r\n```\r\nexecutor_base.py:110] # CPU blocks: 220478, # CPU blocks: 21845\r\n```\r\n\r\nAfter this pr\r\n```\r\nexecutor_base.py:110] # NPU blocks: 220478, # CPU blocks: 21845\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nN/A\r\n\r\n### How was this patch tested?\r\nCI passed and log printed as above\r\n\r\n", "comments_url": "https://api.github.com/repos/vllm-project/vllm-ascend/issues/17/comments", "author": "MengqingCao", "comments": [{"user": "wangxiyuan", "created_at": "2025-02-17T01:43:00Z", "body": "from torch expert suggestion: change the value to `PrivateUse1`. "}, {"user": "MengqingCao", "created_at": "2025-02-17T02:53:10Z", "body": "> from torch expert suggestion: change the value to `PrivateUse1`.\r\n\r\nThanks a lot! This could fix the issue that torch does not recognize key `npu`.\r\nBut the log printed will become `executor_base.py:110] # PrivateUse1 blocks: 220478, # CPU blocks: 21845`, I'll fix this in vLLM."}, {"user": "wangxiyuan", "created_at": "2025-02-17T02:55:08Z", "body": "> > from torch expert suggestion: change the value to `PrivateUse1`.\r\n> \r\n> Thanks a lot! This could fix the issue that torch does not recognize key `npu`. But the log printed will become `executor_base.py:110] # PrivateUse1 blocks: 220478, # CPU blocks: 21845`, I'll fix this in vLLM.\r\n\r\nYes, the log in vllm should use device_name instead."}, {"user": "MengqingCao", "created_at": "2025-02-17T02:57:46Z", "body": "> Yes, the log in vllm should use device_name instead.\r\n\r\nAgree"}], "satisfaction_conditions": ["A solution that allows torch to recognize the NPU device type", "A way to display the correct device name in logs", "Guidance from torch experts on the proper approach for adding custom device types"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:12:00"}, "dockerfile": null, "language": "python"}
{"number": 133, "title": "step与使用的数据量不符", "created_at": "2025-03-07T12:41:55Z", "closed_at": "2025-03-12T08:13:32Z", "commit_id": "451b8cd64c465dd2f3412f17bf1d2a0ce47b8329", "labels": [], "url": "https://github.com/om-ai-lab/VLM-R1/issues/133", "body": "我使用7k数据并调用如下脚本进行训练，step数却有3500，请问要怎么解决\n\n#!/bin/bash\n\n# 设置环境变量（如果需要）\nexport MASTER_ADDR=\"127.0.0.1\"\nexport MASTER_PORT=\"12346\"\nexport RUN_NAME=\"original_reward_4k_4k\"  # 你可以替换这个为实际的RUN_NAME\n\n# 运行命令\ntorchrun --nproc_per_node=\"8\" \\\n    --nnodes=\"1\" \\\n    --node_rank=\"0\" \\\n    --master_addr=\"$MASTER_ADDR\" \\\n    --master_port=\"$MASTER_PORT\" \\\n    src/open_r1/grpo_text.py \\\n    --deepspeed local_scripts/zero3.json \\\n    --output_dir output/$RUN_NAME \\\n    --model_name_or_path /workspace/denglinger/Qwen2.5-VL-3B-Instruct \\\n    --dataset_name /workspace/denglinger/VLM-R1-main/src/open-r1-multimodal/data_config/chartqa.yaml \\\n    --image_root /workspace/denglinger/Dataset/ChartQA_Dataset \\\n    --max_prompt_length 1024 \\\n    --num_generations 8 \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 2 \\\n    --logging_steps 1 \\\n    --bf16 \\\n    --torch_dtype bfloat16 \\\n    --data_seed 42 \\\n    --report_to wandb \\\n    --gradient_checkpointing false \\\n    --attn_implementation flash_attention_2 \\\n    --num_train_epochs 1 \\\n    --run_name $RUN_NAME \\\n    --save_steps 100 \\\n    --save_only_model true\n", "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/133/comments", "author": "dle666", "comments": [{"user": "SZhanZ", "created_at": "2025-03-07T13:02:42Z", "body": "你好，我们的train sampler更新过了，现在要把`--per_device_train_batch_size`设置为`8`，应该就正常了。"}, {"user": "dle666", "created_at": "2025-03-07T13:07:42Z", "body": "已解决，非常感谢\n"}], "satisfaction_conditions": ["An explanation of why the number of training steps doesn't match the expected amount based on the dataset size", "A configuration adjustment that aligns the training steps with the actual dataset size", "Clear instructions on which parameter needs to be modified to fix the step count issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:06:23"}, "dockerfile": null, "language": "python"}
{"number": 83, "title": "有关图像分辨率方面的疑问", "created_at": "2025-02-27T09:38:53Z", "closed_at": "2025-02-28T01:15:34Z", "commit_id": "7cd17f489e15b4d42131b1e1a2135172f72be410", "labels": [], "url": "https://github.com/om-ai-lab/VLM-R1/issues/83", "body": "非常漂亮的工作！有个疑问，关于准备自定义数据集的。请问R1训练过程是动态分辨率还是固定分辨率？LLaMA-Factory的SFT训练呢(假设用的是QWen2.5-VL-3B)？", "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/83/comments", "author": "CaptainEven", "comments": [{"user": "SZhanZ", "created_at": "2025-02-27T10:32:29Z", "body": "Hello，你好\nR1和SFT都是动态分辨率"}, {"user": "CaptainEven", "created_at": "2025-02-28T01:15:34Z", "body": "好的，感谢！"}], "satisfaction_conditions": ["Clear information about the resolution handling approach in both R1 training and LLaMA-Factory SFT training", "Concise and direct answer to technical questions about image resolution in model training"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:06:29"}, "dockerfile": null, "language": "python"}
{"number": 81, "title": "About OOM", "created_at": "2025-02-27T05:33:56Z", "closed_at": "2025-03-31T06:47:14Z", "commit_id": "7cd17f489e15b4d42131b1e1a2135172f72be410", "labels": [], "url": "https://github.com/om-ai-lab/VLM-R1/issues/81", "body": "My parameters are as follows: \n    --nproc_per_node=\"6\"\n    --num_generations 4 \n    --per_device_train_batch_size 4 \n    --gradient_accumulation_steps 1 \n\nI used 6 x H20（6 x 96G）, Out of memory error after 4 steps of training.\nThat doesn't seem to fit with the minimum configuration, and what should I do to fix it?", "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/81/comments", "author": "Lane315", "comments": [{"user": "qizheng-1-1z", "created_at": "2025-02-27T05:57:42Z", "body": "i got the same problem,sad.."}, {"user": "LaFeuilleMorte", "created_at": "2025-02-27T06:22:04Z", "body": "> My parameters are as follows: --nproc_per_node=\"6\" --num_generations 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 1\n> \n> I used 6 x H20（6 x 96G）, Out of memory error after 4 steps of training. That doesn't seem to fit with the minimum configuration, and what should I do to fix it?\n\nMaybe you can reduce per_device_train_batch_size to 1?"}, {"user": "Lane315", "created_at": "2025-02-27T06:48:33Z", "body": "> > My parameters are as follows: --nproc_per_node=\"6\" --num_generations 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 1\n> > I used 6 x H20（6 x 96G）, Out of memory error after 4 steps of training. That doesn't seem to fit with the minimum configuration, and what should I do to fix it?\n> \n> Maybe you can reduce per_device_train_batch_size to 1?\n\nYou are right. I know it's possible to reduce the batchsize, but this increases the training time. sad.."}], "satisfaction_conditions": ["A solution that reduces memory usage while maintaining reasonable training time", "An explanation of why their current configuration is causing OOM errors despite seemingly meeting minimum requirements", "Alternative approaches to resolve OOM errors beyond just reducing batch size"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:06:37"}, "dockerfile": null, "language": "python"}
{"number": 5, "title": "Still missing gta_subsample.json", "created_at": "2025-02-18T13:56:51Z", "closed_at": "2025-02-18T17:21:27Z", "commit_id": "d12688cb8add22a435ff5f5d317bd24358486b69", "labels": [], "url": "https://github.com/om-ai-lab/VLM-R1/issues/5", "body": "Hi, authors. Thanks for your great work. As the title said, the gta_subsamples.json is still missing in you r provided link in antohr issue.\n\n#2 ", "comments_url": "https://api.github.com/repos/om-ai-lab/VLM-R1/issues/5/comments", "author": "yueyang130", "comments": [{"user": "SZhanZ", "created_at": "2025-02-18T15:14:59Z", "body": "Sorry, I uploaded an incorrect zip file. Let me upload it again."}, {"user": "SZhanZ", "created_at": "2025-02-18T16:28:53Z", "body": "The current one should be ok.\n"}, {"user": "yueyang130", "created_at": "2025-02-18T17:21:27Z", "body": "thanks！"}], "satisfaction_conditions": ["Access to the missing gta_subsample.json file", "A working download link for the required file", "Timely response to the file access request"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:06:51"}, "dockerfile": null, "language": "python"}
{"number": 18, "title": "A few questions about stage2 2B model training", "created_at": "2025-03-04T03:51:10Z", "closed_at": "2025-03-18T10:56:12Z", "commit_id": "a2e8a67da8ccc978ddbdbc10cff3e52c3fc5b083", "labels": [], "url": "https://github.com/FoundationVision/FlashVideo/issues/18", "body": "Hello, I would like to ask a few questions about stage2 2B model training:\n\n1. What is the sampling interval during training? That is, what the `num_noise_interval` parameter setting in the code?\n2. How to set some parameters during training, such as the `learning rate scheduler` and `cfg scale`?\n3. Are the sampler and denoiser in the code useful? As far as I understand, if flow matching is used for training, these two modules should not be used?\n4. In the code, when solving the integral, inference uses `rk4` instead of `euler`. Do these two have a big impact on the result video?\n5. In the pre-training 1 and 2 stages, is the `add noise range` for training images and training videos 600-900? Because I saw that the noise range for images and videos in the code uses two different parameters `img_ref_noise_step_range` and `ref_noise_step_range`, so I want to confirm.", "comments_url": "https://api.github.com/repos/FoundationVision/FlashVideo/issues/18/comments", "author": "frozoul", "comments": [{"user": "jshilong", "created_at": "2025-03-04T10:57:37Z", "body": "\n\nWe appreciate your interest in our work.\n\n1. The parameter `num_noise_interval` was ultimately not used. It was originally intended to encode a latent input once and sample multiple noise timesteps $t$ to accelerate training. Because the encoding process proved to be  slow in practice.  \n\n2. Model training does not use the classifier-free guidance (CFG) scale. The learning rate scheduler is kept constant throughout training.\n\n3. Both the sampler and denoiser components are not used in the implementation.  \n\n4. There was a misunderstanding regarding the numerical method employed. The actual method passed and used is Euler, not default `rk4`, you can check this in the inference code.\n\n5. The range of `img_ref_noise_step_range` is set to \\[100, 300\\] in the implementation, based on empirical observations. However, we are not certain if this range is optimal, as ablation studies could not be conducted due to computational limitations and time constraints.  \n\nIf you have any questions or require adaptations of our algorithm to suit your specific problem, I am more than happy to share insights and experiences from this project with you :)"}, {"user": "frozoul", "created_at": "2025-03-04T12:30:18Z", "body": "Thank you for your reply. I am trying to train from scratch the second stage model in your paper, and your answer is very helpful.\nSo in the second stage of pre-training, when images and videos are mixed at a 1:2 ratio, the image noise range is 100-300, and the video noise range is 600-900?\nIn addition, the paper mentioned adjusting the latent degradation strength based on the Signal-to-Noise Ratio (SNR). How does this part work specifically?"}, {"user": "jshilong", "created_at": "2025-03-04T14:49:11Z", "body": "\n1. So in the second stage of pre-training, when images and videos are mixed at a 1:2 ratio, the image noise range is 100-300, and the video noise range is 600-900? \n- yes\n\n2. the paper mentioned adjusting the latent degradation strength based on the Signal-to-Noise Ratio (SNR)\n- This is the key insight we aim to share with other researchers: For larger resolutions and a higher number of frames, the degradation strength needs to be increased."}, {"user": "frozoul", "created_at": "2025-03-05T06:47:35Z", "body": "I understand, but what is the specific indicator used to calculate this SNR? Is there a quantitative relationship between the SNR indicator and the noise range?\n\nIn addition, is SD3's `t_transform` used during training (if so, what is the corresponding `shift_t` parameter)? If not, what kind of `t_transform` is used?"}, {"user": "jshilong", "created_at": "2025-03-05T12:10:25Z", "body": "1. As discussed in the paper, higher frame counts and larger resolutions require greater noise strength. However, directly calculating the optimal value is challenging. Therefore, we use a wide range of noise strengths during the initial training phase to search for the optimal setting.\n\n2. We do not utilize `logit_norm` in SD3 because, in our setting—where the starting point is a low-resolution video—the most challenging $t$ interval may differ from starting with pure noise. When starting from pure noise(SD3), the most challenging part is typically in the middle of the $t$ interval. However, in our setting, where we start with a low-resolution video, I believe the most challenging part should be near $t = 0$. While I have not conducted specific ablation studies to confirm this, I consider this assumption to be reasonable. So I only apply a $t$ shift, setting it to 3 during training."}, {"user": "frozoul", "created_at": "2025-03-05T13:26:00Z", "body": "Thanks very much for your reply！"}], "satisfaction_conditions": ["Clear explanation of training parameters and their values", "Clarification on which components of the architecture are actually used in implementation", "Information about noise ranges for different data types during training", "Explanation of the numerical methods used during inference", "Insights into the relationship between SNR and degradation strength", "Details about time step transformation techniques"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:18:13"}, "dockerfile": null, "language": "python"}
{"number": 6, "title": "missing sat.model", "created_at": "2025-02-11T05:19:28Z", "closed_at": "2025-02-11T07:47:57Z", "commit_id": "bb93f9d7b537482c4da91c2534ba6dd3fb44ae87", "labels": ["help wanted"], "url": "https://github.com/FoundationVision/FlashVideo/issues/6", "body": "Hello, thank you for open-sourcing such an excellent work !\n\nBut when I run `sat/demo.ipynb`, an error will be reported at the line from `sat.model.base_model import get_model`, prompting that `sat.model cannot be found`. \nI see that there is indeed no model folder in the sat, and there is no `get_model` function in the `base_model` file. Is it because a file was uploaded missing?", "comments_url": "https://api.github.com/repos/FoundationVision/FlashVideo/issues/6/comments", "author": "frozoul", "comments": [{"user": "jshilong", "created_at": "2025-02-11T06:17:50Z", "body": "Indeed, there seems to be a confusion over the `sat` reference. Here, `sat` points to the module in the environment, corresponding to `SwissArmyTransformer>=0.4.12` in the requirements. Have you executed `pip install -r requirements`? Feel free to comment here if you continue to face issues.\n\nTo avoid this confusion for more people, I may consider renaming the folder to `flashvideo`. Thanks for your feedback"}, {"user": "frozoul", "created_at": "2025-02-11T06:34:54Z", "body": "Thank you for your reply. I installed the dependencies according to the requirements, but I moved the `demo` file out of the `sat` folder and executed it, which caused the problem. If it is in the sat folder, it will be normal."}, {"user": "jshilong", "created_at": "2025-02-11T06:37:31Z", "body": "Indeed, the naming of the 'sat' folder in CogVideoX can be confusing. It is something I should address to improve clarity.\n"}], "satisfaction_conditions": ["Clarification of why the import error occurs", "Explanation of the correct file structure or execution context", "Acknowledgment of potential naming confusion in the project"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:18:18"}, "dockerfile": null, "language": "python"}
{"number": 111, "title": "No Wandb logging", "created_at": "2025-04-03T16:25:54Z", "closed_at": "2025-04-03T17:26:36Z", "commit_id": "5e2979d55e2aab0da80bd8a8ca8cf09c6f4ab909", "labels": ["bug"], "url": "https://github.com/roboflow/rf-detr/issues/111", "body": "### Search before asking\n\n- [x] I have searched the RF-DETR issues and found no similar bug report.\n\n\n### Bug\n\n- pip install \"rfdetr[metrics]\"\n- wandb login -> success\n- Ensure the PROJECT is created on wandb\n\n```python\nPROJECT = \"my_project\"\n\nmodel.train(\n    dataset_dir=dataset.location, \n    epochs=15, \n    batch_size=4, \n    grad_accum_steps=4, \n    lr=1e-4,\n    wandb=True,\n    output_dir=output_dir,\n    project=PROJECT,\n    run=\"some_name\",\n)\n```\nNo errors, but no logs on wandb\n\n### Environment\n\n- rfdetr==1.0.8\n- wandb==0.19.9\n\n### Minimal Reproducible Example\n\nsee above\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes, I'd like to help by submitting a PR!", "comments_url": "https://api.github.com/repos/roboflow/rf-detr/issues/111/comments", "author": "robmarkcole", "comments": [{"user": "SkalskiP", "created_at": "2025-04-03T16:45:22Z", "body": "Your RF-DETR package is outdated. Please use 1.1.0. "}, {"user": "robmarkcole", "created_at": "2025-04-03T17:26:36Z", "body": "OK resolved with 1.1.0.\nStrange I got 1.0.8 as I pip installed yesterday\nThanks!"}, {"user": "SkalskiP", "created_at": "2025-04-03T18:19:19Z", "body": "We released it this morning."}], "satisfaction_conditions": ["A solution that enables Wandb logging functionality to work properly", "Information about using the correct version of the RF-DETR package", "Clear guidance on how to resolve the issue without requiring code changes to their implementation"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:09:01"}, "dockerfile": "FROM python:3.10-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    build-essential \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/roboflow/rf-detr.git . \\\n    && git checkout 5e2979d55e2aab0da80bd8a8ca8cf09c6f4ab909\n\n# Install Python dependencies with metrics extras which includes wandb\nRUN pip install --no-cache-dir -e \".[metrics]\" --extra-index-url https://download.pytorch.org/whl/cpu\n\n# Install additional dependencies for wandb logging\nRUN pip install --no-cache-dir wandb==0.19.9\n\n# Set default command\nCMD [\"bash\"]", "language": "python"}
{"number": 47, "title": "Parameters in model.predict() function", "created_at": "2025-03-25T11:21:57Z", "closed_at": "2025-03-27T09:07:50Z", "commit_id": "a3248a35df3a1c6deb799d87ee0b85ba18adf20e", "labels": [], "url": "https://github.com/roboflow/rf-detr/issues/47", "body": "Hi, Can you please provide me the details of different parameters in model.predict() function so i can play around them\nThanks", "comments_url": "https://api.github.com/repos/roboflow/rf-detr/issues/47/comments", "author": "MuhammadMoinFaisal", "comments": [{"user": "SkalskiP", "created_at": "2025-03-25T13:21:39Z", "body": "Hi @MuhammadMoinFaisal thanks for your interest in RF-DETR. As far as I know we do not have any other parameters configurable at `prediction` step."}, {"user": "isaacrob-roboflow", "created_at": "2025-03-26T18:11:14Z", "body": "@MuhammadMoinFaisal the only used argument to predict is \"threshold\", which sets the confidence threshold. was there something else you were expecting?"}, {"user": "MuhammadMoinFaisal", "created_at": "2025-03-27T09:07:47Z", "body": "Okay got it, \nThanks @SkalskiP @isaacrob-roboflow  "}], "satisfaction_conditions": ["Information about available parameters in the model.predict() function", "Clarification on which parameters can be configured during prediction", "Explanation of parameter functionality"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:05"}, "dockerfile": null, "language": "python"}
{"number": 21, "title": "How to run prediction on a video file?", "created_at": "2025-03-22T07:47:07Z", "closed_at": "2025-03-23T09:37:39Z", "commit_id": "5b2af103a1a9ee7b957507b9b1d7dd783a23049e", "labels": [], "url": "https://github.com/roboflow/rf-detr/issues/21", "body": "Hi,\n\nPlease share how I can run my fine-tuned model on a video file and save its output video?", "comments_url": "https://api.github.com/repos/roboflow/rf-detr/issues/21/comments", "author": "dsbyprateekg", "comments": [{"user": "farukalamai", "created_at": "2025-03-23T08:05:50Z", "body": "Hey @dsbyprateekg you can use this code\n\n```bash\nimport supervision as sv\nfrom rfdetr import RFDETRBase\nfrom tqdm import tqdm\nimport json\n\n# Define input and output video paths\nSOURCE_VIDEO_PATH = \"3727445-hd_1920_1080_30fps.mp4\"  # Change this to your input video path\nTARGET_VIDEO_PATH = \"output_video.mp4\"  # Change this to your desired output path\n\n# Load class mapping from JSON file\nwith open(\"classes.json\", \"r\") as f:\n    class_mapping = json.load(f)\n\n# Initialize the RFDETRBase model\nmodel = RFDETRBase()\n\n# Create a generator for video frames\nframe_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n\n# Get video information (resolution, fps, etc.)\nvideo_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n\n# Process the video frame by frame\nwith sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n    for frame in tqdm(frame_generator, desc=\"Processing video\"):\n        # Get detections using RFDETRBase model\n        detections = model.predict(frame, threshold=0.3)\n        \n        # Map numeric class IDs to class names for labels\n        labels = []\n        for class_id in detections.class_id:\n            # Convert int to string for dictionary lookup\n            class_id_str = str(class_id)\n            # Get class name if exists in mapping\n            class_name = class_mapping.get(class_id_str)\n            labels.append(class_name)\n        \n        # Create annotated frame\n        annotated_frame = frame.copy()\n        \n        # Apply box annotations\n        annotated_frame = sv.BoxAnnotator().annotate(scene=annotated_frame, detections=detections)\n        \n        # Apply label annotations with proper class names\n        annotated_frame = sv.LabelAnnotator(text_thickness=2).annotate(\n            scene=annotated_frame, \n            detections=detections,\n            labels=labels\n        )\n        \n        # Write the annotated frame to output video\n        sink.write_frame(annotated_frame)\n\nprint(f\"Video processing complete. Output saved to {TARGET_VIDEO_PATH}\")\n```"}, {"user": "dsbyprateekg", "created_at": "2025-03-23T09:23:33Z", "body": "@farukalamai Thanks a lot for sharing the code.\nIt's working."}, {"user": "probicheaux", "created_at": "2025-03-23T15:45:26Z", "body": "thanks for sharing that @farukalamai !"}, {"user": "MuhammadMoinFaisal", "created_at": "2025-03-24T10:18:10Z", "body": "Hi \nCan any one please share the code to do object detection using RF-DETR on Live Webcam Feed\n\nThanks"}, {"user": "ediardo", "created_at": "2025-03-25T21:35:26Z", "body": "> Hi Can any one please share the code to do object detection using RF-DETR on Live Webcam Feed\n> \n> Thanks\n\n@MuhammadMoinFaisal: for rtsp\n\n```py\nimport json\nimport cv2\nimport os\nfrom rfdetr import RFDETRBase\nimport supervision as sv\n\nmodel = RFDETRBase()\n\n# Load class mapping from JSON file\nwith open(\"classes.json\", \"r\") as f:\n    class_mapping = json.load(f)\n\nclass RTSPImageCapture:\n    def __init__(self, rtsp_url, output_dir):\n        self.rtsp_url = rtsp_url\n        self.output_dir = output_dir\n        self.cap = None\n        self.image_count = 0\n\n    def open_stream(self):\n        # Create a VideoCapture object to connect to the RTSP stream\n        self.cap = cv2.VideoCapture(self.rtsp_url)\n\n        # Check if the VideoCapture object was successfully created\n        if not self.cap.isOpened():\n            print(\"Error: Could not open RTSP stream.\")\n            exit()\n\n        # Create the output directory if it doesn't exist\n        os.makedirs(self.output_dir, exist_ok=True)\n\n    def capture_images(self):\n        while True:\n            # Capture a frame from the RTSP stream\n            ret, frame = self.cap.read()\n\n            # Check if the frame was captured successfully\n            if not ret:\n                print(\"Error: Could not read frame from RTSP stream.\")\n                break\n            \n            detections =model.predict(frame)\n            # Map numeric class IDs to class names for labels\n            labels = []\n            for class_id in detections.class_id:\n                # Convert int to string for dictionary lookup\n                class_id_str = str(class_id)\n                # Get class name if exists in mapping\n                class_name = class_mapping.get(class_id_str)\n                labels.append(class_name)\n\n            # Create annotated frame\n            annotated_frame = frame.copy()\n            \n            # Apply box annotations\n            annotated_frame = sv.BoxAnnotator().annotate(scene=annotated_frame, detections=detections)\n\n            # Apply label annotations with proper class names\n            annotated_frame = sv.LabelAnnotator(text_thickness=2).annotate(\n                scene=annotated_frame, \n                detections=detections,\n                labels=labels\n            )\n            # Display the captured frame (optional)\n            cv2.imshow('Captured Frame', annotated_frame)\n\n            # Exit the loop when 'q' is pressed\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    def close_stream(self):\n        # Release the VideoCapture object and close the OpenCV window\n        if self.cap is not None:\n            self.cap.release()\n            cv2.destroyAllWindows()\n\n    def main(self):\n        try:\n            self.open_stream()\n            self.capture_images()\n        finally:\n            self.close_stream()\n\nif __name__ == \"__main__\":\n    # Define the RTSP stream URL and output directory\n    rtsp_url = 'rtsp://username:passwd@192.168.1.203:554/stream1'\n\n    # Create an instance of the RTSPImageCapture class\n    image_capture = RTSPImageCapture(rtsp_url, output_dir)\n\n    # Run the main function of the class\n    image_capture.main()\n```\n\ncoco class mappings:\n```json\n{\n    \"1\": \"person\",\n    \"2\": \"bicycle\",\n    \"3\": \"car\",\n    \"4\": \"motorcycle\",\n    \"5\": \"airplane\",\n    \"6\": \"bus\",\n    \"7\": \"train\",\n    \"8\": \"truck\",\n    \"9\": \"boat\",\n    \"10\": \"traffic light\",\n    \"11\": \"fire hydrant\",\n    \"13\": \"stop sign\",\n    \"14\": \"parking meter\",\n    \"15\": \"bench\",\n    \"16\": \"bird\",\n    \"17\": \"cat\",\n    \"18\": \"dog\",\n    \"19\": \"horse\",\n    \"20\": \"sheep\",\n    \"21\": \"cow\",\n    \"22\": \"elephant\",\n    \"23\": \"bear\",\n    \"24\": \"zebra\",\n    \"25\": \"giraffe\",\n    \"27\": \"backpack\",\n    \"28\": \"umbrella\",\n    \"31\": \"handbag\",\n    \"32\": \"tie\",\n    \"33\": \"suitcase\",\n    \"34\": \"frisbee\",\n    \"35\": \"skis\",\n    \"36\": \"snowboard\",\n    \"37\": \"sports ball\",\n    \"38\": \"kite\",\n    \"39\": \"baseball bat\",\n    \"40\": \"baseball glove\",\n    \"41\": \"skateboard\",\n    \"42\": \"surfboard\",\n    \"43\": \"tennis racket\",\n    \"44\": \"bottle\",\n    \"46\": \"wine glass\",\n    \"47\": \"cup\",\n    \"48\": \"fork\",\n    \"49\": \"knife\",\n    \"50\": \"spoon\",\n    \"51\": \"bowl\",\n    \"52\": \"banana\",\n    \"53\": \"apple\",\n    \"54\": \"sandwich\",\n    \"55\": \"orange\",\n    \"56\": \"broccoli\",\n    \"57\": \"carrot\",\n    \"58\": \"hot dog\",\n    \"59\": \"pizza\",\n    \"60\": \"donut\",\n    \"61\": \"cake\",\n    \"62\": \"chair\",\n    \"63\": \"couch\",\n    \"64\": \"potted plant\",\n    \"65\": \"bed\",\n    \"67\": \"dining table\",\n    \"70\": \"toilet\",\n    \"72\": \"tv\",\n    \"73\": \"laptop\",\n    \"74\": \"mouse\",\n    \"75\": \"remote\",\n    \"76\": \"keyboard\",\n    \"77\": \"cell phone\",\n    \"78\": \"microwave\",\n    \"79\": \"oven\",\n    \"80\": \"toaster\",\n    \"81\": \"sink\",\n    \"82\": \"refrigerator\",\n    \"84\": \"book\",\n    \"85\": \"clock\",\n    \"86\": \"vase\",\n    \"87\": \"scissors\",\n    \"88\": \"teddy bear\",\n    \"89\": \"hair drier\",\n    \"90\": \"toothbrush\"\n  }\n```"}], "satisfaction_conditions": ["Code that processes a video file with a fine-tuned model and saves the output with annotations", "Complete, executable code sample that requires minimal modification", "Visual representation of model predictions on the video frames", "Integration with the specific model architecture (RF-DETR) the user is working with"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:10"}, "dockerfile": null, "language": "python"}
{"number": 17, "title": "Train on multiple gpus", "created_at": "2025-03-21T19:07:23Z", "closed_at": "2025-03-21T21:12:18Z", "commit_id": "5b1a767ecc769e46a09eaef69d149fba5d25b83a", "labels": [], "url": "https://github.com/roboflow/rf-detr/issues/17", "body": "Hello, \n\nThanks for the great work!  How could one train on multiple gpus with the current set up? ", "comments_url": "https://api.github.com/repos/roboflow/rf-detr/issues/17/comments", "author": "quocanh010", "comments": [{"user": "probicheaux", "created_at": "2025-03-21T19:23:58Z", "body": "Hey, @quocanh010 !\n\nI haven't tested multi-gpu in the code's current state, but I'm fairly sure if you create a script named main.py that calls RFDETRBase().train() and then launch that script with a shell command\n\n```\npython -u -m torch.distributed.launch \\\n    --nproc_per_node=8 \\ # change to your number of gpus\n    --use_env \\\n    main.py \\\n```\n\neverything should just work, as we used DDP with an earlier version of the code. Would love to hear if this works for you! Please remember that this will multiply your batch size by the number of gpus."}, {"user": "quocanh010", "created_at": "2025-03-21T21:11:42Z", "body": "That works! Thanks! "}, {"user": "HandsLing", "created_at": "2025-04-07T07:06:59Z", "body": "@probicheaux Hello, I am using four GPUs for training, but I noticed that there are four processes running on the first GPU, and after training for a while, it causes an out-of-memory error. Do you know how to resolve this issue?"}], "satisfaction_conditions": ["A working approach to distribute training across multiple GPUs", "A solution that integrates with the existing codebase structure", "Clear instructions that can be implemented without extensive prior knowledge"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:17"}, "dockerfile": null, "language": "python"}
{"number": 16, "title": "best model", "created_at": "2025-03-21T18:31:35Z", "closed_at": "2025-03-24T19:10:40Z", "commit_id": "5b1a767ecc769e46a09eaef69d149fba5d25b83a", "labels": [], "url": "https://github.com/roboflow/rf-detr/issues/16", "body": "Hello,  I try train rfdet 100 epochs on coco dataset with 1 class, and\nget good metrics ({\"class_map\": {\"valid\": [{\"map95\": 0.7727289854838079, \"map50\": 0.9457314068264822, \"class\": \"all\"}]}}) and have basic questions - \nI have 1 class and it indexed with 0 (not 1 as coco) is this correct?:\n    \"info\": {},\n    \"licenses\": [],\n    \"categories\": [\n        {\n            \"id\": 0,\n            \"name\": \"Mhp\"\n        }\n    ],\n\nI get best model on path \"/content/output/checkpoint_best_total.pth\" but there is also other candidate for \"best\" -  checkpoint_best_regular.pth and checkpoint_best_ema.pth and checkpoint.pth. Where is best model for best metric value? I read in blog that 50 epochs is good start for model training ( I choose 100). My question is there any documentation (or plans for it)  for hyper params settings, augmentation and optimization? How resume from checkpoint, and other basic staff? Thank you!\n\n", "comments_url": "https://api.github.com/repos/roboflow/rf-detr/issues/16/comments", "author": "ai8049520", "comments": [{"user": "probicheaux", "created_at": "2025-03-21T19:16:39Z", "body": "These are great questions and we definitely have more work to do documenting the things you've mentioned.\n\n> I get best model on path \"/content/output/checkpoint_best_total.pth\" but there is also other candidate for \"best\" - checkpoint_best_regular.pth and checkpoint_best_ema.pth and checkpoint.pth. Where is best model for best metric value?\n\nDuring training, we essentially create 2 models -- the regular model that you're training, and another version called the EMA model, which is an average of all model checkpoints. The best regular is saved to `checkpoint_best_regular.pth` and the best EMA is saved to `checkpoint_best_ema.pth`. At the end of training we see who got the all time highest performance on the val set and copy over to `checkpoint_best_total.pth`, so that one should be used for downstream inference.\n\n> How resume from checkpoint\n```python\nfrom rfdetr import RFDETRBase\nmodel = RFDETRBase(pretrain_weights=\"your/pretrain/weights.pth\", resume=\"your/pretrain/weights.pth\")\nmodel.train(...)\n```\n\n> augmentation\n\nthis isn't currently configurable\n\n\n> optimization\n\nwe haven't measured anything except for the defaults\n\nHope this helps and please reach out if anything is unclear!"}, {"user": "ai8049520", "created_at": "2025-03-24T19:10:40Z", "body": "Thank you!"}], "satisfaction_conditions": ["Clarification on which model checkpoint file represents the best performance", "Instructions for resuming training from a checkpoint", "Information about model training parameters and configuration options", "Explanation of how the training process works"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:25"}, "dockerfile": null, "language": "python"}
{"number": 148, "title": "Bug Report: The ArduPilot Guru needs around 2000 documents", "created_at": "2025-02-25T17:21:25Z", "closed_at": "2025-03-05T09:39:00Z", "commit_id": "15125ebd5bfd0e4e4c221dc63ce48018518e3612", "labels": ["bug"], "url": "https://github.com/Gurubase/gurubase/issues/148", "body": "## Description\nThe ArduPilot Guru needs around 2000 documents but there is a limit of 1500\n\n## Steps to Reproduce\nAdd the pages that we need to get proper responses\n\n\n## Environment\n- [x] Gurubase.io\n- [ ] Self-hosted\n\n## Solution\n\nOn the ArduPilot guru raise the limit to 2000\n", "comments_url": "https://api.github.com/repos/Gurubase/gurubase/issues/148/comments", "author": "amilcarlucas", "comments": [{"user": "kursataktas", "created_at": "2025-02-26T11:17:17Z", "body": "Hey @amilcarlucas \n\nWe set a 1500-page limit on purpose. I took a look at how ArduPilot is being used and saw that the number of daily questions is pretty high. I don’t want to block you because of this limit. I’ll check into it and figure out what I can do. I’ll update this thread soon."}, {"user": "kursataktas", "created_at": "2025-03-04T21:24:59Z", "body": "@amilcarlucas I have increased the website limit of ArduPilot Guru to 4000. You should now be able to add your sources. Can you confirm?"}, {"user": "amilcarlucas", "created_at": "2025-03-05T09:05:31Z", "body": "Thank you!!\n\nAdded the sources. It is processing sources as we speak. Let's see if it finishes.\n"}, {"user": "amilcarlucas", "created_at": "2025-03-05T09:39:00Z", "body": "It finished, thanks!\n"}], "satisfaction_conditions": ["Increased document limit for ArduPilot Guru", "Ability to add all necessary documentation sources", "Successful processing of the added documents"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:09:30"}, "dockerfile": null, "language": "python"}
{"number": 123, "title": "Error processing multiple transcripts", "created_at": "2025-03-27T00:45:14Z", "closed_at": "2025-03-27T09:13:36Z", "commit_id": "ebab2a584c9454791f4a5548091d6182783a2915", "labels": [], "url": "https://github.com/SesameAILabs/csm/issues/123", "body": "I get up to this point and get the following error:\n\n```\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-36-a8de7bbe24cc> in <cell line: 0>()\n     20     return audio_tensor\n     21 \n---> 22 segments = [\n     23     Segment(text=transcript, speaker=speaker, audio=load_audio(audio_path))\n     24     for transcript, speaker, audio_path in zip(transcripts, speakers, audio_paths)\n\n5 frames\n/usr/local/lib/python3.11/dist-packages/torio/io/_streaming_media_decoder.py in __init__(self, src, format, option, buffer_size)\n    524             self._be = ffmpeg_ext.StreamingMediaDecoderFileObj(src, format, option, buffer_size)\n    525         else:\n--> 526             self._be = ffmpeg_ext.StreamingMediaDecoder(os.path.normpath(src), format, option)\n    527 \n    528         i = self._be.find_best_audio_stream()\n\nRuntimeError: Failed to open the input \"utterance_0.wav\" (No such file or directory).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7af35e777f86 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\n```\n\nI copied the code that was provided and still no dice.  What am I missing or is there a bug in the code?\n\n`speakers = [0, 1, 0, 0]\ntranscripts = [\n    \"Hey how are you doing.\",\n    \"Pretty good, pretty good.\",\n    \"I'm great.\",\n    \"So happy to be speaking to you.\",\n]\naudio_paths = [\n    \"utterance_0.wav\",\n    \"utterance_1.wav\",\n    \"utterance_2.wav\",\n    \"utterance_3.wav\",\n]\n\ndef load_audio(audio_path):\n    audio_tensor, sample_rate = torchaudio.load(audio_path)\n    audio_tensor = torchaudio.functional.resample(\n        audio_tensor.squeeze(0), orig_freq=sample_rate, new_freq=generator.sample_rate\n    )\n    return audio_tensor\n\nsegments = [\n    Segment(text=transcript, speaker=speaker, audio=load_audio(audio_path))\n    for transcript, speaker, audio_path in zip(transcripts, speakers, audio_paths)\n]\naudio = generator.generate(\n    text=\"Me too, this is some cool stuff huh?\",\n    speaker=1,\n    context=segments,\n    max_audio_length_ms=10_000,\n)\n\ntorchaudio.save(\"audio.wav\", audio.unsqueeze(0).cpu(), generator.sample_rate)`", "comments_url": "https://api.github.com/repos/SesameAILabs/csm/issues/123/comments", "author": "vleandro09", "comments": [{"user": "vleandro09", "created_at": "2025-03-27T00:51:24Z", "body": "Sorry very new to all this...is the expectation here (correct me if I'm wrong) is to provide these audio waves pre-recorded with the transcript?  Or is the expectation that we're building a matrix?  In other words utterance wave 0 should be the [0] index of the transcript?"}, {"user": "ZackHodari", "created_at": "2025-03-27T09:13:36Z", "body": "> RuntimeError: Failed to open the input \"utterance_0.wav\" (No such file or directory).\nThis file does not exist, you will need to pick audio that you want to provide the model with if using context.\n\nThe contextual example in the readme is for demonstration purposes\n\nYou can use run_csm.py, this uses files that do exist"}, {"user": "vleandro09", "created_at": "2025-03-30T18:15:57Z", "body": "Got it.  Thank you so much!"}], "satisfaction_conditions": ["Clarification about the source of audio files referenced in the code", "Explanation of the error message related to missing files", "Alternative approach to run the code successfully", "Clarification about the purpose of the example code in the documentation"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:31"}, "dockerfile": null, "language": "python"}
{"number": 62, "title": "How to generate longer song?", "created_at": "2025-02-10T08:49:23Z", "closed_at": "2025-02-11T01:43:57Z", "commit_id": "5d4ec8d7d7897c101735f94cd22fd0f5984db0da", "labels": [], "url": "https://github.com/multimodal-art-projection/YuE/issues/62", "body": "Hello, I found this project amazing and want to try out locally.\nI have 1 * RTX 4090 24G running and followed the instruction in readme.md,\nbut the result is short (length=37 sec at most, max_new_token=12000) and stop abruptly (the verse is not finished).\nAt first I thought it was the problem that the verse is too long,\nbut even after I remove some sentence in verse, the situation remains.\nI hope to generate a song with 1 complete verse and 1 complete chrous, what can I do?\n\nlyrics.txt:\n```\n[verse]\n對這個世界如果你有太多的抱怨\n跌倒了就不敢繼續往前走\n我們是不是該知足\n珍惜一切 就算沒有擁有\n\n[chorus]\n還記得你說家是唯一的城堡\n隨著稻香河流繼續奔跑\n微微笑 小時候的夢我知道\n不要哭讓螢火蟲帶著你逃跑\n鄉間的歌謠永遠的依靠\n回家吧 回到最初的美好\n```\n\ngenre.txt:\n```\ninspiring male guitar 流行 Rap acoustic guitar drums voice keyboard\n```\n\ncommand:\n`python infer.py     --cuda_idx 0     --stage1_model m-a-p/YuE-s1-7B-anneal-zh-cot     --stage2_model m-a-p/YuE-s2-1B-general     --genre_txt ../yue_example/genre.txt     --lyrics_txt ../yue_example/lyrics.txt     --run_n_segments 2     --stage2_batch_size 4     --output_dir ../output_lyric_short     --max_new_tokens 12000     --repetition_penalty 1.1  --keep_intermediate`", "comments_url": "https://api.github.com/repos/multimodal-art-projection/YuE/issues/62/comments", "author": "clairelee5740", "comments": [{"user": "a43992899", "created_at": "2025-02-10T12:27:31Z", "body": "max_new_token=3000, but you should provide more sessions of lyrics. \n\nyou should also change `--run_n_segments` accordingly, e.g. change it to 4 or larger number if you want longer audio\n\ne.g.\n\n```bash\n[verse]\n對這個世界如果你有太多的抱怨\n跌倒了就不敢繼續往前走\n我們是不是該知足\n珍惜一切 就算沒有擁有\n\n[chorus]\n還記得你說家是唯一的城堡\n隨著稻香河流繼續奔跑\n微微笑 小時候的夢我知道\n不要哭讓螢火蟲帶著你逃跑\n鄉間的歌謠永遠的依靠\n回家吧 回到最初的美好\n\n[verse]\n對這個世界如果你有太多的抱怨\n跌倒了就不敢繼續往前走\n我們是不是該知足\n珍惜一切 就算沒有擁有\n\n[chorus]\n還記得你說家是唯一的城堡\n隨著稻香河流繼續奔跑\n微微笑 小時候的夢我知道\n不要哭讓螢火蟲帶著你逃跑\n鄉間的歌謠永遠的依靠\n回家吧 回到最初的美好\n\n[verse]\n對這個世界如果你有太多的抱怨\n跌倒了就不敢繼續往前走\n我們是不是該知足\n珍惜一切 就算沒有擁有\n\n\n[bridge]\n對這個世界如果你有太多的抱怨\n跌倒了就不敢繼續往前走\n我們是不是該知足\n珍惜一切 就算沒有擁有\n\n[outro]\n回家吧 回到最初的美好\n```"}, {"user": "clairelee5740", "created_at": "2025-02-11T01:43:57Z", "body": "I have generate longer song by changing `--run_n_segments=4`, thanks!"}], "satisfaction_conditions": ["Instructions for generating longer songs that don't stop abruptly", "Guidance on appropriate parameter settings to control song length", "A solution that works with their existing hardware setup (RTX 4090)", "A method to generate complete song structures with full verses and choruses"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:08:18"}, "dockerfile": "FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n\n# Set environment variables to avoid interactive prompts\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=UTC\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wget \\\n    python3 \\\n    python3-pip \\\n    ffmpeg \\\n    libsndfile1 \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/multimodal-art-projection/YuE.git . && \\\n    git checkout 5d4ec8d7d7897c101735f94cd22fd0f5984db0da\n\n# Install Python dependencies\nRUN pip3 install --no-cache-dir -r requirements.txt\n\n# Install additional dependencies needed for the project\n# These are inferred from the issue and the project structure\nRUN pip3 install --no-cache-dir \\\n    torch \\\n    torchaudio \\\n    transformers \\\n    sentencepiece \\\n    librosa \\\n    scipy \\\n    matplotlib \\\n    huggingface_hub\n\n# Create directories for input/output\nRUN mkdir -p /app/output /app/input\n\n# Set the working directory to the inference folder\nWORKDIR /app/inference\n\n# Download the xcodec_mini_infer binary if needed (placeholder - adjust if needed)\n# The repository structure suggests this is needed but may already be included\n\n# Set default command to show usage information\nCMD [\"echo\", \"YuE Docker environment is ready. Run your inference command with appropriate parameters.\"]", "language": "python"}
{"number": 26, "title": "Update top_200_tags.json - remove \"instrumental\" keyword", "created_at": "2025-01-29T23:36:38Z", "closed_at": "2025-01-31T19:05:20Z", "commit_id": "45ec788c113d7c739ed149dd8f95f36b6cbbcc49", "labels": [], "url": "https://github.com/multimodal-art-projection/YuE/pull/26", "body": "\"instrumental\" is used to as a keyword when recombining audio files at the end of stage 2. If it's added to the genre configuration file vocal_output is never created as all files contain the keyword causing the application to crash. Removing \"instrument\" keyword from tags for to prevent wide use.", "comments_url": "https://api.github.com/repos/multimodal-art-projection/YuE/issues/26/comments", "author": "mattjamo", "comments": [{"user": "a43992899", "created_at": "2025-01-30T04:25:30Z", "body": "Thx, we will update the naming of the output file. It would fix the problem, no need to remove \"instrument\" keyword in top_200_tags.json ."}, {"user": "mattjamo", "created_at": "2025-01-31T19:05:07Z", "body": "Np, sounds good! Yeah, I thought implementing a fix myself would likely end up not being the preferred solution so went with the short term quick fix just to highlight the issue."}, {"user": "mattjamo", "created_at": "2025-01-31T19:05:20Z", "body": "closing"}], "satisfaction_conditions": ["A solution that prevents the application from crashing when processing files with the 'instrumental' keyword", "Acknowledgment of the issue without requiring the user's specific implementation", "A maintainer-approved approach to fixing the file processing problem"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:08:27"}, "dockerfile": null, "language": "python"}
{"number": 18, "title": "How to generate just music, no lyrics?", "created_at": "2025-01-29T06:11:03Z", "closed_at": "2025-01-29T06:31:46Z", "commit_id": "29055d3930f50ebe86a767704b2edc428ba5f9b5", "labels": [], "url": "https://github.com/multimodal-art-projection/YuE/issues/18", "body": "I tried passing in an empty lyrics.txt and not passing in the --lyrics_txt argument.  Both give errors.\n\nIs it possible to generate a song without lyrics/vocals?", "comments_url": "https://api.github.com/repos/multimodal-art-projection/YuE/issues/18/comments", "author": "SoftologyPro", "comments": [{"user": "a43992899", "created_at": "2025-01-29T06:31:42Z", "body": "You can provide session label with empty lyrics \" \", e.g. a space. \n\nFor genre.txt, you should remove the tags related to vocal.\n\nFor lyrics.txt, it will look something like this:\n```\n[verse]\n \n[chorus]\n \n[outro]\n \n```\n\nI am not sure about the musicality though. You need to play around with the prompt and find a stable one.\n\nOr you can simply use the instrumental track in the output folder. Our model provides both vocal track and instrumental backing track."}, {"user": "a43992899", "created_at": "2025-02-16T17:19:57Z", "body": "We just checked. Using several `\\n` to replace lyrics will get you non-vocal result. e.g.\n\n```bash\n[verse]\n\n\n\n\n \n[chorus]\n\n\n\n\n[chorus]\n\n\n\n\n[outro]\n\n```"}, {"user": "SoftologyPro", "created_at": "2025-02-16T23:16:58Z", "body": "> We just checked. Using several `\\n` to replace lyrics will get you non-vocal result. e.g.\n> \n> [verse]\n> \n> \n> \n> \n>  \n> [chorus]\n> \n> \n> \n> \n> [chorus]\n> \n> \n> \n> \n> [outro]\n\nI can confirm this works.  Thanks."}], "satisfaction_conditions": ["A method to generate music without vocals/lyrics", "A workaround that doesn't cause errors in the system"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:08:35"}, "dockerfile": null, "language": "python"}
{"number": 85, "title": "Problem running demo.py", "created_at": "2025-01-01T11:34:08Z", "closed_at": "2025-01-27T20:47:35Z", "commit_id": "4160a3ecc59f31151297c5d90e18a66206723a26", "labels": [], "url": "https://github.com/yangchris11/samurai/issues/85", "body": "Enter your own video and .txt runtime   \r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    #print(\"1111\")\r\n    parser.add_argument(\"--video_path\",default=\"./assets/3.mp4\", help=\"Input video path or directory of frames.\")\r\n    parser.add_argument(\"--txt_path\",default=\"./assets/test.txt\", help=\"Path to ground truth text file.\")\r\n    parser.add_argument(\"--model_path\", default=\"sam2/checkpoints/sam2.1_hiera_base_plus.pt\", help=\"Path to the model checkpoint.\")\r\n    parser.add_argument(\"--video_output_path\", default=\"./assets/output.mp4\", help=\"Path to save the output video.\")\r\n    parser.add_argument(\"--save_to_video\", default=True, help=\"Save results to a video.\")\r\n    args = parser.parse_args()\r\n    main(args)     \r\nThe following questions will appear. Please help me answer the specific reasons. thank you\r\n/home/cyy/anaconda3/envs/samurai/bin/python /home/cyy/code/grp/samurai-master/demo.py \r\nSAMURAI mode: True\r\n\r\nProcess finished with exit code 137 (interrupted by signal 9: SIGKILL)", "comments_url": "https://api.github.com/repos/yangchris11/samurai/issues/85/comments", "author": "Grpab", "comments": [{"user": "hsiangwei0903", "created_at": "2025-01-02T00:01:06Z", "body": "Looks like you ran out of memory. You might need to use a shorter video. "}, {"user": "Grpab", "created_at": "2025-01-02T00:47:22Z", "body": "> 看来您的内存不足了。您可能需要使用较短的视频。\r\n\r\nThank you for your answer, I will try your suggestion"}], "satisfaction_conditions": ["An explanation for why the program terminated with exit code 137", "A practical solution to resolve the memory-related termination issue"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:10:46"}, "dockerfile": "FROM python:3.10-slim\n\n# Set environment variables\nENV PYTHONUNBUFFERED=1\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1 \\\n    wget \\\n    build-essential \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/yangchris11/samurai.git . \\\n    && git checkout 4160a3ecc59f31151297c5d90e18a66206723a26\n\n# Install PyTorch and TorchVision dependencies first (lighter version to avoid timeout)\nRUN pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cpu torch==2.3.1+cpu torchvision==0.18.1+cpu\n\n# Install SAM2 with minimal dependencies\nRUN cd sam2 && pip install -e . --no-deps && pip install numpy pillow\n\n# Install other requirements with minimal dependencies\nRUN pip install --no-cache-dir matplotlib==3.7 opencv-python pandas scipy loguru\n\n# Create directories\nRUN mkdir -p sam2/checkpoints assets data\n\n# Download a smaller SAM 2.1 checkpoint to avoid timeout\nRUN cd sam2/checkpoints && \\\n    wget -q https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\n\n# Create a sample test.txt file for demo\nRUN echo \"0,0,100,100\" > ./assets/test.txt\n\n# Download a smaller sample video for testing\nRUN wget -q -O ./assets/3.mp4 https://github.com/intel-iot-devkit/sample-videos/raw/master/face-demographics-walking.mp4 || \\\n    echo \"Using placeholder video\" && touch ./assets/3.mp4\n\n# Set the working directory\nWORKDIR /app", "language": "python"}
{"number": 109, "title": "The kf_socre is always None", "created_at": "2025-03-18T05:36:13Z", "closed_at": "2025-03-19T01:53:42Z", "commit_id": "be4853ebedbae57cdbf523ed6ee70968837c64ad", "labels": [], "url": "https://github.com/yangchris11/samurai/issues/109", "body": "i found the kf_socre in always  none in sam2_base.py/_prepare_memory_conditioned_features, maybe the kf_socre mean kf_ious (but not compact to \"current_out\" in sam2_vidio_predictor.py/_run_single_frame_inference)?\n\n", "comments_url": "https://api.github.com/repos/yangchris11/samurai/issues/109/comments", "author": "txzhou-hust", "comments": [{"user": "yangchris11", "created_at": "2025-03-18T20:47:42Z", "body": "@InfluenceFunction  Thank you for pointing this out!\n\nWe double check out implementation/commit history and found out this two lines should be added to `_run_single_frame_inference` in the `sam2_video_predictor.py` file (which is somehow mistaken during refactoring of the code):\n```\n        # object pointer is a small tensor, so we always keep it on GPU memory for fast access\n        obj_ptr = current_out[\"obj_ptr\"]\n        object_score_logits = current_out[\"object_score_logits\"]\n        best_iou_score = current_out[\"best_iou_score\"]\n        best_kf_score = current_out[\"kf_ious\"]                                             # this line\n        # make a compact version of this frame's output to reduce the state size\n        compact_current_out = {\n            \"maskmem_features\": maskmem_features, # (B, C, H, W)\n            \"maskmem_pos_enc\": maskmem_pos_enc, \n            \"pred_masks\": pred_masks,\n            \"obj_ptr\": obj_ptr,\n            \"object_score_logits\": object_score_logits,\n            \"best_iou_score\": best_iou_score,\n            \"kf_score\": best_kf_score,                                                   # and this line\n        }\n```\n\nPlease try to see in the updated version works, thank you!"}, {"user": "txzhou-hust", "created_at": "2025-03-19T01:52:40Z", "body": "ok，thank you, i have no other question"}], "satisfaction_conditions": ["Identification of the correct relationship between kf_score and kf_ious variables", "Clear explanation of what code changes are needed to fix the issue", "Acknowledgment that there was a mistake in the code during refactoring"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:10:38"}, "dockerfile": null, "language": "python"}
{"number": 31, "title": "Installing Kreuzberg 3.0.0 via poetry/pip does not install the _ocr module on windows", "created_at": "2025-03-26T08:53:17Z", "closed_at": "2025-03-26T09:41:08Z", "commit_id": "265cd491203b213b4a393696caa5816913085274", "labels": [], "url": "https://github.com/Goldziher/kreuzberg/issues/31", "body": "I recently tried to install Kreuzberg via Poetry using `poetry add kreuzberg`. \nThe line added to my pyproject.toml was `kreuzberg = \"^3.0.0\"`\nThe error that ensued after running my project was:\n```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"D:\\dev\\landtagszusammenfasser\\collector\\collector\\main.py\", line 10, in <module>\n    from collector.config import CollectorConfiguration\n  File \"D:\\dev\\landtagszusammenfasser\\collector\\collector\\config.py\", line 6, in <module>\n    from collector.scrapercache import ScraperCache\n  File \"D:\\dev\\landtagszusammenfasser\\collector\\collector\\scrapercache.py\", line 5, in <module>\n    from collector.document import Document\n  File \"D:\\dev\\landtagszusammenfasser\\collector\\collector\\document.py\", line 8, in <module>\n    from kreuzberg import ExtractionConfig, extract_file, TesseractConfig, PSMMode\n  File \"C:\\Users\\Benedikt\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\collector-nweI4fBd-py3.13\\Lib\\site-packages\\kreuzberg\\__init__.py\", line 1, in <module>\n    from kreuzberg._ocr._easyocr import EasyOCRConfig\nModuleNotFoundError: No module named 'kreuzberg._ocr'\n```\nAnd indeed, looking at the site-packages in the local environment there is no _ocr module installed even if the module clearly is in main.\nThe contents of the `kreuzberg` site-package are:\n```\n__pycache__/\n__init__.py\n_chunker.py\n_constants.py\n_mime_types.py\n_playa.py\n_registry.py\n_types.py\nexceptions.py\nextraction.py\npy.typed\n```\n\nI tried the same via pip in poetry's venv (pip install kreuzberg) as well as installing various extras [easyocr/paddleocr/all], but none of these options actually installed the _ocr package.\n\nThe issue is not present in 2.1, where everything works smoothly.\n\nIs this a known issue? Did I overlook something? Help is appreciated!", "comments_url": "https://api.github.com/repos/Goldziher/kreuzberg/issues/31/comments", "author": "Chrystalkey", "comments": [{"user": "Goldziher", "created_at": "2025-03-26T09:14:59Z", "body": "thanks for reporing this. \n\nVery weird. I will have to take a look into this asap. "}, {"user": "Goldziher", "created_at": "2025-03-26T09:36:56Z", "body": "Hi, please see if version 3.0.1 resolves the issue for you. \n\nI verified this locally - the tar.gz file is correct. Wanna be absolutely sure though."}, {"user": "Chrystalkey", "created_at": "2025-03-26T09:41:08Z", "body": "Yes, the module is found and the extraction is running. Thanks a lot for the quick fix!"}], "satisfaction_conditions": ["A fix that resolves the missing module error", "A working installation of Kreuzberg with OCR functionality on Windows", "A timely resolution to the package installation issue", "A solution that works with their existing package management setup (Poetry/pip)"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:06:57"}, "dockerfile": null, "language": "python"}
{"number": 3, "title": "TesseractNotFoundError: \"tesseract is not installed or it's not in your PATH\" during image extraction", "created_at": "2025-02-07T16:15:51Z", "closed_at": "2025-02-07T18:18:24Z", "commit_id": "e32b52dca268563238f082e28d56d42d70c7a52a", "labels": [], "url": "https://github.com/Goldziher/kreuzberg/issues/3", "body": "When attempting to extract text from an image file (e.g., `page1_img1.png`) using Kreuzberg, the following error is encountered:\n\n```\nTraceback (most recent call last):\n  File \"/path/to/project/.venv/lib/python3.10/site-packages/pytesseract/pytesseract.py\", line 275, in run_tesseract\n    proc = subprocess.Popen(cmd_args, **subprocess_args())\n  File \"/path/to/python3.10/subprocess.py\", line 971, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"/path/to/python3.10/subprocess.py\", line 1863, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory: 'tesseract'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/path/to/project/main.py\", line 35, in <module>\n    asyncio.run(main())\n  File \"/path/to/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/path/to/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/path/to/project/main.py\", line 29, in main\n    await extract_image()\n  File \"/path/to/project/main.py\", line 14, in extract_image\n    result = await extract_file(\"page1_img1.png\")\n  File \"/path/to/project/.venv/lib/python3.10/site-packages/kreuzberg/extraction.py\", line 138, in extract_file\n    return ExtractionResult(content=await _extract_image_with_tesseract(file_path), mime_type=PLAIN_TEXT_MIME_TYPE)\n  File \"/path/to/project/.venv/lib/python3.10/site-packages/kreuzberg/_extractors.py\", line 156, in _extract_image_with_tesseract\n    return normalize_spaces(cast(str, image_to_string(str(file_path))))\n  File \"/path/to/project/.venv/lib/python3.10/site-packages/pytesseract/pytesseract.py\", line 486, in image_to_string\n    return {\n  File \"/path/to/project/.venv/lib/python3.10/site-packages/pytesseract/pytesseract.py\", line 489, in <lambda>\n    Output.STRING: lambda: run_and_get_output(*args),\n  File \"/path/to/project/.venv/lib/python3.10/site-packages/pytesseract/pytesseract.py\", line 352, in run_and_get_output\n    run_tesseract(**kwargs)\n  File \"/path/to/project/.venv/lib/python3.10/site-packages/pytesseract/pytesseract.py\", line 280, in run_tesseract\n    raise TesseractNotFoundError()\npytesseract.pytesseract.TesseractNotFoundError: tesseract is not installed or it's not in your PATH. See README file for more information.\n```\n\n**Steps to Reproduce:**  \n1. Install and set up Kreuzberg in a Python 3.10 environment.  \n2. Attempt to extract text from an image file (e.g., `page1_img1.png`) using the `extract_file` method.  \n3. Observe the traceback indicating that Tesseract is not found.\n\n**Expected Behavior:**  \nThe library should either provide a clear error message with instructions on how to install Tesseract or handle the missing dependency gracefully by suggesting that the Tesseract package be downloaded and installed.\n\n**Additional Context:**  \n- **Operating System:** Linux  \n- **Python Version:** 3.10  \n- The error clearly indicates that Tesseract OCR is not installed or not available in the PATH. Since Tesseract is required for image text extraction, it might be beneficial to update the documentation (or error messaging) with installation instructions or suggestions to download the package if it is missing.\n\nAny guidance on how to resolve this issue or suggestions for improving the dependency check would be appreciated.", "comments_url": "https://api.github.com/repos/Goldziher/kreuzberg/issues/3/comments", "author": "Programmer-RD-AI", "comments": [{"user": "Goldziher", "created_at": "2025-02-07T18:13:46Z", "body": "Hi. The readme has a section about dependencies. \n\nIn a nutshell, install them 😁"}, {"user": "Programmer-RD-AI", "created_at": "2025-02-07T18:18:24Z", "body": "Got it, thank you :)"}], "satisfaction_conditions": ["Clear direction on where to find installation instructions for the required dependency", "Brief, straightforward guidance that acknowledges the need to install the missing dependency", "Confirmation that the error was due to missing dependencies rather than a code issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:07:06"}, "dockerfile": null, "language": "python"}
{"number": 110, "title": "Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`.`", "created_at": "2025-03-18T06:54:19Z", "closed_at": "2025-04-10T14:31:05Z", "commit_id": "384f47c1eb66f2fe76df5d330c11b9b481dd1791", "labels": [], "url": "https://github.com/Liuziyu77/Visual-RFT/issues/110", "body": "I download ViRFT_COCO dataset from huggingface, there is only [.parquet] files, no dataset_dict.json. Howevre when I run code,  An Error like this: \n`No such file: '/root/sourcecode/Visual-RFT-main/data/ViRFT_COCO/dataset_dict.json'. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`.`", "comments_url": "https://api.github.com/repos/Liuziyu77/Visual-RFT/issues/110/comments", "author": "Jay-zzcoder", "comments": [{"user": "Liuziyu77", "created_at": "2025-03-18T14:04:50Z", "body": "change\n```\nfrom datasets import DatasetDict\ndataset = DatasetDict.load_from_disk(script_args.dataset_name)\n```\nto\n```\ndataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n```\nin `grpo.py`"}, {"user": "Jay-zzcoder", "created_at": "2025-03-19T01:43:49Z", "body": "> change\n> \n> ```\n> from datasets import DatasetDict\n> dataset = DatasetDict.load_from_disk(script_args.dataset_name)\n> ```\n> \n> to\n> \n> ```\n> dataset = load_dataset(script_args.dataset_name, name=script_args.dataset_config)\n> ```\n> \n> in `grpo.py`\n\n已解决，感谢！"}], "satisfaction_conditions": ["A solution that correctly loads the ViRFT_COCO dataset from Hugging Face", "A code modification that handles the parquet file format appropriately", "A fix that addresses the specific error message about DatasetDict loading"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:05:06"}, "dockerfile": null, "language": "python"}
{"number": 516, "title": "docs: update copyright", "created_at": "2025-04-08T01:05:04Z", "closed_at": "2025-04-09T14:26:52Z", "commit_id": "ad75976d1f3ff32445a2df8e5ade5038b453a039", "labels": [], "url": "https://github.com/i-am-bee/beeai/pull/516", "body": null, "comments_url": "https://api.github.com/repos/i-am-bee/beeai/issues/516/comments", "author": "jenna-winkler", "comments": [{"user": "JanPokorny", "created_at": "2025-04-09T11:03:20Z", "body": "@jenna-winkler Is the copyright year intentionally missing?"}, {"user": "jenna-winkler", "created_at": "2025-04-09T13:04:49Z", "body": "> @jenna-winkler Is the copyright year intentionally missing?\r\n\r\n@JanPokorny good catch I checked with the LF because this is the string they provided me with but they said that wasn’t intentional we can include year  "}, {"user": "JanPokorny", "created_at": "2025-04-09T13:53:01Z", "body": "@jenna-winkler OK nice, it's easier if I don't have to hack NWA to _not_ include year 😁 In that case I'll just update the NWA config in this PR so that the checks pass"}], "satisfaction_conditions": ["Confirmation that including the copyright year is appropriate", "Permission to update the NWA configuration to include the year", "Clarification on the official copyright notice format"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:45"}, "dockerfile": null, "language": "python"}
{"number": 68, "title": "ValueError: No insider trades returned (when selecting Sentiment Agent)", "created_at": "2025-01-16T14:33:29Z", "closed_at": "2025-01-16T23:15:43Z", "commit_id": "b6635f5590b85aa8d7f64bb8d76fc25da8c8db97", "labels": ["bug"], "url": "https://github.com/virattt/ai-hedge-fund/issues/68", "body": "**Describe the bug**\nA ValueError is raised when sentiment agent is selected (my chosen ticker was 'ONON')\n\n**Copy of Output - my PII redacted**\n\n```\n$ poetry run python src/main.py --ticker ONON --show-reasoning\n\n...\n...\n\nFile \"/Users/.../.../ai-hedge-fund/src/agents/sentiment.py\", line 25, in sentiment_agent\n    insider_trades = get_insider_trades(\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/.../.../ai-hedge-fund/src/tools/api.py\", line 162, in get_insider_trades\n    raise ValueError(\"No insider trades returned\")\nValueError: No insider trades returned\nDuring task with name 'sentiment_agent' and id 'f03273d3-4704-b88b-9c55-e04c1c0721f1'\n```\n\n**Additional context**\nI would love to see the output of the sentiment agent for 'ONON' :) \n", "comments_url": "https://api.github.com/repos/virattt/ai-hedge-fund/issues/68/comments", "author": "lewisosborne", "comments": [{"user": "virattt", "created_at": "2025-01-16T23:15:37Z", "body": "Thank you for reporting @lewisosborne - I have made some changes to make this more graceful.\n\nInstead of erroring out when data is missing, the agent will simply print an error message and continue (without crashing)."}, {"user": "lewisosborne", "created_at": "2025-01-17T08:58:30Z", "body": "Fabulous! Thank you for adjusting @virattt \n\nI have updated branch and re-run cmds. Also, the 'backtester' output now works for me too (it was also failing on the same error).\n\nThis is a wonderful app :)\n"}], "satisfaction_conditions": ["Graceful handling of missing insider trade data", "Continued operation of the application when data is unavailable", "Functional backtester component", "Ability to see sentiment agent output for specific tickers"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:53"}, "dockerfile": null, "language": "python"}
{"number": 456, "title": "[Bug] 一直提示r: 您的配置文件太旧了，缺少了：", "created_at": "2025-03-20T13:44:13Z", "closed_at": "2025-03-21T05:43:36Z", "commit_id": "5ebc62a084686f9e33ada3cb26cf0e10b8988a9d", "labels": ["bug"], "url": "https://github.com/xinnan-tech/xiaozhi-esp32-server/issues/456", "body": ".ACGNTTS.output_file\n2025-03-20 21:41:32 xiaozhi-esp32-server  | - TTS.OpenAITTS.output_file\n2025-03-20 21:41:32 xiaozhi-esp32-server  | - TTS.CustomTTS.output_file\n2025-03-20 21:41:32 xiaozhi-esp32-server  | 建议您：\n2025-03-20 21:41:32 xiaozhi-esp32-server  | 1、备份data/.config.yaml文件\n2025-03-20 21:41:32 xiaozhi-esp32-server  | 2、将根目录的config.yaml文件复制到data下，重命名为.config.yaml\n2025-03-20 21:41:32 xiaozhi-esp32-server  | 3、将密钥逐个复制到新的配置文件中\n2025-03-20 21:41:32 xiaozhi-esp32-server  | \n2025-03-20 21:41:40 xiaozhi-esp32-server  | Traceback (most recent call last):\n2025-03-20 21:41:40 xiaozhi-esp32-server  |   File \"/opt/xiaozhi-esp32-server/app.py\", line 49, in <module>\n2025-03-20 21:41:40 xiaozhi-esp32-server  |     asyncio.run(main())\n2025-03-20 21:41:40 xiaozhi-esp32-server  |   File \"/usr/local/lib/python3.10/asyncio/runners.py\", line 44, in run\n2025-03-20 21:41:40 xiaozhi-esp32-server  |     return loop.run_until_complete(main)\n2025-03-20 21:41:40 xiaozhi-esp32-server  |   File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n2025-03-20 21:41:40 xiaozhi-esp32-server  |     return future.result()\n2025-03-20 21:41:40 xiaozhi-esp32-server  |   File \"/opt/xiaozhi-esp32-server/app.py\", line 27, in main\n2025-03-20 21:41:40 xiaozhi-esp32-server  |     check_config_file()\n2025-03-20 21:41:40 xiaozhi-esp32-server  |   File \"/opt/xiaozhi-esp32-server/config/settings.py\", line 84, in check_config_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  |     raise ValueError(error_msg)\n2025-03-20 21:41:40 xiaozhi-esp32-server  | ValueError: 您的配置文件太旧了，缺少了：\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - iot\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.EdgeTTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.DoubaoTTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.CosyVoiceSiliconflow.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.CozeCnTTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.FishSpeech.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.GPT_SOVITS_V2.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.GPT_SOVITS_V3.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.MinimaxTTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.AliyunTTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.TTS302AI.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.ACGNTTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.OpenAITTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | - TTS.CustomTTS.output_file\n2025-03-20 21:41:40 xiaozhi-esp32-server  | 建议您：\n2025-03-20 21:41:40 xiaozhi-esp32-server  | 1、备份data/.config.yaml文件\n2025-03-20 21:41:40 xiaozhi-esp32-server  | 2、将根目录的config.yaml文件复制到data下，重命名为.config.yaml\n2025-03-20 21:41:40 xiaozhi-esp32-server  | 3、将密钥逐个复制到新的配置文件中\n2025-03-20 21:41:40 xiaozhi-esp32-server  | \n2025-03-20 21:41:56 xiaozhi-esp32-server  | Traceback (most recent call last):\n2025-03-20 21:41:56 xiaozhi-esp32-server  |   File \"/opt/xiaozhi-esp32-server/app.py\", line 49, in <module>\n2025-03-20 21:41:56 xiaozhi-esp32-server  |     asyncio.run(main())\n2025-03-20 21:41:56 xiaozhi-esp32-server  |   File \"/usr/local/lib/python3.10/asyncio/runners.py\", line 44, in run\n2025-03-20 21:41:56 xiaozhi-esp32-server  |     return loop.run_until_complete(main)\n2025-03-20 21:41:56 xiaozhi-esp32-server  |   File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n2025-03-20 21:41:56 xiaozhi-esp32-server  |     return future.result()\n2025-03-20 21:41:56 xiaozhi-esp32-server  |   File \"/opt/xiaozhi-esp32-server/app.py\", line 27, in main\n2025-03-20 21:41:56 xiaozhi-esp32-server  |     check_config_file()\n2025-03-20 21:41:56 xiaozhi-esp32-server  |   File \"/opt/xiaozhi-esp32-server/config/settings.py\", line 84, in check_config_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  |     raise ValueError(error_msg)\n2025-03-20 21:41:56 xiaozhi-esp32-server  | ValueError: 您的配置文件太旧了，缺少了：\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - iot\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.EdgeTTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.DoubaoTTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.CosyVoiceSiliconflow.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.CozeCnTTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.FishSpeech.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.GPT_SOVITS_V2.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.GPT_SOVITS_V3.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.MinimaxTTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.AliyunTTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.TTS302AI.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.ACGNTTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.OpenAITTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | - TTS.CustomTTS.output_file\n2025-03-20 21:41:56 xiaozhi-esp32-server  | 建议您：\n2025-03-20 21:41:56 xiaozhi-esp32-server  | 1、备份data/.config.yaml文件\n2025-03-20 21:41:56 xiaozhi-esp32-server  | 2、将根目录的config.yaml文件复制到data下，重命名为.config.yaml\n2025-03-20 21:41:56 xiaozhi-esp32-server  | 3、将密钥逐个复制到新的配置文件中\n2025-03-20 21:41:56 xiaozhi-esp32-server  | \n2036-01-01 00:00:00 \nxiaozhi-esp32-server exited with code 1", "comments_url": "https://api.github.com/repos/xinnan-tech/xiaozhi-esp32-server/issues/456/comments", "author": "81199000", "comments": [{"user": "openrz", "created_at": "2025-03-20T13:50:28Z", "body": "试一下下面的命令清空当前版本版本，然后再使用docker-compose启动试一下。\n\n```\ndocker stop xiaozhi-esp32-server\ndocker rm xiaozhi-esp32-server\ndocker rmi ghcr.nju.edu.cn/xinnan-tech/xiaozhi-esp32-server:server_latest\n```"}, {"user": "81199000", "created_at": "2025-03-20T14:20:27Z", "body": "牛逼"}], "satisfaction_conditions": ["A solution that resolves the configuration file outdated error", "A simple command-line approach to reset the system", "Instructions that can be executed without deep technical knowledge"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:17"}, "dockerfile": null, "language": "python"}
{"number": 337, "title": "项目启动报错提示缺少music.music_commands配置", "created_at": "2025-03-14T08:37:44Z", "closed_at": "2025-03-14T09:24:36Z", "commit_id": "b8e9aded6b36b9807bcbe7ba1facca9140e58509", "labels": [], "url": "https://github.com/xinnan-tech/xiaozhi-esp32-server/issues/337", "body": "xiaozhi-esp32-server    | Traceback (most recent call last):\nxiaozhi-esp32-server    |   File \"/opt/xiaozhi-esp32-server/app.py\", line 26, in <module>\nxiaozhi-esp32-server    |     asyncio.run(main())\nxiaozhi-esp32-server    |   File \"/usr/local/lib/python3.10/asyncio/runners.py\", line 44, in run\nxiaozhi-esp32-server    |     return loop.run_until_complete(main)\nxiaozhi-esp32-server    |   File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\nxiaozhi-esp32-server    |     return future.result()\nxiaozhi-esp32-server    |   File \"/opt/xiaozhi-esp32-server/app.py\", line 10, in main\nxiaozhi-esp32-server    |     check_config_file()\nxiaozhi-esp32-server    |   File \"/opt/xiaozhi-esp32-server/config/settings.py\", line 84, in check_config_file\nxiaozhi-esp32-server    |     raise ValueError(error_msg)\nxiaozhi-esp32-server    | ValueError: 您的配置文件太旧了，缺少了：\nxiaozhi-esp32-server    | - music.music_commands\nxiaozhi-esp32-server    | 建议您：\nxiaozhi-esp32-server    | 1、备份data/.config.yaml文件\nxiaozhi-esp32-server    | 2、将根目录的config.yaml文件复制到data下，重命名为.config.yaml\nxiaozhi-esp32-server    | 3、将密钥逐个复制到新的配置文件中", "comments_url": "https://api.github.com/repos/xinnan-tech/xiaozhi-esp32-server/issues/337/comments", "author": "cikichen", "comments": [{"user": "journey-ad", "created_at": "2025-03-14T08:41:37Z", "body": "> xiaozhi-esp32-server | ValueError: 您的配置文件太旧了，缺少了：\n> xiaozhi-esp32-server | - music.music_commands\n> xiaozhi-esp32-server | 建议您：\n> xiaozhi-esp32-server | 1、备份data/.config.yaml文件\n> xiaozhi-esp32-server | 2、将根目录的config.yaml文件复制到data下，重命名为.config.yaml\n> xiaozhi-esp32-server | 3、将密钥逐个复制到新的配置文件中\n\n配置项有变动，按提示改下就行了。配置太多不想来回改也可以参照最新文件，手动增加`music.music_commands`这条配置"}, {"user": "cikichen", "created_at": "2025-03-14T08:42:52Z", "body": "> > xiaozhi-esp32-server | ValueError: 您的配置文件太旧了，缺少了：\n> > xiaozhi-esp32-server | - music.music_commands\n> > xiaozhi-esp32-server | 建议您：\n> > xiaozhi-esp32-server | 1、备份data/.config.yaml文件\n> > xiaozhi-esp32-server | 2、将根目录的config.yaml文件复制到data下，重命名为.config.yaml\n> > xiaozhi-esp32-server | 3、将密钥逐个复制到新的配置文件中\n> \n> 配置项有变动，按提示改下就行了。配置太多不想来回改也可以参照最新文件，手动增加`music.music_commands`这条配置\n\n看了代码最新配置文件，都没有这个选项"}, {"user": "cikichen", "created_at": "2025-03-14T08:46:16Z", "body": "ValueError: 不支持的记忆服务类型: nomem"}, {"user": "xinnan-tech", "created_at": "2025-03-14T08:57:48Z", "body": "你可能用的旧版docker？试试\n```\ndocker stop xiaozhi-esp32-server\ndocker rm xiaozhi-esp32-server\ndocker rmi ghcr.nju.edu.cn/xinnan-tech/xiaozhi-esp32-server:server_latest\n```\n然后下载最新的docker-compose启动docker"}, {"user": "cikichen", "created_at": "2025-03-14T09:24:36Z", "body": "> 你可能用的旧版docker？试试\n> \n> ```\n> docker stop xiaozhi-esp32-server\n> docker rm xiaozhi-esp32-server\n> docker rmi ghcr.nju.edu.cn/xinnan-tech/xiaozhi-esp32-server:server_latest\n> ```\n> \n> 然后下载最新的docker-compose启动docker\n\n我习惯性用的latest，没注意是server_latest"}], "satisfaction_conditions": ["Guidance on using the correct Docker image tag", "A solution that addresses version compatibility issues between the user's configuration and the current software requirements"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:22"}, "dockerfile": null, "language": "python"}
{"number": 260, "title": "从Log文件看到运行地址不是内网地址，正常吗", "created_at": "2025-03-10T03:01:01Z", "closed_at": "2025-03-10T13:40:32Z", "commit_id": "f9472627f471b64da136bdf57377c4bfe74b39c6", "labels": [], "url": "https://github.com/xinnan-tech/xiaozhi-esp32-server/issues/260", "body": "Log文件显示的地址是172.18.0.2\n\n25-03-10 10:57:19[core.websocket_server] - INFO - Server is running at ws://172.18.0.2:8000\n25-03-10 10:57:19[core.websocket_server] - INFO - =======上面的地址是websocket协议地址，请勿用浏览器访问=======\n\n这是正常的吗，我docker-compose.yml中设置了本地ip，但是config.yaml中默认那个0.0.0.0没有更改，这样设置是正确的吗？谢谢", "comments_url": "https://api.github.com/repos/xinnan-tech/xiaozhi-esp32-server/issues/260/comments", "author": "norsizu", "comments": [{"user": "xinnan-tech", "created_at": "2025-03-10T08:38:14Z", "body": "1、从Log文件看到运行地址不是内网地址，正常吗？\n回答：正常的，我们的教程文档说了，接口地址不能以日志显示的地址为主，要以机器在局域网的地址为主。\n\n\n2、config.yaml中默认那个0.0.0.0要不要更改？\n回答：用docker部署，就不需要改了。如果用conda部署，并且是运行在公网上，从网络安全角度上看，最好要改。"}, {"user": "norsizu", "created_at": "2025-03-10T09:03:01Z", "body": "> 1、从Log文件看到运行地址不是内网地址，正常吗？ 回答：正常的，我们的教程文档说了，接口地址不能以日志显示的地址为主，要以机器在局域网的地址为主。\n> \n> 2、config.yaml中默认那个0.0.0.0要不要更改？ 回答：用docker部署，就不需要改了。如果用conda部署，并且是运行在公网上，从网络安全角度上看，最好要改。\n\n多谢指导~"}], "satisfaction_conditions": ["Confirmation that the Docker container's internal IP address (172.18.0.2) showing in logs is normal behavior", "Clarification about whether the default '0.0.0.0' setting in config.yaml needs to be changed when using Docker", "Understanding the relationship between the displayed IP in logs and the actual network interface to use"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:26"}, "dockerfile": null, "language": "python"}
{"number": 41, "title": "RuntimeError: Unknown layout", "created_at": "2025-01-31T22:40:03Z", "closed_at": "2025-02-04T09:13:05Z", "commit_id": "1abe727322a0298840e231c6af94f1cd0b69a724", "labels": [], "url": "https://github.com/microsoft/mattergen/issues/41", "body": "Hello and congratulations on the Nature publication!\n\nI am attempting to follow the README for getting started with mattergen and keep receiving a Runtime Error. \n\nMy steps to reproduce:\n\n`export MODEL_NAME=checkpoints/mattergen_base`\n`export RESULTS_PATH=results/`\n`python generate.py $RESULTS_PATH $MODEL_NAME --batch_size=4 --num_batches 1`\n\nAs an aside the 'mattergen-generate' command was not recognized, which is why I called python and generate.py\n\nThe error traceback:\n\n`INFO:mattergen.common.utils.eval_utils:Loading model from checkpoint: /home/krkaufma/PycharmProjects/mattergen_proj/checkpoints/mattergen_base/checkpoints/last.ckpt\n/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/common/utils/data_classes.py:95: UserWarning: \nThe version_base parameter is not specified.\nPlease specify a compatability version level, or None.\nWill assume defaults for version 1.1\n  with initialize_config_dir(str(self.model_path)):\n  0%|                                                                                                                                                                                     | 0/1000 [00:00<?, ?it/s]\nGenerating samples:   0%|                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/generate.py\", line 84, in <module>\n    fire.Fire(main)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/generate.py\", line 79, in main\n    generator.generate(output_dir=Path(output_path))\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/generator.py\", line 370, in generate\n    generated_structures = draw_samples_from_sampler(\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/generator.py\", line 58, in draw_samples_from_sampler\n    sample, mean, intermediate_samples = sampler.sample_with_record(conditioning_data, mask)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/diffusion/sampling/pc_sampler.py\", line 130, in sample_with_record\n    return self._sample_maybe_record(conditioning_data, mask=mask, record=True)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/diffusion/sampling/pc_sampler.py\", line 157, in _sample_maybe_record\n    return self._denoise(batch=batch, mask=mask, record=record)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/diffusion/sampling/pc_sampler.py\", line 187, in _denoise\n    score = self._score_fn(batch, t)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/diffusion/sampling/classifier_free_guidance.py\", line 71, in _score_fn\n    return get_unconditional_score()\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/diffusion/sampling/classifier_free_guidance.py\", line 59, in get_unconditional_score\n    return super(GuidedPredictorCorrector, self)._score_fn(\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/diffusion/sampling/pc_sampler.py\", line 94, in _score_fn\n    return self._diffusion_module.score_fn(x, t)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/diffusion/diffusion_module.py\", line 129, in score_fn\n    model_out: T = self.model(x, t)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/denoiser.py\", line 248, in forward\n    output = self.gemnet(\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/common/gemnet/gemnet.py\", line 665, in forward\n    ) = self.generate_interaction_graph(\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/common/gemnet/gemnet.py\", line 535, in generate_interaction_graph\n    edge_index, to_jimages, num_bonds = radius_graph_pbc(\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/common/utils/data_utils.py\", line 263, in radius_graph_pbc\n    edge_index, unit_cell, num_neighbors_image, _, _ = radius_graph_pbc_ocp(\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/common/utils/ocp_graph_utils.py\", line 229, in radius_graph_pbc\n    mask_num_neighbors, num_neighbors_image = get_max_neighbors_mask(\n  File \"/home/krkaufma/PycharmProjects/mattergen_proj/mattergen/common/utils/ocp_graph_utils.py\", line 280, in get_max_neighbors_mask\n    num_neighbors = segment_coo(ones.to(pyg_device), index.to(pyg_device), dim_size=num_atoms).to(\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch_scatter/segment_coo.py\", line 124, in segment_coo\n    return segment_sum_coo(src, index, out, dim_size)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch_scatter/segment_coo.py\", line 9, in segment_sum_coo\n    return torch.ops.torch_scatter.segment_sum_coo(src, index, out, dim_size)\n  File \"/home/krkaufma/anaconda3/envs/mattergen25/lib/python3.10/site-packages/torch/_ops.py\", line 755, in __call__\n    return self._op(*args, **(kwargs or {}))\nRuntimeError: Unknown layout\n`\nI have already tried modifying gcc, nvcc, and $PATH to no avail.\n\nThank you in advance for your assistance.", "comments_url": "https://api.github.com/repos/microsoft/mattergen/issues/41/comments", "author": "krkaufma", "comments": [{"user": "ClaudioZeni", "created_at": "2025-02-03T13:08:00Z", "body": "Hi and thanks for reaching out.\n\nCould you try pulling the latest commits, re-installing the environment and re-run the script?\nAlso, which architecture are you on?"}, {"user": "krkaufma", "created_at": "2025-02-03T22:22:09Z", "body": "Hi @ClaudioZeni and thanks for the reply. I pulled the latest version, re-installed the environment, and re-ran the script and everything worked. I am on Ubuntu 18.04 with x86_64 architecture. Let me know if you need further information about my architecture. \n\nIf you don't mind me asking this question here, do the mattersim relaxed structures and predicted propert(ies) get written anywhere in the file system? Is there an argument to have this done when calling the evaluation?"}, {"user": "ClaudioZeni", "created_at": "2025-02-04T09:12:28Z", "body": "Hi, glad everything works now.\n\nAs for the relaxation, currently `evaluate.py` does not store any information regarding the relaxed structures.\nIf you are interested in these info, you can simply run the relaxation script and then save the properties you are interested in:\n\n``` python\n\nfrom mattergen.evaluation.utils.relaxation import relax_structures\n\nrelaxed_structures, total_energies = relax_structures(structures)\n```"}, {"user": "ClaudioZeni", "created_at": "2025-02-04T09:13:05Z", "body": "Closing as issue appears to be resolved"}], "satisfaction_conditions": ["A solution that resolves the 'Unknown layout' runtime error when running the mattergen generation script", "Information about how to access or save relaxed structures and their properties"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:01:15"}, "dockerfile": "FROM nvidia/cuda:11.8.0-devel-ubuntu22.04\n\n# Set non-interactive mode for apt-get\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    git-lfs \\\n    python3.10 \\\n    python3.10-venv \\\n    python3-pip \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Initialize git-lfs\nRUN git lfs install\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/microsoft/mattergen.git . && \\\n    git checkout 1abe727322a0298840e231c6af94f1cd0b69a724\n\n# Set up Python environment using uv\nRUN pip install uv && \\\n    uv venv .venv --python 3.10 && \\\n    . .venv/bin/activate && \\\n    uv pip install -e .\n\n# Pull Git LFS files (model checkpoints) with increased timeout\nRUN git lfs pull || echo \"Git LFS pull failed, continuing anyway\"\n\n# Make sure the model directory structure exists\nRUN mkdir -p checkpoints/mattergen_base/checkpoints\n\n# Set environment variable for PyTorch\nENV PYTORCH_ENABLE_MPS_FALLBACK=1\n\n# Set PATH to include the virtual environment\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Default command to activate the virtual environment\nCMD [\"/bin/bash\"]", "language": "python"}
{"number": 111, "title": "having trouble running CSP version", "created_at": "2025-03-25T19:53:50Z", "closed_at": "2025-03-27T09:28:03Z", "commit_id": "6abb3842858083c1bf106d15328ed4d7059b9314", "labels": [], "url": "https://github.com/microsoft/mattergen/issues/111", "body": "Hi! We retrained the model with the CSP settings, using MP-20 to start. It seems to have finished training successfully. But we can't figure out how to run the generate commands, though. We tried this:\n`mattergen-generate $RESULTS_PATH --model_path=$MODEL_PATH --sampling-config-name=csp --target_compositions=[{\"Na\": 1, \"Cl\": 1}] --batch_size=16 --num_batches ` \nwith `$MODEL_PATH` set to `outputs/singlerun/2025-03-19/12-11-18`\n\nthe checkpoint files are in there, but what I get is:\n```\n(errors)\n...\nFile \"/home/fas/MATTERGEN/mattergen/mattergen/scripts/generate.py\", line 55, in main\n    pretrained_name is None or model_path is None\n```\nI tried using the path to the checkpoint file itself, which didn't help. Any tips?", "comments_url": "https://api.github.com/repos/microsoft/mattergen/issues/111/comments", "author": "asedova", "comments": [{"user": "danielzuegner", "created_at": "2025-03-26T08:45:38Z", "body": "Hi @asedova,\n\nIt appears that for some reason both `pretrained_name` and `model_path` are non-`None` in your run. Can you add a print statement right before the assertion error so we can see what's going wrong? It also looks like you're not providing a number for `--num_batches` in your CLI command."}, {"user": "asedova", "created_at": "2025-03-26T13:32:20Z", "body": "Sorry,`num_batches` was set to 1, it just got cut off above in the copy-paste.\n\nHere is the full error:\n```\nmattergen-generate $RESULTS_PATH --model_path=$MODEL_PATH --target_compositions=[{\"Na\": 1, \"Cl\": 1}] --batch_size=16 --num_batches 1\nMODELS_PROJECT_ROOT: /home/fas/MATTERGEN/mattergen/mattergen\nTraceback (most recent call last):\n  File \"/home/fas/MATTERGEN/.venv/bin/mattergen-generate\", line 10, in <module>\n    sys.exit(_main())\n  File \"/home/fas/MATTERGEN/mattergen/mattergen/scripts/generate.py\", line 102, in _main\n    fire.Fire(main)\n  File \"/home/fas/MATTERGEN/.venv/lib/python3.10/site-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"/home/fas/MATTERGEN/.venv/lib/python3.10/site-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File \"/home/fas/MATTERGEN/.venv/lib/python3.10/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"/home/fas/MATTERGEN/mattergen/mattergen/scripts/generate.py\", line 55, in main\n    pretrained_name is None or model_path is None\nAssertionError: Only one of pretrained_name or model_path can be provided.\n(.venv) 1 fas@milan2:~/MATTERGEN/mattergen$ export MODEL_PATH=../Mark-mattergen/mattergen/outputs/singlerun/2025-03-19/12-11-18/checkpoints/epoch=899-step=48600.ckpt\n(.venv) fas@milan2:~/MATTERGEN/mattergen$ mattergen-generate $RESULTS_PATH --model_path=$MODEL_PATH --target_compositions=[{\"Na\": 1, \"Cl\": 1}] --batch_size=16 --num_batches 1\nMODELS_PROJECT_ROOT: /home/fas/MATTERGEN/mattergen/mattergen\nTraceback (most recent call last):\n  File \"/home/fas/MATTERGEN/.venv/bin/mattergen-generate\", line 10, in <module>\n    sys.exit(_main())\n  File \"/home/fas/MATTERGEN/mattergen/mattergen/scripts/generate.py\", line 102, in _main\n    fire.Fire(main)\n  File \"/home/fas/MATTERGEN/.venv/lib/python3.10/site-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"/home/fas/MATTERGEN/.venv/lib/python3.10/site-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File \"/home/fas/MATTERGEN/.venv/lib/python3.10/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"/home/fas/MATTERGEN/mattergen/mattergen/scripts/generate.py\", line 55, in main\n    pretrained_name is None or model_path is None\nAssertionError: Only one of pretrained_name or model_path can be provided.\n```\n\nI'll work on that print statement in a bit and report back."}, {"user": "asedova", "created_at": "2025-03-26T15:04:01Z", "body": "Hey, here is the result after the print statement. The `model_path` variable is nonempty and correct:\n```\n(.venv) fas@milan2:~/MATTERGEN/mattergen$ mattergen-generate $RESULTS_PATH --model_path=$MODEL_PATH --target_compositions=[{\"Na\": 1, \"Cl\": 1}] --batch_size=16 --num_batches 1\nMODELS_PROJECT_ROOT: /home/fas/MATTERGEN/mattergen/mattergen\npretrained_name: (1,), model_path: ../Mark-mattergen/mattergen/outputs/singlerun/2025-03-19/12-11-18/\n...\n```\nI get the same final error. What is strange, is that `pretrained_name` is also NOT EMPTY! Also, in the assert, should it be an XOR?\n\n"}, {"user": "danielzuegner", "created_at": "2025-03-26T16:32:47Z", "body": "Hi @asedova, can you also show the result of printing `target_compositions` in the code?"}, {"user": "danielzuegner", "created_at": "2025-03-26T16:54:44Z", "body": "Ok, I think I figured it out. Can you try adding single quotes around the dictionary in the condition? I.e., `--target_compositions=['{\"Na\": 1, \"Cl\": 1}']`. Also, you have to pass `--sampling-config-name=csp` in order to use CSP sampling. Once you confirm this works I'll update the instructions in the README."}, {"user": "asedova", "created_at": "2025-03-26T19:03:49Z", "body": "Ok, that seems to have helped the previous error! By the way I did try adding the `--sampling-config-name=csp` flag yesterday also, and got that same error above. \n\nI now get an error about the model not being trained for csp... looks like after all our debugging of the training we left off the csp flag, so I will have to retrain it again and get back to you about next steps!"}], "satisfaction_conditions": ["Correct command syntax for running the CSP version of the model", "Clear explanation of parameter conflicts in the command", "Proper formatting of JSON-like parameters in command line arguments", "Guidance on required flags for CSP model execution"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:00:41"}, "dockerfile": "FROM nvidia/cuda:11.8.0-devel-ubuntu22.04\n\n# Set non-interactive mode for apt-get\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    git-lfs \\\n    python3.10 \\\n    python3.10-venv \\\n    python3-pip \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Initialize git-lfs\nRUN git lfs install\n\n# Create a working directory\nWORKDIR /app\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/microsoft/mattergen.git . && \\\n    git checkout 6abb3842858083c1bf106d15328ed4d7059b9314\n\n# Set up Python environment using uv\nRUN pip install uv && \\\n    uv venv .venv --python 3.10 && \\\n    . .venv/bin/activate && \\\n    uv pip install -e .\n\n# Set environment variable for PyTorch MPS fallback (useful for Apple Silicon)\nENV PYTORCH_ENABLE_MPS_FALLBACK=1\n\n# Set PATH to include the virtual environment\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Pull Git LFS files (model checkpoints) with increased timeout\nRUN git lfs pull || echo \"Git LFS pull failed, continuing anyway\"\n\n# Make the model directory structure if it doesn't exist\nRUN mkdir -p checkpoints/mattergen_base/checkpoints\n\n# Default command to activate the virtual environment\nCMD [\"/bin/bash\"]", "language": "python"}
{"number": 88, "title": "How to generate stuctures efficiently...?", "created_at": "2025-03-05T12:58:38Z", "closed_at": "2025-03-07T10:59:44Z", "commit_id": "e7c9ce2700d0163863fd6db8625bf44c64422de5", "labels": [], "url": "https://github.com/microsoft/mattergen/issues/88", "body": "Hi @ClaudioZeni \nWhen I am trying to generate more structure by increasing number of batch it is throughing out of memory (OOM) in between, and the code is saving generated structures after processing all batch so this leads to waste of gpu time without any structures. How to generate suppose 50k structures efficently? If I run the same script with less number of batchs then each time its replacing the previously generated structures. \nCould you please suggest how to handle the generation more efficiently?", "comments_url": "https://api.github.com/repos/microsoft/mattergen/issues/88/comments", "author": "chiku-parida", "comments": [{"user": "danielzuegner", "created_at": "2025-03-07T10:59:44Z", "body": "Hi @chiku-parida,\n\nhave you tried passing `--record-trajectories=False` to the generation command? This will reduce the amount of processing after each batch as it does not store the full trajectories. If you want to go with the route of running the same script with fewer batches, I recommend you provide a different `--output-path` to each individual run to avoid overwriting the results."}, {"user": "chiku-parida", "created_at": "2025-03-07T11:43:07Z", "body": "Thanks @danielzuegner \nIf I generate 50 k with 10 independent runs generating 5k structures in each run then the model might generate similar structures, right?"}, {"user": "danielzuegner", "created_at": "2025-03-07T13:44:41Z", "body": "Hi @chiku-parida,\n\nstructures are always generated independently, no matter how you slice the generation."}], "satisfaction_conditions": ["A method to generate large numbers of structures without running out of memory", "A way to preserve previously generated structures rather than overwriting them", "Guidance on how to split large generation tasks into smaller batches while maintaining output quality", "Clarification on whether multiple independent generation runs would produce duplicate structures"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:50"}, "dockerfile": null, "language": "python"}
{"number": 51, "title": "Enquiry", "created_at": "2025-02-07T05:19:24Z", "closed_at": "2025-02-08T14:47:03Z", "commit_id": "74b9fcab2b5140b4e41001ba800c1c2170bafe8d", "labels": [], "url": "https://github.com/microsoft/mattergen/issues/51", "body": "Dear manager\n    When i want to run the code in my laptop,i encounter a problem.\nThe command in your  Train MatterGen yourself chapter      csv-to-dataset  ,does it need any packages,\nwhich package should i install to run it? python 3.12  and python 3.10 ,can not recognize this command ,\n    looking forward to your reply!\nThank you !", "comments_url": "https://api.github.com/repos/microsoft/mattergen/issues/51/comments", "author": "nelence2k", "comments": [{"user": "401-Nick", "created_at": "2025-02-08T08:39:17Z", "body": "Did you use this? \n```\npip install uv\nuv venv .venv --python 3.10 \nsource .venv/bin/activate\nuv pip install -e .\n```"}, {"user": "nelence2k", "created_at": "2025-02-08T14:46:55Z", "body": "ok, i found the problem. My net work sometimes is very bad.Thank you my friend."}], "satisfaction_conditions": ["Information about package dependencies or installation requirements for the csv-to-dataset command", "A solution that addresses Python version compatibility issues", "Guidance on proper environment setup for running the code", "Troubleshooting assistance for command recognition errors"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:54"}, "dockerfile": null, "language": "python"}
{"number": 50, "title": "Error on torch distributor on cpu device", "created_at": "2025-02-05T19:31:01Z", "closed_at": "2025-02-06T18:07:03Z", "commit_id": "74b9fcab2b5140b4e41001ba800c1c2170bafe8d", "labels": [], "url": "https://github.com/microsoft/mattergen/issues/50", "body": "I am training the mattergen to generate structures using the mp_20 dataset. The csv_to_dataset step worked without issues. However I had issues with the running the line: python scripts/run.py data_module=mp_20 ~trainer.logger\nI changed the \"accelerator: 'cpu'\" in mattergen/conf/trainer/default.yaml file because I was running on a cpu node with 48 cores and was using all the 48 cores. This is what I got as output:\n  ret = run_job(\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\nat the end this is what I saw:\n\"torch.distributed.DistStoreError: Timed out after 1801 seconds waiting for clients. 1/48 clients joined.\"\n\ndo you know the source of this error?\ndo I need to change anything else in \"trainer\" \"conf\" .yaml file or any other .yaml file in \"conf\"?\n", "comments_url": "https://api.github.com/repos/microsoft/mattergen/issues/50/comments", "author": "Mofahdi", "comments": [{"user": "danielzuegner", "created_at": "2025-02-06T07:20:50Z", "body": "Hi @Mofahdi,\n\ncan you also remove the `strategy` dict from `conf/trainer/default.yml` (or alternatively add `~trainer.strategy` to your command)?\n\nLet me know if that helped."}, {"user": "Mofahdi", "created_at": "2025-02-06T18:06:59Z", "body": "Thanks so much! the package runs fine now!\nJust want to let you know and other users who encounter the same issue that both methods work: 1- commenting the \"strategy dict\" lines from conf/trainer/default.yml or 2- add \"~trainer.strategy\" to the command"}, {"user": "nelence2k", "created_at": "2025-02-07T05:30:41Z", "body": "@Mofahdi  How to make  the csv-to-dataset  command work ? Do I need to install some packages? "}, {"user": "Mofahdi", "created_at": "2025-02-07T05:40:31Z", "body": "@nelence2k no, after you run the git ... command then the unzip .... command. you have to run `python scripts/csv_to_dataset.py --csv-folder datasets/alex_mp_20/ --dataset-name alex_mp_20 --cache-folder datasets/cache\n` \nI noticed that the developers changed the README file"}], "satisfaction_conditions": ["A solution that resolves the torch.distributed timeout error when running on CPU", "Clear instructions on configuration changes needed for CPU-only execution", "Options for solving the problem rather than a single fixed approach"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:01"}, "dockerfile": null, "language": "python"}
{"number": 43, "title": "Evaluations on individual compounds", "created_at": "2025-02-02T14:55:05Z", "closed_at": "2025-02-03T09:42:54Z", "commit_id": "1abe727322a0298840e231c6af94f1cd0b69a724", "labels": [], "url": "https://github.com/microsoft/mattergen/issues/43", "body": "Is it possible to run evaluations on individual compounds (cif files)?\n\nI'm working on a ex: (15 .cif files in zip file => 1 .cif file in 15 zip files => evaluate.py for each .zip) but it feels very inefficient especially with a long startup time for each eval.\n\nPossibly exploring using MatterSim to directly generate from the CIF but curious if there's any possible insight or plans into this", "comments_url": "https://api.github.com/repos/microsoft/mattergen/issues/43/comments", "author": "401-Nick", "comments": [{"user": "ClaudioZeni", "created_at": "2025-02-03T09:42:07Z", "body": "Hi Nick, thanks for reaching out.\n\nFrom reading your script it seems to me that what you are really interested in is not the metric evaluation, but an energy computation using MatterSim.\n\nIf this is the case, I would advise directly using the MatterSim functions to compute the properties of interest on a batch of structures, and then saving them individually. For example, assuming you have a list of `Structure` objects inside `structures`:\n\n``` python\n\nfrom mattersim.datasets.utils.build import build_dataloader\nfrom mattersim.forcefield.potential import Potential\nfrom pymatgen.io.ase import AseAtomsAdaptor\nimport json\n\n\npotential = Potential.from_checkpoint(\n    device=\"cuda\", load_training_state=False\n)\natoms = [AseAtomsAdaptor.get_atoms(s) for s in structures]\ndataloader = build_dataloader(\n    atoms, batch_size=len(atoms), only_inference=True\n)\nenergy_batch, forces_batch, stress_batch = potential.predict_properties(\n    dataloader, include_forces=True, include_stresses=True\n)\n\nfor i in range(len(structures)):\n    structure_with_metadata = {}\n    structure_with_metadata['structure'] = structures[i].as_dict()\n    structure_with_metadata['energy'] = energy_batch[i]\n    structure_with_metadata['forces'] = forces_batch[i].tolist()\n    structure_with_metadata['stress'] = stress_batch[i].tolist()\n    with open(f'structures_with_metadata_{i}.json', 'w') as f:\n        json.dump(structure_with_metadata, f, indent=4)\n```"}, {"user": "401-Nick", "created_at": "2025-02-03T10:39:58Z", "body": "I had only searched in the readme and official docs and didn't see that but I found it in the user_guide. Thank you, I appreciate it! Literally about 1000x faster.\n\n```\nimport os\nimport glob\nimport shutil\nimport zipfile\nimport tempfile\nimport torch\nimport json\nimport numpy as np\n\nfrom ase import io\nfrom ase.units import GPa\nfrom mattersim.forcefield.potential import Potential\nfrom mattersim.datasets.utils.build import build_dataloader\nfrom CifFile import ReadCif\n\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return super().default(obj)\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running MatterSim on device: {device}\")\n\nzip_file_path = \"results/generated_crystals_cif.zip\"\njson_file_path = \"results/report.json\"\n\ntemp_dir = tempfile.mkdtemp()\n\nwith zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n    zip_ref.extractall(temp_dir)\n\nextracted_tmp_dir = os.path.join(temp_dir, \"tmp\")\nif not os.path.exists(extracted_tmp_dir):\n    shutil.rmtree(temp_dir)\n    raise SystemExit(\"Error: 'tmp/' directory not found inside the ZIP file. Exiting.\")\n\ncif_files = glob.glob(os.path.join(extracted_tmp_dir, \"*.cif\"))\nif not cif_files:\n    shutil.rmtree(temp_dir)\n    raise SystemExit(\"No CIF files found in 'tmp/'! Exiting.\")\n\nreport_data = []\natoms_list = []\nvalid_report_indexes = []\n\nfor cif_file in cif_files:\n    file_report = {\n        \"cif_file\": cif_file,\n        \"cif_metadata\": {},\n        \"atom_sites\": [],\n        \"matter_sim_results\": {},\n    }\n    try:\n        cif_content = ReadCif(cif_file)\n        block = cif_content[list(cif_content.keys())[0]]\n    except Exception as e:\n        file_report[\"error\"] = f\"Failed to parse CIF data from {cif_file}: {e}\"\n        report_data.append(file_report)\n        continue\n\n    fields_to_extract = [\n        \"_chemical_formula_structural\",\n        \"_chemical_formula_sum\",\n        \"_cell_length_a\",\n        \"_cell_length_b\",\n        \"_cell_length_c\",\n        \"_cell_angle_alpha\",\n        \"_cell_angle_beta\",\n        \"_cell_angle_gamma\",\n        \"_space_group_name_H-M_alt\",\n        \"_space_group_IT_number\",\n    ]\n    for field in fields_to_extract:\n        file_report[\"cif_metadata\"][field] = block.get(field, \"N/A\")\n\n    symops_data = []\n    try:\n        symops_loop = block.GetLoop(\"_space_group_symop_operation_xyz\")\n        if symops_loop:\n            for row in symops_loop:\n                symops_data.append(row)\n    except Exception:\n        pass\n    file_report[\"cif_metadata\"][\"space_group_symops\"] = symops_data\n\n    atom_sites_list = []\n    atom_loop_key = None\n    for loop_key, columns in block.loops.items():\n        if any(\"_atom_site_\" in col for col in columns):\n            atom_loop_key = loop_key\n            break\n\n    if atom_loop_key is not None:\n        columns = block.loops[atom_loop_key]\n        row_count = len(block[columns[0]])\n        for i in range(row_count):\n            row_data = {col: block[col][i] for col in columns}\n            atom_sites_list.append(row_data)\n    file_report[\"atom_sites\"] = atom_sites_list\n\n    try:\n        atoms = io.read(cif_file)\n        atoms_list.append(atoms)\n        valid_report_indexes.append(len(report_data))\n    except Exception as e:\n        file_report[\"matter_sim_results\"] = {\n            \"error\": f\"Failed to read ASE atoms from {cif_file}: {e}\"\n        }\n\n    report_data.append(file_report)\n\nif atoms_list:\n    try:\n        print(\"Loading MatterSim potential from checkpoint...\")\n        potential = Potential.from_checkpoint(device=device)\n        print(\"Building dataloader for MatterSim predictions...\")\n        dataloader = build_dataloader(atoms_list, only_inference=True)\n        print(\"Running MatterSim predictions in batch mode...\")\n        energies, forces_list, stresses_list = potential.predict_properties(\n            dataloader, include_forces=True, include_stresses=True\n        )\n        for idx, atom_obj in enumerate(atoms_list):\n            report_idx = valid_report_indexes[idx]\n            n_atoms = len(atom_obj)\n            energy = energies[idx]\n            energy_per_atom = energy / n_atoms if n_atoms > 0 else None\n            forces = forces_list[idx]\n            forces_first_atom = forces[0] if len(forces) > 0 else None\n            stress = stresses_list[idx]\n            try:\n                stress_00 = stress[0][0]\n            except Exception:\n                stress_00 = None\n\n            report_data[report_idx][\"matter_sim_results\"] = {\n                \"energy_eV\": energy,\n                \"energy_per_atom_eV\": energy_per_atom,\n                \"forces_first_atom_eVA\": forces_first_atom,\n                \"stress_00_eVA3\": stress_00,\n                \"stress_00_GPa\": stress_00 / GPa if stress_00 is not None else None,\n                \"forces_eVA\": forces,\n                \"stress_tensor\": stress,\n            }\n        print(\"MatterSim predictions successfully added to the report.\")\n    except Exception as e:\n        print(f\"Error during MatterSim batch prediction: {e}\")\n        for idx in valid_report_indexes:\n            report_data[idx][\"matter_sim_results\"] = {\"error\": f\"Batch prediction failed: {e}\"}\nelse:\n    print(\"No valid ASE Atoms objects were read; skipping MatterSim predictions.\")\n\nshutil.rmtree(temp_dir)\nprint(\"Cleaned up temporary files.\")\n\nwith open(json_file_path, \"w\") as jf:\n    json.dump(report_data, jf, cls=NumpyEncoder, indent=2)\nprint(f\"Report written to {json_file_path}\")\n```"}], "satisfaction_conditions": ["A method to efficiently evaluate multiple individual CIF files without restarting for each file", "A batch processing approach using MatterSim that works directly with multiple structures", "A solution that avoids the overhead of creating separate zip files for each structure"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:07"}, "dockerfile": null, "language": "python"}
{"number": 35, "title": "no kokoro-onnx version 0.3.9 found by pip", "created_at": "2025-03-09T22:09:15Z", "closed_at": "2025-03-10T04:33:17Z", "commit_id": "e629e799c66c530e6cfe36f1fad82d6eaeadfd4f", "labels": ["need more info"], "url": "https://github.com/nazdridoy/kokoro-tts/issues/35", "body": "The below error is popping when running pip install -r requirements.txt\nI cant seem to find a version 0.3.9. Is this issue on my end or due to some update?\n\n\"ERROR: Could not find a version that satisfies the requirement kokoro-onnx==0.3.9 (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.4.4, 0.4.5)\nERROR: No matching distribution found for kokoro-onnx==0.3.9\"", "comments_url": "https://api.github.com/repos/nazdridoy/kokoro-tts/issues/35/comments", "author": "CrimsonWillow", "comments": [{"user": "nazdridoy", "created_at": "2025-03-10T02:58:59Z", "body": "can't reproduce,\n\nyou must have supported python environment\n>requires-python = \">=3.9, <3.13\""}, {"user": "CrimsonWillow", "created_at": "2025-03-10T04:33:52Z", "body": "Ah, don't know how I missed this. Issue solved. Thanks "}, {"user": "nazdridoy", "created_at": "2025-03-10T05:35:43Z", "body": "you are welcome"}], "satisfaction_conditions": ["Explanation of why the package version cannot be found", "Information about environment requirements for the package"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:18:25"}, "dockerfile": null, "language": "python"}
{"number": 172, "title": "Error fetching Jira projects caused by character '@'", "created_at": "2025-03-31T05:17:25Z", "closed_at": "2025-04-02T15:55:18Z", "commit_id": "08e5fa25883ff5c70ca3c3b2d738d0b208378ee7", "labels": [], "url": "https://github.com/sooperset/mcp-atlassian/issues/172", "body": "I am encountering the following errors when I use the latest version of mcp-atlassian.\n\nInstall:\nbrew install uv\nuvx mcp-atlassian\n\nUsage:\nTransport Type: STDIO\nCommand: uvx\nArguments: mcp-atlassian --confluence-url my_url --confluence-personal-token my_token_1 --jira-url my_url --jira-personal-token my_token_2\n\nError output from MCP server when connecting:\n`ERROR - mcp-atlassian - Error fetching Jira projects: Error in the JQL Query: The character '@' is a reserved JQL character. You must enclose it in a string or use the escape '\\u0040' instead. (line 1, character 17)`\n\nSeems main features are still correct, I can invoke the tools in windsurf. please help to address the above error.", "comments_url": "https://api.github.com/repos/sooperset/mcp-atlassian/issues/172/comments", "author": "langpingxue", "comments": [{"user": "sooperset", "created_at": "2025-04-01T18:00:20Z", "body": "Thanks for reporting this issue. The error occurs because the '@' character in email addresses isn't properly quoted in JQL queries. I've identified the root cause in how user identifiers are inserted into queries and will submit a fix shortly. Glad to hear main functionality still works despite this error."}, {"user": "sooperset", "created_at": "2025-04-02T15:55:55Z", "body": "Hi, I fixed this issue with #180. would you try testing with the current main branch?"}, {"user": "langpingxue", "created_at": "2025-04-03T08:57:47Z", "body": "looks good, thank you."}], "satisfaction_conditions": ["A fix for the JQL query error caused by the '@' character", "Maintaining functionality of the main features while fixing the error", "A solution that works with the user's existing setup (uvx/brew installation method)"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:23"}, "dockerfile": null, "language": "python"}
{"number": 167, "title": "Support for legacy confluence (6.0.x)", "created_at": "2025-03-28T11:09:07Z", "closed_at": "2025-04-02T15:17:09Z", "commit_id": "08e5fa25883ff5c70ca3c3b2d738d0b208378ee7", "labels": [], "url": "https://github.com/sooperset/mcp-atlassian/issues/167", "body": "I'm wondering what would be missing to have support for the confluence series 6.0.\n\nI've tried the confluence client lib ( `atlassian-python-api` ) against such a legacy server and it seems to work just fine, at least for the basic features.\n\nLooking at the mcp-atlassian code, I couldn't find the reason why it doesn't seem to work with confluence 6.0.x . \n\nAre there any useful pointers about how to start diagnosing this issue?\n\nI'd be keen on building support for the 6.0.x confluence branch, even if it has to be limited.\n\n ", "comments_url": "https://api.github.com/repos/sooperset/mcp-atlassian/issues/167/comments", "author": "jeteve", "comments": [{"user": "sooperset", "created_at": "2025-03-29T05:04:22Z", "body": "That sounds great! The supported Confluence version was set in PR #92. For Jira DC/server, the supported version was set due to the PAT support. It would be great if we could support the legacy version seamlessly, if possible."}, {"user": "jeteve", "created_at": "2025-03-31T08:08:29Z", "body": "So, I ran some test using the test suite and they pass just fine for my legacy confluence:\n\n```\n\npytest -vx tests/test_real_api_validation.py --use-real-data\ntests/test_real_api_validation.py::TestRealConfluenceValidation::test_get_page_content PASSED                             [ 11%]\ntests/test_real_api_validation.py::TestRealConfluenceValidation::test_get_page_comments PASSED                            [ 14%]\ntests/test_real_api_validation.py::TestRealConfluenceValidation::test_search_content PASSED                               [ 17%]\ntests/test_real_api_validation.py::test_confluence_get_page_content[asyncio] PASSED                                       [ 29%]\n\n```\n\nBUT, when I run the MCP in claude with exactly the same environment variables, it just doesn't work and I can see anything significantly interesting in the MCP logs. Maybe it;s some sort of windows thing. MCP with Claude works perfectly with my cloud JIRA."}, {"user": "jeteve", "created_at": "2025-03-31T08:57:07Z", "body": "PR #173 makes this work in the MCP server itself."}, {"user": "sooperset", "created_at": "2025-04-01T17:52:41Z", "body": "Thank you for your contribution. I've just reviewed the PR and we're ready to proceed with a few updates. Once those are implemented, we can move forward with the merge."}, {"user": "jeteve", "created_at": "2025-04-02T15:17:09Z", "body": "Great! Yes, using basic auth, it works fine against an old confluence. (6.0.x line). Thanks a lot @sooperset !"}], "satisfaction_conditions": ["Support for Confluence 6.0.x series in the library", "Identification of what was preventing compatibility with Confluence 6.0.x", "A solution that works with the MCP server", "Authentication method that works with legacy Confluence"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:29"}, "dockerfile": null, "language": "python"}
{"number": 141, "title": "jira_search fields not working", "created_at": "2025-03-23T13:19:33Z", "closed_at": "2025-03-23T14:36:52Z", "commit_id": "99ca0b9c9fa062c6c938cd9700dfffcb187d2ece", "labels": [], "url": "https://github.com/sooperset/mcp-atlassian/issues/141", "body": "\nI expected  fields of `jira search` arguments are Fields to return.  but it doesnt work.\n\n- actual arguments\n```json\n{\n  \"jql\": \"project = PROJECT AND cf[10049] IS NOT EMPTY ORDER BY cf[10049] DESC\",\n  \"fields\": \"summary,assignee,customfield_10049\",\n  \"limit\": 100\n}\n\n```\n\nactual result has not customfield_10049. also result has other field. \n\n```\n    \"id\": \n    \"summary\": \n    \"key\": \n    \"description\": null,\n    \"created\": \"\",\n    \"updated\": \"\",\n    \"status\": null,\n    \"issue_type\": null,\n    \"priority\": null,\n    \"assignee\": {\n      \"display_name\": \n      \"name\": \n      \"email\": \n      \"avatar_url\": \"\n      ...\n```", "comments_url": "https://api.github.com/repos/sooperset/mcp-atlassian/issues/141/comments", "author": "youngkyo0504", "comments": [{"user": "sooperset", "created_at": "2025-03-23T14:37:00Z", "body": "Hi @youngkyo0504 \nThank you for reporting this issue. I've implemented a fix #142  that addresses both problems you identified:\n\n1. The `jira_search` fields parameter now properly filters which fields are returned in the response\n2. Custom fields are now included when requested\n\nThis has been merged and will be part of the next release. Please let us know if you encounter any other issues."}, {"user": "youngkyo0504", "created_at": "2025-03-23T14:50:59Z", "body": "Thank you. You're incredibly fast"}], "satisfaction_conditions": ["Fix for the fields parameter in jira_search to properly filter returned fields", "Support for custom fields in the response when requested", "Timely resolution of the reported issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:36"}, "dockerfile": null, "language": "python"}
{"number": 25, "title": "Incorrect Tool Calls in Multi Agents with Supervisor", "created_at": "2025-02-21T15:02:18Z", "closed_at": "2025-02-26T02:15:13Z", "commit_id": "bc0030b7f8761eef253da57e20340f20254018ab", "labels": [], "url": "https://github.com/langchain-ai/langgraph-supervisor-py/issues/25", "body": "Hello everyone, I'm having trouble getting the MultiAgents to work together correctly with the Langgraph-Supervisor.\n\nWhen I have more than one agent, each with their own tools, it seems that an agent keeps trying to call the tools of other agents several times, until he receives an error saying that that tool is not available and he must try one of the available ones, listing the tools that exist. Only at this moment does the agent make the correct call to the right tool and finally respond correctly.\n\nI don't understand why the agent keeps calling another agent's tools if it was binded with only its own tools. \n\nBelow I put the Debug excerpt that shows this moment of the problem and also my complete source code for the three agents and the supervisor so that you can understand what is happening and can help me solve this problem.\n\nThank you in advance for everyone's help.\n\n```python\nfrom typing import Literal\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# --- Temperature tool ---\n@tool\ndef get_temperature(location: str):\n    \"\"\"Call to get the current temperature on location.\"\"\"\n    if location.lower() in [\"munich\"]:\n        return \"It's 15 degrees Celsius.\"\n    else:\n        return \"It's 32 degrees Celsius.\"\n\n# We'll create a model and bind the tool so the LLM knows it can call `get_temperature`.\ntools = [get_temperature]\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n\n# --- Existing agent workflow definition ---\ndef call_model(state: MessagesState):\n    \"\"\"Call the LLM with the conversation so far.\"\"\"\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    \"\"\"If there's a tool call requested, go to 'tools', else end.\"\"\"\n    messages = state[\"messages\"]\n    print(\"Temperature Agent\")\n    print(messages)\n    last_message = messages[-1]\n    print(last_message.tool_calls)\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ntemperature_workflow = StateGraph(MessagesState)\n\ntool_node = ToolNode(tools)\n\ntemperature_workflow.add_node(\"agent\", call_model)\ntemperature_workflow.add_node(\"tools\", tool_node)\n\ntemperature_workflow.add_edge(START, \"agent\")\ntemperature_workflow.add_conditional_edges(\"agent\", should_continue)\ntemperature_workflow.add_edge(\"tools\", \"agent\")\n\ntemperature_agent_graph =  temperature_workflow.compile(name=\"temperature_agent\")\n\nfrom typing import Literal\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# --- Wind tool ---\n@tool\ndef get_wind(location: str):\n    \"\"\"Call to get the current wind on location.\"\"\"\n    if location.lower() in [\"munich\"]:\n        return \"It's High Wind.\"\n    else:\n        return \"It's Low Wind.\"\n\n# We'll create a model and bind the tool so the LLM knows it can call `get_wind`.\ntools = [get_wind]\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n#model = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\").bind_tools(tools)\n# --- Existing agent workflow definition ---\ndef call_model(state: MessagesState):\n    \"\"\"Call the LLM with the conversation so far.\"\"\"\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    \"\"\"If there's a tool call requested, go to 'tools', else end.\"\"\"\n    messages = state[\"messages\"]\n    print(\"Wind Agent\")\n    print(messages)\n    last_message = messages[-1]\n    print(last_message.tool_calls)\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\nwind_workflow = StateGraph(MessagesState)\n\ntool_node = ToolNode(tools)\n\nwind_workflow.add_node(\"agent\", call_model)\nwind_workflow.add_node(\"tools\", tool_node)\n\nwind_workflow.add_edge(START, \"agent\")\nwind_workflow.add_conditional_edges(\"agent\", should_continue)\nwind_workflow.add_edge(\"tools\", \"agent\")\n\nwind_agent_graph =  wind_workflow.compile(name=\"wind_agent\")\n\nfrom typing import Literal\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# --- Rain tool ---\n@tool\ndef get_rain(location: str):\n    \"\"\"Call to get the current rain on location.\"\"\"\n    if location.lower() in [\"munich\"]:\n        return \"It's High rain.\"\n    else:\n        return \"It's Low rain.\"\n\n# We'll create a model and bind the tool so the LLM knows it can call `get_rain`.\ntools = [get_rain]\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)\n#model = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\").bind_tools(tools)\n\n# --- Existing agent workflow definition ---\ndef call_model(state: MessagesState):\n    \"\"\"Call the LLM with the conversation so far.\"\"\"\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    \"\"\"If there's a tool call requested, go to 'tools', else end.\"\"\"\n    messages = state[\"messages\"]\n    print(\"Rain Agent\")\n    print(messages)\n    last_message = messages[-1]\n    print(last_message.tool_calls)\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\nrain_workflow = StateGraph(MessagesState)\n\ntool_node = ToolNode(tools)\n\nrain_workflow.add_node(\"agent\", call_model)\nrain_workflow.add_node(\"tools\", tool_node)\n\nrain_workflow.add_edge(START, \"agent\")\nrain_workflow.add_conditional_edges(\"agent\", should_continue)\nrain_workflow.add_edge(\"tools\", \"agent\")\n\nrain_agent_graph =  rain_workflow.compile(name=\"rain_agent\")\n\nfrom langgraph_supervisor import create_supervisor\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\n\nclass MyCustomState(AgentState):\n    foo: str\n    bar: int\n\nmodel_supervisor = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools)\n#model_supervisor = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\").bind_tools(tools)\n\nsupervisor_workflow = create_supervisor(\n    agents=[temperature_agent_graph, wind_agent_graph, rain_agent_graph],\n    model=model_supervisor,\n    prompt=(\n        \"You are a supervisor managing the following agents: a temperature agent, a wind agent and a rain agent.\"\n        \"You know what each agent does and how they can help you. You can ask them to perform tasks and provide you with the results.\"\n        \"You also understand which agent to call for which task.\"\n        \"When one or more agents were called, You have to use the results from the agents to formulate your answer.\"\n    ),\n    output_mode=\"last_message\",\n    #output_mode=\"full_history\",\n    supervisor_name=\"supervisor_agent\",\n    state_schema=MyCustomState\n    \n)\n\nsupervisor_app = supervisor_workflow.compile()\n\nsupervisor_app.invoke(\n    {\"messages\": [HumanMessage(content=\"How is the wind in France and Munich?\")], \"foo\":\"foo\", \"bar\":1},\n    debug=True\n)\n\n```\n\n```\nWind Agent\n[HumanMessage(content='How is the wind in France and Munich?', additional_kwargs={}, response_metadata={}, id='f2ee6ddd-aaf3-44ee-b489-b2e22e60d8a5'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_NiPkIW0HXh6sNtmKXD1etU0t', 'function': {'arguments': '{}', 'name': 'transfer_to_wind_agent'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 160, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'tool_calls', 'logprobs': None}, name='supervisor_agent', id='run-587735ea-afaa-4a61-8377-29bfb5186b19-0', tool_calls=[{'name': 'transfer_to_wind_agent', 'args': {}, 'id': 'call_NiPkIW0HXh6sNtmKXD1etU0t', 'type': 'tool_call'}], usage_metadata={'input_tokens': 160, 'output_tokens': 14, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to wind_agent', name='transfer_to_wind_agent', id='cd987d67-ce6c-403e-9075-600b47d4a9a7', tool_call_id='call_NiPkIW0HXh6sNtmKXD1etU0t'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_MjKI7LAjJdXpgTjOZH5Mh6Gw', 'function': {'arguments': '{\"location\": \"France\"}', 'name': 'get_temperature'}, 'type': 'function'}, {'id': 'call_gGTMEca1TrrC1OVc8rR11BxE', 'function': {'arguments': '{\"location\": \"Munich\"}', 'name': 'get_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 88, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_00428b782a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e74fe640-0041-41cc-946d-d5f2eea3dcd7-0', tool_calls=[{'name': 'get_temperature', 'args': {'location': 'France'}, 'id': 'call_MjKI7LAjJdXpgTjOZH5Mh6Gw', 'type': 'tool_call'}, {'name': 'get_temperature', 'args': {'location': 'Munich'}, 'id': 'call_gGTMEca1TrrC1OVc8rR11BxE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 88, 'output_tokens': 46, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Error: get_temperature is not a valid tool, try one of [get_wind].', name='get_temperature', id='04ce3a9b-9e04-4663-bff8-65718e5771e5', tool_call_id='call_MjKI7LAjJdXpgTjOZH5Mh6Gw', status='error'), ToolMessage(content='Error: get_temperature is not a valid tool, try one of [get_wind].', name='get_temperature', id='d984472b-9b17-4ad8-bf7e-123552140066', tool_call_id='call_gGTMEca1TrrC1OVc8rR11BxE', status='error'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Dj5BqOlE1Ro968TbFHajNPEH', 'function': {'arguments': '{\"location\": \"France\"}', 'name': 'get_temperature'}, 'type': 'function'}, {'id': 'call_1yYapvolYW6fFrBWbRBoBOhE', 'function': {'arguments': '{\"location\": \"Munich\"}', 'name': 'get_temperature'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 183, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b13305b4-f4d9-48ae-b781-5a12142fb911-0', tool_calls=[{'name': 'get_temperature', 'args': {'location': 'France'}, 'id': 'call_Dj5BqOlE1Ro968TbFHajNPEH', 'type': 'tool_call'}, {'name': 'get_temperature', 'args': {'location': 'Munich'}, 'id': 'call_1yYapvolYW6fFrBWbRBoBOhE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 183, 'output_tokens': 46, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n[{'name': 'get_temperature', 'args': {'location': 'France'}, 'id': 'call_Dj5BqOlE1Ro968TbFHajNPEH', 'type': 'tool_call'}, {'name': 'get_temperature', 'args': {'location': 'Munich'}, 'id': 'call_1yYapvolYW6fFrBWbRBoBOhE', 'type': 'tool_call'}]\n\n```\n\nThis part of debug Messages shows the problem: \n\n*ToolMessage(content='Error: get_temperature is not a valid tool, try one of [get_wind]*\n", "comments_url": "https://api.github.com/repos/langchain-ai/langgraph-supervisor-py/issues/25/comments", "author": "giu-ferreira-cientista", "comments": [{"user": "XinyueZ", "created_at": "2025-02-25T14:35:02Z", "body": "I just found that this framework never calls my tools, does anyone else have similar problems?"}, {"user": "vbarda", "created_at": "2025-02-26T02:15:13Z", "body": "@giu-ferreira-cientista it's because you're re-defining the model with different bound tools - you have this line for two different agents`model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools)`"}, {"user": "vbarda", "created_at": "2025-02-26T02:15:49Z", "body": "@XinyueZ feel free to open an issue with a concrete reproducible example -- likely it's a problem of the LLM you're using"}, {"user": "giu-ferreira-cientista", "created_at": "2025-02-27T17:05:37Z", "body": "Thank you @vbarda ! My bad. Now it's working!"}], "satisfaction_conditions": ["Explanation of why tools are being incorrectly called across different agents", "Identification of the model binding configuration issue", "A solution that prevents agents from attempting to call tools that belong to other agents"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:15:36"}, "dockerfile": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1 \\\n    PIP_NO_CACHE_DIR=off \\\n    PIP_DISABLE_PIP_VERSION_CHECK=on\n\n# Install git and other dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends git && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/langchain-ai/langgraph-supervisor-py.git . && \\\n    git checkout bc0030b7f8761eef253da57e20340f20254018ab\n\n# Install project dependencies\nRUN pip install --upgrade pip && \\\n    pip install -e . && \\\n    # Install additional dependencies that might be needed for testing\n    pip install langchain-openai langchain-anthropic langchain-core langgraph\n\n# Set the default command\nCMD [\"bash\"]", "language": "python"}
{"number": 94, "title": "Custom Agent state not sharing between supervisor and agents after 0.0.12 release", "created_at": "2025-03-26T08:48:47Z", "closed_at": "2025-03-26T13:28:12Z", "commit_id": "021807a81d36f899dacedb19926353742ee19d87", "labels": [], "url": "https://github.com/langchain-ai/langgraph-supervisor-py/issues/94", "body": "I have a custom state that is shared between the supervisor and its agents:\n```\nclass State(AgentState):\n    context_id: str\n```\n\nI update the `context_id` on the supervisor init, but the `context_id` is not visible in the agent state before running a tool. This worked in version 0.0.11.\n\n\n\n\n", "comments_url": "https://api.github.com/repos/langchain-ai/langgraph-supervisor-py/issues/94/comments", "author": "mikkkeldp", "comments": [{"user": "vbarda", "created_at": "2025-03-26T12:43:31Z", "body": "can you provide full example?"}, {"user": "vbarda", "created_at": "2025-03-26T13:31:20Z", "body": "@mikkkeldp the issue should be fixed in 0.0.13 - thanks for reporting!"}, {"user": "mikkkeldp", "created_at": "2025-03-26T13:56:00Z", "body": "@vbarda Thanks for the quick fix!"}], "satisfaction_conditions": ["A fix for the state sharing issue between supervisor and agents in version 0.0.12", "Timely resolution of the reported regression bug"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:26"}, "dockerfile": null, "language": "python"}
{"number": 1162, "title": "AttributeError: type object 'RtfConverter' has no attribute 'register_converters'", "created_at": "2025-03-28T21:07:31Z", "closed_at": "2025-03-31T14:00:56Z", "commit_id": "9e067c42b647eaf14e842e70e47540b36c0c4a08", "labels": [], "url": "https://github.com/microsoft/markitdown/issues/1162", "body": "I have the following plugin:\n```python\n# src/markdown/_rtf_converter\n\nimport re\nfrom striprtf import striprtf\nfrom typing import BinaryIO, Any\nfrom markitdown import (\n    DocumentConverter, \n    DocumentConverterResult, \n    StreamInfo, \n    MarkItDown\n)\n\n\n# The version of the plugin interface that this plugin uses. \n# The only supported version is 1 for now.\n__plugin_interface_version__ = 1 \n\n\ndef register_converters(markitdown: MarkItDown, **kwargs):\n    \"\"\"\n    Called during construction of MarkItDown instances to register converters provided by plugins.\n    \"\"\"\n\n    # Simply create and attach an RtfConverter instance\n    markitdown.register_converter(RtfConverter())\n\n\n\nclass RtfConverter(DocumentConverter):\n    def __init__(\n        self\n    ):\n        super().__init__()\n\n    def accepts(\n        self,\n        file_stream: BinaryIO,\n        stream_info: StreamInfo,\n        **kwargs: Any,\n    ) -> bool:\n        \"\"\"\n        Check if the file is an RTF document.\n        RTF files typically start with \"{\\rtf1\" signature.\n        \"\"\"\n        import pdb; pdb.set_trace() \n        # Save the current position\n        current_position = file_stream.tell()\n        \n        # Read first 10 bytes to check for RTF signature\n        header = file_stream.read(10).decode('ascii', errors='ignore')\n        \n        # Restore the original position\n        file_stream.seek(current_position)\n        \n        # Check if the file starts with RTF signature\n        return header.startswith('{\\\\rtf')\n\n    def convert(\n        self,\n        file_stream: BinaryIO,\n        stream_info: StreamInfo,\n        **kwargs: Any,\n    ) -> DocumentConverterResult:\n\n        \"\"\"\n        Convert RTF content to Markdown.\n        \"\"\"\n        # Read the RTF content\n        rtf_content = file_stream.read().decode('ascii', errors='ignore')\n        \n        # Convert RTF to plain text\n        plain_text = striprtf.rtf_to_text(rtf_content)\n        \n        # Basic formatting conversions\n        markdown_text = self._format_as_markdown(plain_text)\n        \n        # Return the conversion result\n        return DocumentConverterResult(\n            markdown=markdown_text,\n        )\n        \n    def _format_as_markdown(self, text: str) -> str:\n        \"\"\"\n        Perform basic formatting to convert plain text to Markdown.\n        This is a simplified conversion that handles common RTF elements.\n        \"\"\"\n        result = text\n        \n        # Handle paragraphs (ensure proper line breaks)\n        result = re.sub(r'\\n\\s*\\n', '\\n\\n', result)\n        \n        # Handle bullet points (often represented as * or • in RTF)\n        result = re.sub(r'^\\s*[•*]\\s*(.+)$', r'* \\1', result, flags=re.MULTILINE)\n        \n        # Handle numbered lists\n        result = re.sub(r'^\\s*(\\d+)[.)]\\s*(.+)$', r'\\1. \\2', result, flags=re.MULTILINE)\n        \n        # Clean up extra whitespace\n        result = re.sub(r' +', ' ', result)\n        result = re.sub(r'\\n{3,}', '\\n\\n', result)\n        return result.strip()\n```\n\nand  the following pyproject.toml\n\n```\n...\n[project.entry-points.\"markitdown.plugin\"]\nmarkitdown_rtf_plugin = \"src.markdown._rtf_converter:RtfConverter\"\n...\n```\n\nI then install the module with `poetry install`.\n\nWhen I attempt to run \n\n```python\nmd = MarkItDown(enable_plugins=True) # Set to True to enable plugins\nresult = md.convert(\"tests/files/file-sample_100kB.rtf\")\nprint(result.text_content)\n```\n\nI get the following error:\n\n```\nUserWarning: Plugin '<class 'iliad_utils.markdown._rtf_converter.RtfConverter'>' failed to register converters:\nTraceback (most recent call last):\n  File \"/Users/hoangsx/Library/CloudStorage/OneDrive-AbbVieInc(O365)/workspace/projects/iliad-utils/.venv/lib/python3.12/site-packages/markitdown/_markitdown.py\", line 221, in enable_plugins\n    plugin.register_converters(self, **kwargs)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: type object 'RtfConverter' has no attribute 'register_converters'\n\n  warn(f\"Plugin '{plugin}' failed to register converters:\\n{tb}\")\n```\n\nIs there anything I'm doing wrong? Does my plugin class need a `register_converters` method?", "comments_url": "https://api.github.com/repos/microsoft/markitdown/issues/1162/comments", "author": "shoang22", "comments": [{"user": "afourney", "created_at": "2025-03-31T02:35:29Z", "body": "Thanks for the report. I'll\nLook into it asap.\n\nBut, register_converters does not need to be in the class... only the package.\n\nTry removing :RtfConverter from entry point in pyproject "}, {"user": "shoang22", "created_at": "2025-03-31T14:00:56Z", "body": "that worked! thanks"}], "satisfaction_conditions": ["Correct configuration of the plugin entry point in pyproject.toml", "Understanding of the proper plugin registration mechanism", "A solution that resolves the AttributeError without requiring changes to the converter class implementation"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:46"}, "dockerfile": null, "language": "python"}
{"number": 149, "title": "ValueError: numpy.dtype size changed", "created_at": "2024-12-19T15:29:56Z", "closed_at": "2024-12-20T00:24:46Z", "commit_id": "a5f39d692225a5e698b90b945a7a2691ddd46eee", "labels": [], "url": "https://github.com/microsoft/markitdown/issues/149", "body": "I get the following error trying to convert any pdf to md:\n\n```\n❯ markitdown notes.pdf > notes.md\nTraceback (most recent call last):\n  File \"/Users/davidwoodburn/.config/python/bin/markitdown\", line 5, in <module>\n    from markitdown.__main__ import main\n  File \"/Users/davidwoodburn/.config/python/lib/python3.12/site-packages/markitdown/__init__.py\", line 5, in <module>\n    from ._markitdown import MarkItDown, FileConversionException, UnsupportedFormatException\n  File \"/Users/davidwoodburn/.config/python/lib/python3.12/site-packages/markitdown/_markitdown.py\", line 22, in <module>\n    import pandas as pd\n  File \"/Users/davidwoodburn/.config/python/lib/python3.12/site-packages/pandas/__init__.py\", line 46, in <module>\n    from pandas.core.api import (\n  File \"/Users/davidwoodburn/.config/python/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n    from pandas._libs import (\n  File \"/Users/davidwoodburn/.config/python/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 18, in <module>\n    from pandas._libs.interval import Interval\n  File \"interval.pyx\", line 1, in init pandas._libs.interval\nValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n```", "comments_url": "https://api.github.com/repos/microsoft/markitdown/issues/149/comments", "author": "davidwoodburn", "comments": [{"user": "l-lumin", "created_at": "2024-12-19T15:40:47Z", "body": "could you try downgrading your NumPy version to 1.26.4? I think the issue might be caused by changes to the C API in NumPy 2.0"}, {"user": "davidwoodburn", "created_at": "2024-12-19T23:22:23Z", "body": "That worked. Thank you. Now, should being compatible with NumPy 2.0 be a separate issue?"}], "satisfaction_conditions": ["A solution that resolves the ValueError related to numpy.dtype size incompatibility", "A clear explanation of the cause of the error", "Guidance on appropriate package version compatibility"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:52"}, "dockerfile": null, "language": "python"}
{"number": 157, "title": "Garbage collection failed", "created_at": "2025-03-20T11:37:11Z", "closed_at": "2025-03-22T20:08:53Z", "commit_id": "cc2c2f5488fc94c84acdcac3214f1d303acc879a", "labels": [], "url": "https://github.com/ai-christianson/RA.Aid/issues/157", "body": "When responding to agent, the following error started appearing, any advice on how to fix?\n\nra_aid.ra_aid.database.repositories.human_input_repository - ERROR - Failed to garbage collect human input records: FOREIGN KEY constraint failed", "comments_url": "https://api.github.com/repos/ai-christianson/RA.Aid/issues/157/comments", "author": "myevolve", "comments": [{"user": "ai-christianson", "created_at": "2025-03-20T12:30:23Z", "body": "> When responding to agent, the following error started appearing, any advice on how to fix?\n> \n> ra_aid.ra_aid.database.repositories.human_input_repository - ERROR - Failed to garbage collect human input records: FOREIGN KEY constraint failed\n\nYou might need to delete your `.ra-aid` dir and start with fresh state."}, {"user": "myevolve", "created_at": "2025-03-22T20:08:53Z", "body": "I removed the .ra-aid and this resolved the issue, however, it was not optimal."}], "satisfaction_conditions": ["A solution that resolves the garbage collection error without requiring complete data loss", "An explanation of why the foreign key constraint is failing during garbage collection", "A targeted fix that addresses only the corrupted data rather than removing all data"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:38"}, "dockerfile": null, "language": "python"}
{"number": 3, "title": "Several models were automatically downloaded and all of them were stuffed into the cache space of the C drive", "created_at": "2024-12-05T10:39:25Z", "closed_at": "2024-12-05T15:24:57Z", "commit_id": "decf0a85d8021cd108d20717679f76c86462650e", "labels": [], "url": "https://github.com/huanngzh/ComfyUI-MVAdapter/issues/3", "body": "C drive is limited, Can the author modify the code to change the model into Comfyui/models?", "comments_url": "https://api.github.com/repos/huanngzh/ComfyUI-MVAdapter/issues/3/comments", "author": "coddz", "comments": [{"user": "coddz", "created_at": "2024-12-05T10:59:08Z", "body": "the requirements.txt need checking and changing, for example: \r\ntorch>=2.1.1\r\ntorchvision>=0.16.1\r\ndiffusers>=0.31.0\r\ntransformers>=4.46.3\r\npeft\r\nnumpy>=1.26.2\r\nhuggingface_hub>=0.24.6\r\naccelerate>=1.1.1\r\n\r\nthe torch version is too low, may cause a lot of custom nodes failure."}, {"user": "coddz", "created_at": "2024-12-05T11:02:03Z", "body": "the checkpoint(sdxl) and vae(sdxl) of diffusers loader maybe select from the local like ldm, avoid downloading the sdxl base repetitavely."}, {"user": "huanngzh", "created_at": "2024-12-05T15:24:28Z", "body": "Thanks for your suggestions! The above issues have been solved.\r\n\r\nBtw, `diffusers` will not download the checkpoints repetitavely due to its cache mechanism. Now the cache dir has been set to `ComfyUI/models/diffusers`."}], "satisfaction_conditions": ["A way to change the default model download location from the C drive to a custom directory", "Updated dependency requirements that prevent compatibility issues with custom nodes", "A mechanism to avoid redundant downloads of the same model files", "Proper caching configuration for diffusers models"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:07"}, "dockerfile": null, "language": "python"}
{"number": 59, "title": "feat: Add proxy support for browser automation.", "created_at": "2025-01-09T03:31:44Z", "closed_at": "2025-01-10T09:08:00Z", "commit_id": "68d0ea2593b8e9b017d4f89dd57ad528fef75915", "labels": [], "url": "https://github.com/browser-use/web-ui/pull/59", "body": "feat: Add proxy support for browser automation and fix TypeError: MessageManager.__init__() got an unexpected keyword argument 'tool_call_in_content'", "comments_url": "https://api.github.com/repos/browser-use/web-ui/issues/59/comments", "author": "qitest", "comments": [{"user": "warmshao", "created_at": "2025-01-09T11:59:34Z", "body": "feat: Add proxy support for browser automation and fix TypeError: MessageManager.init() got an unexpected keyword argument 'tool_call_in_content'. \r\n\r\nThis is not an error. Please upgrade to browser-use>=0.1.18"}, {"user": "qitest", "created_at": "2025-01-10T02:24:54Z", "body": "> feat: Add proxy support for browser automation and fix TypeError: MessageManager.init() got an unexpected keyword argument 'tool_call_in_content'.\r\n> \r\n> This is not an error. Please upgrade to browser-use>=0.1.18\r\n\r\nThanks for the suggestion. I've upgraded to browser-use>=0.1.18 "}], "satisfaction_conditions": ["A solution that resolves the TypeError related to MessageManager.__init__()", "Clear guidance on the required version of dependencies", "Clarification that the reported issue is expected behavior rather than an error"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:16:39"}, "dockerfile": "FROM python:3.11-slim\n\n# Set working directory\nWORKDIR /app\n\n# Add labels for better container identification\nLABEL maintainer=\"Docker Builder\"\nLABEL description=\"Environment for browser-use/web-ui with proxy support\"\nLABEL version=\"1.0\"\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wget \\\n    curl \\\n    gnupg \\\n    ca-certificates \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Playwright system dependencies\nRUN apt-get update && apt-get install -y \\\n    libglib2.0-0 \\\n    libnss3 \\\n    libnspr4 \\\n    libatk1.0-0 \\\n    libatk-bridge2.0-0 \\\n    libcups2 \\\n    libdrm2 \\\n    libdbus-1-3 \\\n    libxcb1 \\\n    libxkbcommon0 \\\n    libx11-6 \\\n    libxcomposite1 \\\n    libxdamage1 \\\n    libxext6 \\\n    libxfixes3 \\\n    libxrandr2 \\\n    libgbm1 \\\n    libpango-1.0-0 \\\n    libcairo2 \\\n    libasound2 \\\n    libatspi2.0-0 \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/browser-use/web-ui.git . && \\\n    git checkout 68d0ea2593b8e9b017d4f89dd57ad528fef75915\n\n# Install Python dependencies using pip\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Install Playwright browsers\nRUN playwright install\n\n# Copy the .env.example to .env (users can modify this later)\nRUN cp .env.example .env\n\n# Set default environment variables for configuration\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Expose the default port\nEXPOSE 7788\n\n# Set the entrypoint to the webui.py script\n# Users can override with their own command if needed\nCMD [\"python\", \"webui.py\", \"--ip\", \"0.0.0.0\", \"--port\", \"7788\"]", "language": "python"}
{"number": 298, "title": "error: File not found: `requirements.txt`", "created_at": "2025-02-15T07:46:25Z", "closed_at": "2025-02-16T07:13:30Z", "commit_id": "d3eeb81a2ea7fc501cb98f7bb1c92c561df50f89", "labels": [], "url": "https://github.com/browser-use/web-ui/issues/298", "body": "Stuck on 3rd step it can not find the requirements file", "comments_url": "https://api.github.com/repos/browser-use/web-ui/issues/298/comments", "author": "Kaykode", "comments": [{"user": "marginal23326", "created_at": "2025-02-15T09:34:03Z", "body": "> Stuck on 3rd step it can not find the requirements file\n\nIs your terminal on the `web-ui` directory? Make sure you are not running the commands from a different directory. Also quickly check if there is a `requirements.txt` file inside the `web-ui` directory. It should be there if the cloning was successful."}, {"user": "Kaykode", "created_at": "2025-02-16T07:13:30Z", "body": "Hey Thanks, it worked i skipped the cd web_ui step "}], "satisfaction_conditions": ["Guidance on correct directory navigation for running commands", "Troubleshooting assistance for file path errors", "Clear identification of steps missed in a multi-step process"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:12"}, "dockerfile": null, "language": "python"}
{"number": 209, "title": "ImportError: cannot import name 'AgentStepTelemetryEvent' from 'browser_use.telemetry.views'", "created_at": "2025-01-30T17:45:51Z", "closed_at": "2025-02-12T23:10:49Z", "commit_id": "d0b4f4c44133e414f5368bcb9c8158c9cde63816", "labels": [], "url": "https://github.com/browser-use/web-ui/issues/209", "body": "I am having importError when running python webui.py --ip 127.0.0.1 --port 7788\n```\nImportError: cannot import name 'AgentStepTelemetryEvent' from 'browser_use.telemetry.views' \nDid you mean: 'AgentEndTelemetryEvent'?\n\n```\nPC: windows 11\npython v- 3.12.8\n", "comments_url": "https://api.github.com/repos/browser-use/web-ui/issues/209/comments", "author": "santosrai", "comments": [{"user": "warmshao", "created_at": "2025-01-31T00:51:28Z", "body": "update browser-use=0.1.29"}, {"user": "santosrai", "created_at": "2025-02-12T23:10:33Z", "body": "Thank you. Update helped."}], "satisfaction_conditions": ["A solution that resolves the ImportError for 'AgentStepTelemetryEvent'", "A straightforward fix that can be implemented quickly", "Guidance on package version compatibility"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:19"}, "dockerfile": null, "language": "python"}
{"number": 163, "title": "error when running with deepseek API", "created_at": "2025-01-26T11:22:11Z", "closed_at": "2025-01-27T01:15:19Z", "commit_id": "af3a84ff8b953178c79c05189ca1ffc1e3006411", "labels": [], "url": "https://github.com/browser-use/web-ui/issues/163", "body": "Here is the message received when trying to use deepseek_r1 model\n```\n\n📍 Step 1\nERROR    [agent] ❌ Result failed 1/5 times:\n Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 18231\nINFO     [src.agent.custom_agent] \n📍 Step \n```1\n\n", "comments_url": "https://api.github.com/repos/browser-use/web-ui/issues/163/comments", "author": "marcNY", "comments": [{"user": "AlexAtmtit", "created_at": "2025-01-26T13:35:23Z", "body": "The same here, always receive this message with DeepSeek R1:\n`['Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 20447', 'Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 20447', 'Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 20447', 'Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 20447', 'Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum ChatCompletionRequestContent at line 1 column 20447']`"}, {"user": "warmshao", "created_at": "2025-01-26T13:36:45Z", "body": "uncheck use vision！"}, {"user": "AlexAtmtit", "created_at": "2025-01-26T13:57:38Z", "body": "> uncheck use vision！\n\nIt worked, thanks!"}, {"user": "warmshao", "created_at": "2025-01-27T01:15:19Z", "body": "OK"}], "satisfaction_conditions": ["A solution that resolves the JSON deserialization error when using the DeepSeek R1 model", "A simple configuration change that allows the DeepSeek API to function properly", "Clear instructions on how to prevent the ChatCompletionRequestContent error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:26"}, "dockerfile": null, "language": "python"}
{"number": 74, "title": "feat: add llm provider and model default value to supperss `UserWarning`", "created_at": "2025-01-10T09:59:41Z", "closed_at": "2025-01-10T13:33:59Z", "commit_id": "517c8e0cf81592679f4816f577e62dd79c3321ec", "labels": [], "url": "https://github.com/browser-use/web-ui/pull/74", "body": "To suppress the below `UserWarning`, we can set a default value for `LLM Provider` and `LLM Model Name`. The `warning` was as below:\r\n\r\n```\r\nH:\\Documents\\Work\\browser-web-ui\\myenv\\Lib\\site-packages\\gradio\\components\\dropdown.py:226: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include:  or set allow_custom_value=True.\r\n```", "comments_url": "https://api.github.com/repos/browser-use/web-ui/issues/74/comments", "author": "MeshkatShB", "comments": [{"user": "warmshao", "created_at": "2025-01-10T12:07:51Z", "body": "Hi, Please set default llm_provider to openai, deafult llm_model_name to model_names['openai']"}, {"user": "MeshkatShB", "created_at": "2025-01-10T13:39:49Z", "body": "> Hi, Please set default llm_provider to openai, deafult llm_model_name to model_names['openai']\r\n\r\nHi there. There was no model_name Dict so I hardcoded it. I can remove the hardcoding separately if you wish."}, {"user": "warmshao", "created_at": "2025-01-10T13:41:03Z", "body": "> > Hi, Please set default llm_provider to openai, deafult llm_model_name to model_names['openai']\r\n> \r\n> Hi there. There was no model_name Dict so I hardcoded it. I can remove the hardcoding separately if you wish.\r\n\r\nThat's OK"}], "satisfaction_conditions": ["A solution that suppresses the UserWarning in the Dropdown component", "Setting appropriate default values for the LLM provider and model name fields", "A pragmatic implementation that works with the existing codebase structure"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:32"}, "dockerfile": null, "language": "python"}
{"number": 49, "title": "I can't run it - log", "created_at": "2025-01-08T14:21:59Z", "closed_at": "2025-01-08T22:59:09Z", "commit_id": "779c4116a79074a59dd64698ef8dc89d8d989b10", "labels": [], "url": "https://github.com/browser-use/web-ui/issues/49", "body": "Traceback (most recent call last):\r\n  File \"F:\\_browser use webai\\web-ui-main\\webui.py\", line 196, in run_custom_agent\r\n    browser_context_ = await playwright.chromium.launch_persistent_context(\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\playwright\\async_api\\_generated.py\", line 14681, in launch_persistent_context\r\n    await self._impl_obj.launch_persistent_context(\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\playwright\\_impl\\_browser_type.py\", line 159, in launch_persistent_context\r\n    from_channel(await self._channel.send(\"launchPersistentContext\", params)),\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 61, in send\r\n    return await self._connection.wrap_api_call(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 528, in wrap_api_call\r\n    raise rewrite_error(error, f\"{parsed_st['apiName']}: {error}\") from None\r\nplaywright._impl._errors.TargetClosedError: BrowserType.launch_persistent_context: Target page, context or browser has been closed\r\nBrowser logs:\r\n\r\n<launching> C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe --disable-field-trial-config --disable-background-networking --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-back-forward-cache --disable-breakpad --disable-client-side-phishing-detection --disable-component-extensions-with-background-pages --disable-component-update --no-default-browser-check --disable-default-apps --disable-dev-shm-usage --disable-extensions --disable-features=ImprovedCookieControls,LazyFrameLoading,GlobalMediaControls,DestroyProfileOnBrowserClose,MediaRouter,DialMediaRouteProvider,AcceptCHFrame,AutoExpandDetailsElement,CertificateTransparencyComponentUpdater,AvoidUnnecessaryBeforeUnloadCheckSync,Translate,HttpsUpgrades,PaintHolding,ThirdPartyStoragePartitioning,LensOverlay,PlzDedicatedWorker --allow-pre-commit-input --disable-hang-monitor --disable-ipc-flooding-protection --disable-popup-blocking --disable-prompt-on-repost --disable-renderer-backgrounding --force-color-profile=srgb --metrics-recording-only --no-first-run --enable-automation --password-store=basic --use-mock-keychain --no-service-autorun --export-tagged-pdf --disable-search-engine-choice-screen --unsafely-disable-devtools-self-xss-warnings --no-sandbox --user-data-dir=C:\\Users\\User\\AppData\\Local\\Google\\Chrome\\User Data --remote-debugging-pipe about:blank\r\n<launched> pid=19352\r\nCall log:\r\n  - <launching> C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe --disable-field-trial-config --disable-background-networking --disable-background-timer-throttling --disable-backgrounding-occluded-windows --disable-back-forward-cache --disable-breakpad --disable-client-side-phishing-detection --disable-component-extensions-with-background-pages --disable-component-update --no-default-browser-check --disable-default-apps --disable-dev-shm-usage --disable-extensions --disable-features=ImprovedCookieControls,LazyFrameLoading,GlobalMediaControls,DestroyProfileOnBrowserClose,MediaRouter,DialMediaRouteProvider,AcceptCHFrame,AutoExpandDetailsElement,CertificateTransparencyComponentUpdater,AvoidUnnecessaryBeforeUnloadCheckSync,Translate,HttpsUpgrades,PaintHolding,ThirdPartyStoragePartitioning,LensOverlay,PlzDedicatedWorker --allow-pre-commit-input --disable-hang-monitor --disable-ipc-flooding-protection --disable-popup-blocking --disable-prompt-on-repost --disable-renderer-backgrounding --force-color-profile=srgb --metrics-recording-only --no-first-run --enable-automation --password-store=basic --use-mock-keychain --no-service-autorun --export-tagged-pdf --disable-search-engine-choice-screen --unsafely-disable-devtools-self-xss-warnings --no-sandbox --user-data-dir=C:\\Users\\User\\AppData\\Local\\Google\\Chrome\\User Data --remote-debugging-pipe about:blank\r\n  -   - <launched> pid=19352\r\n\r\nTraceback (most recent call last):\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\r\n    response = await route_utils.call_process_api(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2045, in process_api\r\n    result = await self.call_function(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1590, in call_function\r\n    prediction = await fn(*processed_input)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 837, in async_wrapper\r\n    response = await f(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\webui.py\", line 83, in run_browser_agent\r\n    final_result, errors, model_actions, model_thoughts = await run_custom_agent(\r\n                                                          ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\_browser use webai\\web-ui-main\\webui.py\", line 268, in run_custom_agent\r\n    await browser.close()\r\n          ^^^^^^^\r\nUnboundLocalError: cannot access local variable 'browser' where it is not associated with a value", "comments_url": "https://api.github.com/repos/browser-use/web-ui/issues/49/comments", "author": "PaYo90", "comments": [{"user": "warmshao", "created_at": "2025-01-08T14:28:14Z", "body": "Have you closed all chrome windows before clicked the run button?"}, {"user": "PaYo90", "created_at": "2025-01-08T21:52:06Z", "body": "it helped : ) thanks"}], "satisfaction_conditions": ["A solution that resolves the browser instance conflict causing the TargetClosedError", "A simple troubleshooting step that can be performed without code changes", "Clear instructions on how to prepare the environment before running the application"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:44"}, "dockerfile": null, "language": "python"}
{"number": 30, "title": "fix: log for own browser env var issues", "created_at": "2025-01-07T08:45:01Z", "closed_at": "2025-01-08T11:51:09Z", "commit_id": "708172247bb710ecaced9231381ad4ed2191a789", "labels": [], "url": "https://github.com/browser-use/web-ui/pull/30", "body": "Add logs in case user selected on browser but either forgot to add the right env vars (CHROME_PATH, CHROME_USER_DATA) or set them to incorrect paths", "comments_url": "https://api.github.com/repos/browser-use/web-ui/issues/30/comments", "author": "matthew1809", "comments": [{"user": "warmshao", "created_at": "2025-01-07T11:41:08Z", "body": "Hi, chrome_exe and chrome_use_data can actually be set to None. Currently, when the user doesn't specify them, they default to an empty string, which causes an error. You should change it to:\r\n\r\n1. If chrome_exe is an empty string (''), set it to None. Otherwise, check if the chrome_exe path exists, and only then raise an error if it doesn't.\r\n\r\n2. If chrome_use_data is an empty string (''), set it to None. There's no need to check if the chrome_use_data path exists because Playwright will automatically create it if it doesn't.\r\n\r\nThanks!"}, {"user": "matthew1809", "created_at": "2025-01-07T16:41:24Z", "body": "Thanks @warmshao updated"}, {"user": "warmshao", "created_at": "2025-01-08T11:51:17Z", "body": "LGTM"}], "satisfaction_conditions": ["Handle empty environment variable values appropriately", "Only validate paths when necessary", "Prevent unnecessary errors for default configurations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:53"}, "dockerfile": null, "language": "python"}
{"number": 115, "title": "指令遵循问题", "created_at": "2025-02-27T04:18:25Z", "closed_at": "2025-03-03T03:45:22Z", "commit_id": "26d2c05957eae58c9501b717b1d37250151c0e4a", "labels": [], "url": "https://github.com/stepfun-ai/Step-Audio/issues/115", "body": "注意到tts中的zeroshot的systemtoken\n`作为一名卓越的声优演员，你的任务是根据文本中（）或()括号内标注的情感、语种或方言、音乐哼唱、语音调整等标签，以丰富细腻的情感和自然顺畅的语调来朗读文本。\\n# 情感标签涵盖了多种情绪状态，包括但不限于：\\n- \"高兴1\"\\n- \"高兴2\"\\n- \"生气1\"\\n- \"生气2\"\\n- \"悲伤1\"\\n- \"撒娇1\"\\n\\n# 语种或方言标签包含多种语言或方言，包括但不限于：\\n- \"中文\"\\n- \"英文\"\\n- \"韩语\"\\n- \"日语\"\\n- \"四川话\"\\n- \"粤语\"\\n- \"广东话\"\\n\\n# 音乐哼唱标签包含多种类型歌曲哼唱，包括但不限于：\\n- \"RAP\"\\n- \"哼唱\"\\n\\n# 语音调整标签，包括但不限于：\\n- \"慢速1\"\\n- \"慢速2\"\\n- \"快速1\"\\n- \"快速2\"\\n\\n请在朗读时，根据这些情感标签的指示，调整你的情感、语气、语调和哼唱节奏，以确保文本的情感和意义得到准确而生动的传达，如果没有()或（）括号，则根据文本语义内容自由演绎。`\n\n我的调用方式类似于\n\n(\"高兴1\")xxxxxx\n\n但是实际使用中，指令遵循效果较差，甚至还会出现把括号内指令念出来的问题，这个是我的打开方式不对么？", "comments_url": "https://api.github.com/repos/stepfun-ai/Step-Audio/issues/115/comments", "author": "boji123", "comments": [{"user": "mabuyun", "created_at": "2025-02-27T06:44:24Z", "body": "括号内不应该加引号，你试下下面的文本应该可以\n（四川话）你今天吃饭了吗"}, {"user": "boji123", "created_at": "2025-02-27T08:35:02Z", "body": "语速有显著改善，四川话可以说；\n但是情绪不够明显，粤语广州话发音错误"}, {"user": "mabuyun", "created_at": "2025-02-27T08:49:55Z", "body": "这属于效果问题，指令遵循测试可以通过 tts_app.py 进行"}], "satisfaction_conditions": ["Correct syntax for using emotion/dialect tags in TTS system", "Guidance on how to properly invoke the TTS system with emotion/dialect instructions", "Information about how to test instruction compliance"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:55"}, "dockerfile": null, "language": "python"}
{"number": 100, "title": "关于instruction tags", "created_at": "2025-02-25T03:10:30Z", "closed_at": "2025-02-26T10:11:29Z", "commit_id": "c7f4b3334a218958480bfd3fd44b9ee6cc36cceb", "labels": [], "url": "https://github.com/stepfun-ai/Step-Audio/issues/100", "body": "论文中描述『happiness, anger, sadness, and speed variations like fast and slow, each divided into five hierarchical levels.』 怎样加1到5级标签？比如（fast1）、（fast2）一直到（fast5）吗？", "comments_url": "https://api.github.com/repos/stepfun-ai/Step-Audio/issues/100/comments", "author": "shanhaidexiamo", "comments": [{"user": "786440445", "created_at": "2025-02-25T06:32:46Z", "body": "> 论文中描述『happiness, anger, sadness, and speed variations like fast and slow, each divided into five hierarchical levels.』 怎样加1到5级标签？比如（fast1）、（fast2）一直到（fast5）吗？\n\n不是的哈，切分为5级标签，比如 慢速2，慢速1，快速1，快速2，加上正常的一共是五级标签。"}, {"user": "shanhaidexiamo", "created_at": "2025-02-25T06:51:15Z", "body": "> > 论文中描述『happiness, anger, sadness, and speed variations like fast and slow, each divided into five hierarchical levels.』 怎样加1到5级标签？比如（fast1）、（fast2）一直到（fast5）吗？\n> \n> 不是的哈，切分为5级标签，比如 慢速2，慢速1，快速1，快速2，加上正常的一共是五级标签。\n\n原来是这样，非常感谢~"}], "satisfaction_conditions": ["Clear explanation of how the hierarchical levels for emotions and speed variations are structured", "Practical examples of how to format the emotion and speed tags", "Clarification that 'normal' is included as one of the five levels"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:00"}, "dockerfile": null, "language": "python"}
{"number": 19, "title": "Setting up V3 on Mac", "created_at": "2025-02-21T23:27:46Z", "closed_at": "2025-02-24T18:46:14Z", "commit_id": "2a1f92a4474d37628c7e9314aa57361a81cdd000", "labels": ["enhancement"], "url": "https://github.com/coleam00/Archon/issues/19", "body": "I'm running into issues getting the MCP setup on Mac. \n\n1. The original venv / setup kept referencing .exe files and failing to set up the virtual environment. \n2. I made some updates to make it run on Mac -- I can launch the virtual environment. When putting my MCP code into Cursor (it's the venv python file then a space then the path to the py file) it says no tools. Any help for mac users would be great. ", "comments_url": "https://api.github.com/repos/coleam00/Archon/issues/19/comments", "author": "TheMattBerman", "comments": [{"user": "taylor-aparai", "created_at": "2025-02-23T00:13:13Z", "body": "Got this working by change \"Scripts\" to \"bin\" and removing .exe from python.exe. \n\nMy final looked like: \n\n`/Users/{username}/Documents/code/mcp/archon/iterations/v3-mcp-support/venv/bin/python /Users/{username}/Documents/code/mcp/archon/iterations/v3-mcp-support/mcp_server.py`\n\n\nThis is a bit hacky... but I think perfected python path resolution would be OS-specific...\n\n``` # Construct the paths\n    # Handle different venv binary locations based on OS\n    if sys.platform == \"win32\":\n        python_path = os.path.join(base_path, 'venv', 'Scripts', 'python.exe')\n    else:\n        python_path = os.path.join(base_path, 'venv', 'bin', 'python')``` \n\n"}, {"user": "coleam00", "created_at": "2025-02-24T18:46:14Z", "body": "> Got this working by change \"Scripts\" to \"bin\" and removing .exe from python.exe.\n> \n> My final looked like:\n> \n> `/Users/{username}/Documents/code/mcp/archon/iterations/v3-mcp-support/venv/bin/python /Users/{username}/Documents/code/mcp/archon/iterations/v3-mcp-support/mcp_server.py`\n> \n> This is a bit hacky... but I think perfected python path resolution would be OS-specific...\n> \n> ```\n>     # Handle different venv binary locations based on OS\n>     if sys.platform == \"win32\":\n>         python_path = os.path.join(base_path, 'venv', 'Scripts', 'python.exe')\n>     else:\n>         python_path = os.path.join(base_path, 'venv', 'bin', 'python')``` \n> ```\n\nThis is great, thanks @taylor-aparai! Looks very similar to the approach for #20 which I will be merging soon to resolve this."}, {"user": "TheMattBerman", "created_at": "2025-02-26T03:28:20Z", "body": "Thank you @taylor-aparai for the quick fix and @coleam00 for getting this great repo up. I ended up with a hybrid approach with this and #20 which ended up working for me. Thank you! "}], "satisfaction_conditions": ["A solution that adapts the MCP setup process for Mac OS compatibility", "Proper path configuration for Python virtual environment on Mac", "A working command to launch the MCP server on Mac", "Cross-platform compatibility approach for the codebase"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:21"}, "dockerfile": null, "language": "python"}
{"number": 1382, "title": "[Bug]: Data ingestion task failing in DocAgent", "created_at": "2025-03-17T14:52:13Z", "closed_at": "2025-03-19T19:37:50Z", "commit_id": "46e0e6bb880a65e955b79b616e53e906c7d04b6c", "labels": ["bug"], "url": "https://github.com/ag2ai/ag2/issues/1382", "body": "### Describe the bug\n\nDuring first ingestion function call step, below error is observed:\n\n_Swarm_Tool_Executor (to chat_manager):\n\n***** Response from calling tool (call_Ndd1aKWi9w3NfqGFoeab24OC) *****\nData Ingestion Task Failed, Error 'Ingest' object is not subscriptable: ''\n**********************************************************************\n\nInput arguments are correct for execution. Same error is observed in both in-memory and VectorChromaQueryEngine \n\n### Steps to reproduce\n\n_No response_\n\n### Model Used\n\ngpt-4o\n\n### Expected Behavior\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\nag2 = {extras = [\"openai\", \"rag\"], version = \"^0.8.1\"}\nRocky 9.4\nPython 3.12.7", "comments_url": "https://api.github.com/repos/ag2ai/ag2/issues/1382/comments", "author": "gunue", "comments": [{"user": "AgentGenie", "created_at": "2025-03-19T19:37:50Z", "body": "@gunue It should be fixed in release 0.8.2. Could you verify? Thanks."}, {"user": "gunue", "created_at": "2025-03-21T15:23:52Z", "body": "Verified for 0.8.3 and it's fixed. Thanks"}], "satisfaction_conditions": ["A fix for the 'Ingest' object not being subscriptable error during data ingestion", "An updated version of the library that resolves the bug", "Compatibility with their existing environment (Rocky 9.4, Python 3.12.7)"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:01"}, "dockerfile": null, "language": "python"}
{"number": 298, "title": "[Bug]: GroupChatManager.a_run_chat does not handle NoEligibleSpeaker Exception", "created_at": "2024-12-27T06:39:08Z", "closed_at": "2025-01-03T18:27:17Z", "commit_id": "687af856fe7a22d4ab4e5ec05c941ded83de1fe1", "labels": ["bug"], "url": "https://github.com/ag2ai/ag2/issues/298", "body": "### Describe the bug\r\n\r\nAccording to the code in GroupChatManager.run_chat when GroupChat.select_speaker raise NoEligibleSpeaker , the groupchat will be terminated. This feature enables coders to define termination conditions in customized speaker_selection_method. \r\n\r\n```python\r\ndef run_chat(\r\n        self,\r\n        messages: Optional[list[dict]] = None,\r\n        sender: Optional[Agent] = None,\r\n        config: Optional[GroupChat] = None,\r\n    ) -> tuple[bool, Optional[str]]:\r\n        \"\"\"Run a group chat.\"\"\"\r\n        \r\n         # other codes before ...\r\n       \r\n        for i in range(groupchat.max_round):\r\n            self._last_speaker = speaker\r\n            groupchat.append(message, speaker)\r\n            # broadcast the message to all agents except the speaker\r\n            for agent in groupchat.agents:\r\n                if agent != speaker:\r\n                    self.send(message, agent, request_reply=False, silent=True)\r\n            if self._is_termination_msg(message) or i == groupchat.max_round - 1:\r\n                # The conversation is over or it's the last round\r\n                break\r\n            try:\r\n                # select the next speaker\r\n                speaker = groupchat.select_speaker(speaker, self)\r\n                if not silent:\r\n                    iostream = IOStream.get_default()\r\n                    iostream.print(colored(f\"\\nNext speaker: {speaker.name}\\n\", \"green\"), flush=True)\r\n                # let the speaker speak\r\n                reply = speaker.generate_reply(sender=self)\r\n            except KeyboardInterrupt:\r\n                # let the admin agent speak if interrupted\r\n                if groupchat.admin_name in groupchat.agent_names:\r\n                    # admin agent is one of the participants\r\n                    speaker = groupchat.agent_by_name(groupchat.admin_name)\r\n                    reply = speaker.generate_reply(sender=self)\r\n                else:\r\n                    # admin agent is not found in the participants\r\n                    raise\r\n            except NoEligibleSpeaker:\r\n                # No eligible speaker, terminate the conversation\r\n                break\r\n\r\n        # other codes after ...\r\n        return True, None\r\n\r\n```\r\n\r\nHowever, it seems that GroupChatManager.a_run_chat do not have this feature.  \r\nI am not sure whether it is a feature or bug.\r\n```python\r\n\r\nasync def a_run_chat(\r\n        self,\r\n        messages: Optional[list[dict]] = None,\r\n        sender: Optional[Agent] = None,\r\n        config: Optional[GroupChat] = None,\r\n    ):\r\n        # other codes before ...\r\n        for i in range(groupchat.max_round):\r\n            groupchat.append(message, speaker)\r\n\r\n            if self._is_termination_msg(message):\r\n                # The conversation is over\r\n                break\r\n\r\n            # broadcast the message to all agents except the speaker\r\n            for agent in groupchat.agents:\r\n                if agent != speaker:\r\n                    await self.a_send(message, agent, request_reply=False, silent=True)\r\n            if i == groupchat.max_round - 1:\r\n                # the last round\r\n                break\r\n            try:\r\n                # select the next speaker\r\n                speaker = await groupchat.a_select_speaker(speaker, self)\r\n                # let the speaker speak\r\n                reply = await speaker.a_generate_reply(sender=self)\r\n            except KeyboardInterrupt:\r\n                # let the admin agent speak if interrupted\r\n                if groupchat.admin_name in groupchat.agent_names:\r\n                    # admin agent is one of the participants\r\n                    speaker = groupchat.agent_by_name(groupchat.admin_name)\r\n                    reply = await speaker.a_generate_reply(sender=self)\r\n                else:\r\n                    # admin agent is not found in the participants\r\n                    raise\r\n           # It does not have the following exception handler\r\n           #  except NoEligibleSpeaker:  \r\n           #     break\r\n\r\n            if reply is None:\r\n                break\r\n\r\n       # other codes after ...\r\n      \r\n```\r\n### Steps to reproduce\r\n\r\nDefine a speaker_selection_method returning None under some conditions. ( That should be a proper case when we try to define the termination condition\r\n\r\n### Model Used\r\n\r\n_No response_\r\n\r\n### Expected Behavior\r\n\r\n_No response_\r\n\r\n### Screenshots and logs\r\n\r\n_No response_\r\n\r\n### Additional Information\r\n\r\n_No response_", "comments_url": "https://api.github.com/repos/ag2ai/ag2/issues/298/comments", "author": "linmou", "comments": [{"user": "marklysze", "created_at": "2024-12-27T17:45:30Z", "body": "Thanks @linmou, I have addressed this in my Telemetry Phase 1 code, if you need it more urgently then I'll create a new PR. Telemetry Phase 1 #296 "}, {"user": "linmou", "created_at": "2024-12-27T20:56:20Z", "body": "> Thanks @linmou, I have addressed this in my Telemetry Phase 1 code, if you need it more urgently then I'll create a new PR. Telemetry Phase 1 #296\r\n\r\nNot so urgent , I can change my code locally. "}, {"user": "marklysze", "created_at": "2024-12-30T03:02:43Z", "body": "I'm also addressing this in #315, as I think that will merge earlier than #296."}, {"user": "marklysze", "created_at": "2025-01-03T18:27:17Z", "body": "#315 has merged, so we're good to go :)"}], "satisfaction_conditions": ["Confirmation that the NoEligibleSpeaker exception handling will be added to the a_run_chat method", "A timeline for when the fix will be available in the codebase", "Acknowledgment that this is a legitimate issue rather than intended behavior"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:09"}, "dockerfile": null, "language": "python"}
{"number": 169, "title": "[Bug]: Upgraded to AG2 0.5 and imports broke", "created_at": "2024-12-08T06:56:43Z", "closed_at": "2024-12-08T23:49:39Z", "commit_id": "9338c7adfff7faeb371f20eb6307984c16d4dd15", "labels": ["bug"], "url": "https://github.com/ag2ai/ag2/issues/169", "body": "### Describe the bug\n\nUpgraded to v0.5 and I no longer can import \r\n\r\nfrom autogen import (\r\n    SwarmResult,\r\n    AssistantAgent,\r\n    SwarmAgent,\r\n)\r\n\r\nI cannot even import \r\n\r\nfrom autogen.coding import DockerCommandLineCodeExecutor\r\n\r\nNot sure what happened. \n\n### Steps to reproduce\n\nUpgrade from 0.41 to 0.5 via pip install ag2 --upgrade\n\n### Model Used\n\n_No response_\n\n### Expected Behavior\n\n_No response_\n\n### Screenshots and logs\n\n_No response_\n\n### Additional Information\n\n_No response_", "comments_url": "https://api.github.com/repos/ag2ai/ag2/issues/169/comments", "author": "bassilkhilo", "comments": [{"user": "Hk669", "created_at": "2024-12-08T07:08:45Z", "body": "cc @marklysze "}, {"user": "marklysze", "created_at": "2024-12-08T19:00:17Z", "body": "@bassilkhilo, are you able to output the trace when you try and run the program? Just checking if, perhaps, there are any changes made to other files?"}, {"user": "ashim-mahara", "created_at": "2024-12-08T19:26:00Z", "body": "using `pyautogen`works."}, {"user": "ohdearquant", "created_at": "2024-12-08T21:24:49Z", "body": "@bassilkhilo  what environment/package manager do you use?"}, {"user": "bassilkhilo", "created_at": "2024-12-08T23:49:02Z", "body": "Hey all.\r\n\r\nA quick update, @marklysze suggested I run the following commands to fix the issue:\r\n\r\npip uninstall openai pyautogen ag2\r\n\r\npip install ag2\r\n\r\nThis worked, I no longer have import issues.\r\n\r\nI was on AG2 0.41, maybe pyautoagen as well, honestly not too sure. But the above solution fixed the problem.\r\n\r\nCC: @ohdearquant "}], "satisfaction_conditions": ["A solution that resolves import errors after upgrading to AG2 0.5", "Clear instructions for package management to fix dependency conflicts", "A clean installation approach that removes conflicting packages before reinstalling"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:14:14"}, "dockerfile": null, "language": "python"}
{"number": 130, "title": "RayTaskError with hf_transfer or ray.init()", "created_at": "2025-01-30T17:28:28Z", "closed_at": "2025-02-03T09:07:40Z", "commit_id": "356f6a5c4f782c956b2b81d45d9794442b2910b2", "labels": [], "url": "https://github.com/huggingface/open-r1/issues/130", "body": "I have met the error as follows, the error output is so long that I have to copy the last lines:\nRayTaskError(RuntimeError): [36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\nException: Failed too many failures in parallel (3): Request: error decoding response body (no permits available)\n\nThe above exception was the direct cause of the following exception:\n\n[36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/lighteval/models/vllm/vllm_model.py\", line 336, in \nrun_inference_one_model\n    llm = LLM(**model_args)\n          ^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/utils.py\", line 986, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 230, in __init__\n    self.llm_engine = self.engine_class.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 517, in \nfrom_engine_args\n    engine = cls(\n             ^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in\n__init__\n    super().__init__(*args, **kwargs)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 64, in \n_init_executor\n    self._init_workers_ray(placement_group)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 278, in \n_init_workers_ray\n    self._run_workers(\"load_model\",\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 407, in \n_run_workers\n    self.driver_worker.execute_method(method, *driver_args,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 468, in \nexecute_method\n    raise e\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 459, in \nexecute_method\n    return executor(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in load_model\n    self.model_runner.load_model()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1096, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12,\nin get_model\n    return loader.load_model(vllm_config=vllm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 366, \nin load_model\n    loaded_weights = model.load_weights(\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 506, in \nload_weights\n    return loader.load_weights(weights)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 237, in \nload_weights\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 189, in \n_load_module\n    for child_prefix, child_weights in self._groupby_prefix(weights):\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 103, in \n_groupby_prefix\n    for prefix, group in itertools.groupby(weights_by_parts,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 100, in \n<genexpr>\n    weights_by_parts = ((weight_name.split(\".\", 1), weight_data)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 342, \nin _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 298, \nin _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n                                                   ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 251, \nin _prepare_weights\n    hf_folder = download_weights_from_hf(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line\n255, in download_weights_from_hf\n    hf_folder = snapshot_download(\n                ^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 294, in \nsnapshot_download\n    _inner_hf_hub_download(file)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 270, in \n_inner_hf_hub_download\n    return hf_hub_download(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 860, in \nhf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1009, in \n_hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1543, in \n_download_to_tmp_and_move\n    http_get(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 437, in \nhttp_get\n    raise RuntimeError(\nRuntimeError: An error occurred while downloading using `hf_transfer`. Consider disabling HF_HUB_ENABLE_HF_TRANSFER for better \nerror handling.\n(run_inference_one_model pid=602235) Calling ray.init() again after it has already been called. [repeated 7x across cluster]RayTaskError(RuntimeError): [36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\nException: Failed too many failures in parallel (3): Request: error decoding response body (no permits available)\n\nThe above exception was the direct cause of the following exception:\n\n[36mray::run_inference_one_model()[39m (pid=602229, ip=115.154.137.9)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/lighteval/models/vllm/vllm_model.py\", line 336, in \nrun_inference_one_model\n    llm = LLM(**model_args)\n          ^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/utils.py\", line 986, in inner\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 230, in __init__\n    self.llm_engine = self.engine_class.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 517, in \nfrom_engine_args\n    engine = cls(\n             ^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 273, in __init__\n    self.model_executor = executor_class(vllm_config=vllm_config, )\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in\n__init__\n    super().__init__(*args, **kwargs)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 36, in __init__\n    self._init_executor()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 64, in \n_init_executor\n    self._init_workers_ray(placement_group)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 278, in \n_init_workers_ray\n    self._run_workers(\"load_model\",\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 407, in \n_run_workers\n    self.driver_worker.execute_method(method, *driver_args,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 468, in \nexecute_method\n    raise e\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker_base.py\", line 459, in \nexecute_method\n    return executor(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in load_model\n    self.model_runner.load_model()\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1096, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12,\nin get_model\n    return loader.load_model(vllm_config=vllm_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 366, \nin load_model\n    loaded_weights = model.load_weights(\n                     ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/qwen2.py\", line 506, in \nload_weights\n    return loader.load_weights(weights)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 237, in \nload_weights\n    autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 189, in \n_load_module\n    for child_prefix, child_weights in self._groupby_prefix(weights):\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 103, in \n_groupby_prefix\n    for prefix, group in itertools.groupby(weights_by_parts,\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 100, in \n<genexpr>\n    weights_by_parts = ((weight_name.split(\".\", 1), weight_data)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 342, \nin _get_all_weights\n    yield from self._get_weights_iterator(primary_weights)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 298, \nin _get_weights_iterator\n    hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n                                                   ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 251, \nin _prepare_weights\n    hf_folder = download_weights_from_hf(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line\n255, in download_weights_from_hf\n    hf_folder = snapshot_download(\n                ^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 294, in \nsnapshot_download\n    _inner_hf_hub_download(file)\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py\", line 270, in \n_inner_hf_hub_download\n    return hf_hub_download(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in \n_inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 860, in \nhf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1009, in \n_hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1543, in \n_download_to_tmp_and_move\n    http_get(\n  File \"/home/yhpeng/anaconda3/envs/openr1/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 437, in \nhttp_get\n    raise RuntimeError(\nRuntimeError: An error occurred while downloading using `hf_transfer`. Consider disabling HF_HUB_ENABLE_HF_TRANSFER for better \nerror handling.\n(run_inference_one_model pid=602235) Calling ray.init() again after it has already been called. [repeated 7x across cluster]\nI use 4 cards Geforce RTX 4090, could anyone help me? Thanks！", "comments_url": "https://api.github.com/repos/huggingface/open-r1/issues/130/comments", "author": "pyh314", "comments": [{"user": "sam-schorb", "created_at": "2025-01-30T22:06:06Z", "body": "The error comes from Hugging Face's experimental \"hf_transfer\" downloader. Try this:\n1. **Quickest fix**: Disable hf_transfer by running:\n```bash\nexport HF_HUB_ENABLE_HF_TRANSFER=\"false\"\npython your_script.py\n```\n\n2. **Offline approach**: Download model weights locally and point VLLM to the local path:\n```python\nmodel = LLM(model=\"<local folder path>\", ...)\n```\n\n3. **Update dependencies**: Ensure you have recent versions:\n```bash\npip install --upgrade huggingface_hub transformers vllm\n```\n"}, {"user": "pyh314", "created_at": "2025-02-03T09:07:40Z", "body": "The first fix works. "}], "satisfaction_conditions": ["A solution to disable or work around the hf_transfer error", "A straightforward implementation that requires minimal changes to their environment", "A solution that addresses the specific error related to Hugging Face downloads in their Ray/vLLM setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:05:27"}, "dockerfile": null, "language": "python"}
{"number": 137, "title": "Using repo with transformers fix - Model adds words to my text at the end, is there a way to prevent this?", "created_at": "2025-04-11T13:56:46Z", "closed_at": "2025-04-11T14:18:37Z", "commit_id": "81b2f4af5a8429a957508e05fc6a79baa3fe449f", "labels": [], "url": "https://github.com/canopyai/Orpheus-TTS/issues/137", "body": "I'm using the repo suggested in one of the issues with vllm_c, because on windows vllm does not work.\n\nThis means the version may be a bit outdated to the current one, but I wanted to ask if this happens to others, the model adds some words or phrases I don't have in my text, is there a way to prevent this?", "comments_url": "https://api.github.com/repos/canopyai/Orpheus-TTS/issues/137/comments", "author": "juangea", "comments": [{"user": "amuvarma13", "created_at": "2025-04-11T14:08:58Z", "body": "This is not normal or expected behaviour- perhaps someone on Windows can offer more insight - I'd just confirm you have eos token id set to 128258."}, {"user": "juangea", "created_at": "2025-04-11T14:18:37Z", "body": "The stop token was at 49158, I changed it for the one you said, and now it stops correctly, thanks!"}], "satisfaction_conditions": ["A solution that prevents the model from adding unwanted words or phrases at the end of generated text", "Guidance on proper configuration of end-of-sequence tokens for text generation"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:04"}, "dockerfile": null, "language": "python"}
{"number": 132, "title": "About training multilingual models", "created_at": "2025-04-10T08:30:17Z", "closed_at": "2025-04-11T02:31:34Z", "commit_id": "45c6a1f0b173458ee18ef280ed4d863c07ee3fef", "labels": [], "url": "https://github.com/canopyai/Orpheus-TTS/issues/132", "body": "Hi , thanks for your repo! I want to ask some details about training multilingual models. \n1. In your training  stage , did you still follow the rule, the ratio of text batch :speech batch  start with 1:1 and gradually decrease to 0:1 for TTS?   As we all know, for TTS datasets, text and speech should always be one-to-one corresponding, that is, 1:1, 0:1 means there is only speech. Why is the 0:1 situation feasible?\n\n2. In finetuning stage, how did you ensure that so many tags are supported while using only 300 cases ?\n\n3. If I finetune the pre-trained model you released using more of the same language data instead of training from scratch, and then fine-tune the above model using the single speaker data, will I get a better single speaker model?", "comments_url": "https://api.github.com/repos/canopyai/Orpheus-TTS/issues/132/comments", "author": "shanhaidexiamo", "comments": [{"user": "amuvarma13", "created_at": "2025-04-10T12:14:14Z", "body": "1. 1:1 and 0:1 don't mean text:speech in the way you are thinking. There are 2 datasets one is text:text - i.e. question answer. One if text:speech i.e. a Text prompt and speech response. We only used regular TTS (text followed by speech) sequences for multilingual.\n2. Model learns tags very effectively in full parameter finetuning\n3. Yes"}, {"user": "shanhaidexiamo", "created_at": "2025-04-11T02:31:25Z", "body": "Thank you very much for answering my question ！"}], "satisfaction_conditions": ["Clear explanation of the training methodology for multilingual models, particularly regarding the text-to-speech ratio", "Information about how the model effectively learns multiple language tags with limited training data", "Guidance on the optimal training approach for creating a single-speaker model", "Direct answers to specific technical questions about the training methodology"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:09"}, "dockerfile": null, "language": "python"}
{"number": 37, "title": "Pre-train Data Structure", "created_at": "2025-03-21T17:10:40Z", "closed_at": "2025-03-23T02:36:39Z", "commit_id": "cf909a24296f8273a87c6322947e92bc3aff97c8", "labels": [], "url": "https://github.com/canopyai/Orpheus-TTS/issues/37", "body": "Thank you for sharing great work, I want to know about pre-train data format and it meaning given config file\n\n```\n> `\n> # Datasets\n> text_QA_dataset: <speech input-ids>\n> TTS_dataset: <text-input-ids>\n```\nBasically i want to know how can i prepare `text_QA_dataset` and `TTS_dataset` and it's format structure. i am waiting for your response and great-full to you.  \n\nWhat is the different between `text_QA_dataset` and `TTS_dataset`.", "comments_url": "https://api.github.com/repos/canopyai/Orpheus-TTS/issues/37/comments", "author": "saifulislam79", "comments": [{"user": "amuvarma13", "created_at": "2025-03-21T21:27:35Z", "body": "```python\ntokeniser_length = 128256\nstart_of_text = 128000\nend_of_text = 128009\n\nstart_of_speech = tokeniser_length + 1\nend_of_speech = tokeniser_length + 2\n\nstart_of_human = tokeniser_length + 3\nend_of_human = tokeniser_length + 4\n\nstart_of_ai = tokeniser_length + 5\nend_of_ai =  tokeniser_length + 6\npad_token = tokeniser_length + 7\n\naudio_tokens_start = tokeniser_length + 10\n```\n\nstart of human --- start of text --- text tokens --- end of text--- end of human--- start of ai --- start of speech --- speech tokens --- end of speech --- end of ai\n\n\nLet me know if unclear or further questions.\n\nEDIT - for text which I realise you also asked about:\n\nstart of human --- start of text --- question text tokens --- end of text--- end of human --- start of ai --- start of text --- answer text tokens --- end of text --- end of ai\n"}, {"user": "saifulislam79", "created_at": "2025-03-21T21:38:28Z", "body": "Thank you for your reply i had reviewed data processing code into colab, which mentioned into readme file. I need more clear understanding the processing approach,  Is it same processing approach for both fine-tune and pre-train . \n\n```\ndef create_input_ids(example):\n    text_ids = tokenizer.encode(example[\"text\"],  add_special_tokens=True)\n    text_ids.append(end_of_text)\n    example[\"text_tokens\"] = text_ids\n    input_ids = (\n        [start_of_human]\n        + example[\"text_tokens\"]\n        + [end_of_human]\n        + [start_of_ai]\n        + [start_of_speech]\n        + example[\"codes_list\"]\n        + [end_of_speech]\n        + [end_of_ai]\n    )\n    example[\"input_ids\"] = input_ids\n    example[\"labels\"] = input_ids\n    example[\"attention_mask\"] = [1] * len(input_ids)\n\n    return example\n```\n\nhere `text_QA_dataset` and `TTS_dataset` why mentions separately. `text_QA_dataset` is QA textual information with audio or `TTS_dataset` is as normal TTS dataset. it will more convenient , if possible share some data sample about `text_QA_dataset` and `TTS_dataset` format.\n\nI mean that same format as like fine-tune dataset but use different dataset or other. "}, {"user": "amuvarma13", "created_at": "2025-03-21T21:51:12Z", "body": "Yep the `text_QA_dataset` is only text no audio. `tts_dataset` is text and then a spoken version of the text. \n\nHere is what a text sample could look like, all the text samples are chained together so all input_ids are the same length (8192) for pretraining to make the training as efficient as possible:\n\nstart of human --- start of text --- question text tokens (i.e. AutoTokeniser.tokenise(\"What is 2 +2?\")  --- end of text--- end of human --- start of ai --- start of text --- (i.e. AutoTokeniser.tokenise(\"Great question, 2 + 2 =4\") --- end of text --- end of ai\n"}, {"user": "amuvarma13", "created_at": "2025-03-21T21:55:24Z", "body": "Feel free to close this issue - if your question is answered!"}, {"user": "saifulislam79", "created_at": "2025-03-21T22:19:18Z", "body": "**This is the last clarification**\nExample with Token IDs (simplified illustration)\nAssume the tokenizer produces the following (again, just for illustration):\n**input sentence 1** : What is 2 + 2? ----> audio1.mp3\n\n **Answer other sentence** : Great question, 2 + 2 = 4. --->  audio2.mp3\n\n\n```\n\"start of human\" → [101]\n\"start of text\" → [102]\n\"What is 2 + 2?\" → [2001, 2002, 2003, 2004, 2005]\n\"end of text\" → [103]\n\"end of human\" → [104]\n\"start of ai\" → [105]\n\"start of text\" → [102]\n\"Great question, 2 + 2 = 4.\" → [3001, 3002, 3003, 3004, 3005, 3006]\n\"end of text\" → [103]\n\"end of ai\" → [106]\n```\n\n\nChained together example of question and answer:\n\n`[101, 102, 2001, 2002, 2003, 2004, 2005, 103, 104, 105, 102, 3001, 3002, 3003, 3004, 3005, 3006, 103, 106]`\n\nif i have 1M text sentences and it's corresponding audio codes,  what will be `<speech input-ids>` and `<text-input-ids>`. Could you please give a example ."}, {"user": "saiful9379", "created_at": "2025-03-22T09:25:31Z", "body": "@amuvarma13  thank for your clarification. "}, {"user": "amuvarma13", "created_at": "2025-03-22T09:36:55Z", "body": "Sure, \nText input ids (text dataset) is for text question text answer pairs - the format you have given above is correct.\nSpeech input ids i.e. the tts dataset is for text speech pairs no question answering - the format I gave above with start of speech etc is what you want for this,.\n\n"}, {"user": "amuvarma13", "created_at": "2025-03-23T02:36:39Z", "body": "Marking as solved - reopen if unclear."}], "satisfaction_conditions": ["Clear explanation of the difference between text_QA_dataset and TTS_dataset", "Explanation of the data format structure for both dataset types", "Concrete examples showing the token sequence structure", "Clarification on how to process large datasets with the described format"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:17:16"}, "dockerfile": null, "language": "python"}
{"number": 76, "title": "DOCKER_MODS= on arm64 image?", "created_at": "2025-02-04T20:49:18Z", "closed_at": "2025-02-14T23:12:34Z", "commit_id": "af3d2cc358712177c98c067887ec919837222a70", "labels": [], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/76", "body": "is the DOCKER_MODS=calibre still needed? i saw a commit that removed it but seems to be for amd64 only?\n\nI'm running on arm64 rock-5b board ", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/76/comments", "author": "Fuckingnameless", "comments": [{"user": "calibrain", "created_at": "2025-02-04T21:00:11Z", "body": "Oh, you are right, no need for the calibre MODS anymore, I am offloading that to CWA instead :P "}, {"user": "calibrain", "created_at": "2025-02-05T00:04:23Z", "body": "But its if you are using the arm version\nWhich is still not rolled out to `:latest` docker\nIts for the`CF_BYPASS` branch for now"}, {"user": "Fuckingnameless", "created_at": "2025-02-11T15:26:23Z", "body": "eh sorry i confused repos, meant to ask on CWA's, so you're saying i need a DOCKER_MOD on your image too? or only on crocodilestick's?\n\ni just tested his latest image with your CF_Bypass branch and everything seems to be working even PDF ingest/conversion"}, {"user": "calibrain", "created_at": "2025-02-14T23:12:34Z", "body": "No you dont need it in any of the repos, it was too cumbersome and I dropped it and he implemented it directly in the containers.\n\nSo no, you dont need the DOCKER_MODS anymore"}, {"user": "Fuckingnameless", "created_at": "2025-02-15T21:43:14Z", "body": "> But its if you are using the arm version Which is still not rolled out to `:latest` docker Its for the`CF_BYPASS` branch for now\n\njust confirming\n it is NOT needed for any branch right?"}, {"user": "calibrain", "created_at": "2025-02-16T00:31:42Z", "body": "Exact, DOCKER_MODS is not needed for CWA-BD (or CWA anymore)"}], "satisfaction_conditions": ["Clear confirmation about whether DOCKER_MODS=calibre is required for arm64 architecture", "Information about which branches or versions require DOCKER_MODS", "Clarification about repository-specific requirements", "Unambiguous, definitive answer that resolves confusion"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:45"}, "dockerfile": null, "language": "python"}
{"number": 52, "title": "Whatever book I download: corrupt", "created_at": "2025-01-10T19:35:51Z", "closed_at": "2025-01-11T08:03:45Z", "commit_id": "b534664534b316d35266381e210bb66be1f17a99", "labels": [], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/52", "body": "Epub is downloaded just fine:\r\n`2025-01-10 19:31:21,919 - book_manager - INFO - Download finished. Writing to /tmp/cwa-book-downloader/0335e2f8e48c0d3757b9065f6cb804ff.epub\r\n`\r\nBut is recognized as corrupt:\r\n\r\n```\r\n2025-01-10 19:31:21,920 - backend - INFO - Verifying book health: /tmp/cwa-book-downloader/0335e2f8e48c0d3757b9065f6cb804ff.epub\r\n2025-01-10 19:31:28,947 - backend - INFO - Health check result: \r\nOut of 1, 0 are good, 1 are corrupt and 0 need manual inspection\r\n2025-01-10 19:31:28,947 - backend - INFO - Book 0335e2f8e48c0d3757b9065f6cb804ff download failed\r\n```\r\n\r\nHaven't found the tmp folder yet, so can't determine if it's actually corrupt or not.", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/52/comments", "author": "gmeijers", "comments": [{"user": "calibrain", "created_at": "2025-01-10T20:03:57Z", "body": "If the book is corrupted, i will automatically delete it\r\nSo, I just tried for the specific book you downloaded, and it seems fine\r\n\r\nThis might be actually an issue moving the book to $INGEST_DIR (which by default is `/cwa-book-ingest`)\r\nI would bet its a permission issue\r\nTry to go in the docker and write to that folder and see if you are getting any errors ?\r\n\r\nBasically the logic is :\r\nDownload to a temp folder\r\nConvert to Epub with output to actual folder \r\n(even if its an epub, we still convert it to epub to test if its properly formatted. This will be dropped in the next release since CWA will be dooing that instead)\r\n\r\nI guess during the conversion process, the script is unable to write to /cwa-book-ingest for some reason (try chmod 777 or mount another folder instead )\r\n"}, {"user": "gmeijers", "created_at": "2025-01-11T08:03:45Z", "body": "You're right, it was a permission issue. It's solved, and it's probably me but I still don't completely get it:\r\n\r\n- I can touch test.epub in tmp/cwa-book-downloader\r\n- I can mv the .epub to cwa-book-ingest\r\n- Problem when running the program ¯\\_(ツ)_/¯\r\n\r\nI chmod cwa-book-ingest to 777, now everything is fine until I redeploy the container."}, {"user": "calibrain", "created_at": "2025-01-11T08:13:19Z", "body": "It seems to be a permission issue with the folder\r\nYou can either run as root (set UID and GID as 0)\r\nOr delete the mounted folder from your computer and let docker automatically create it in the docker compose"}], "satisfaction_conditions": ["A solution to the file permission issues preventing successful book processing", "An explanation of why manually moving files works but the program fails", "A persistent solution that survives container redeployment"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:51"}, "dockerfile": null, "language": "python"}
{"number": 42, "title": "No Log file?", "created_at": "2025-01-07T13:45:46Z", "closed_at": "2025-01-07T19:04:51Z", "commit_id": "348e5c925ba799601b10e745df4dc38f37403c38", "labels": [], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/42", "body": "Hello,\r\n\r\nI've ran the docker from ghcy and I found out I don't have any log in /var/logs, is that normal? The folder is empty. I tried deleting the folder and on next run, it create it back but still empty. I did change UID to 99 (the image do throw a warning because it's below 1000 but that's required for unraid) but even reverting to 1000 doesn't fix it.\r\n\r\nThank you", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/42/comments", "author": "nodiaque", "comments": [{"user": "calibrain", "created_at": "2025-01-07T19:05:24Z", "body": "Oups, you are right, I was never writing to it\r\nI fixed it, can you repull and retry now ?"}, {"user": "nodiaque", "created_at": "2025-01-07T19:34:24Z", "body": "Docker fail to start\r\n\r\nPermissionError: [Errno 13] Permission denied: '/var/log/cwa-book-downloader'"}, {"user": "nodiaque", "created_at": "2025-01-07T19:37:15Z", "body": "I think it's suppose to be /var/logs/? missing s I think."}, {"user": "nodiaque", "created_at": "2025-01-07T19:39:42Z", "body": "I made it work by mapping a path\r\n/var/log/cwa-book-downloader to a path on my guess. But I think it fail to create the folder else since that folder belong to root."}, {"user": "calibrain", "created_at": "2025-01-07T20:31:36Z", "body": "Oh by bad, I had an uncommitted change to create the folder !\r\nThank you for the heads up\r\nits now fixed"}], "satisfaction_conditions": ["A working log file system in the Docker container", "Proper permissions for the log directory", "Compatibility with custom UID settings", "Clear documentation about log file locations"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:15:56"}, "dockerfile": null, "language": "python"}
{"number": 35, "title": "Improve Mobile Usability, pan right in results to see the buttons", "created_at": "2025-01-04T15:29:51Z", "closed_at": "2025-01-06T16:07:14Z", "commit_id": "268dac069a52db18c92391ab3ea401ded1fc2eed", "labels": [], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/35", "body": "I know designing for mobile is probably a huge can of worms.  But there are times when using my phone is more convenient, and when I try to use it I can initiate a search which is great.  But the results are usually unusable, since I cannot fully pan over to the right where the buttons are.  If even that one piece could somehow be fixed when using the mobile, being able to click detail/download or fully read the results, that would be great.", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/35/comments", "author": "yroyathon", "comments": [{"user": "99brae", "created_at": "2025-01-04T17:35:21Z", "body": "Ran into this issue as well. Should be ideally a one/few line change. "}, {"user": "cthorne", "created_at": "2025-01-04T23:29:57Z", "body": "Try repulling your docker image, I submitted (and had approved) a PR to resolve this."}, {"user": "yroyathon", "created_at": "2025-01-05T00:02:25Z", "body": "Hooray it's fixed!  I'm on iPhone mobile/safari.  Thank you, this is great! "}], "satisfaction_conditions": ["Enable full horizontal scrolling/panning in mobile view to access UI elements on the right side", "Make detail/download buttons accessible on mobile devices", "Ensure search results are fully readable on mobile devices", "Solution works on iPhone/Safari mobile browser"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:01"}, "dockerfile": null, "language": "python"}
{"number": 12, "title": "Best approach to change ports?", "created_at": "2024-12-19T20:12:20Z", "closed_at": "2024-12-20T02:24:32Z", "commit_id": "06f654f4bef1ad7f20e044c21dc4049b7cf74365", "labels": ["bug"], "url": "https://github.com/calibrain/calibre-web-automated-book-downloader/issues/12", "body": "Wondering best approach to remap ports for both the main app and the cloudflare proxy?  Tried using the ENV variables, that didn't work (still used 8084 and 8000), tried remapping ports directly and that seemed to work but couldn't connect to the Cloudflare proxy w/localhost or got errors using an IP instead (connection refused).  \r\n\r\nGuessing I'm just doing something incorrect, but would see shifting ports as a pretty big need for many.", "comments_url": "https://api.github.com/repos/calibrain/calibre-web-automated-book-downloader/issues/12/comments", "author": "necromancyr", "comments": [{"user": "calibrain", "created_at": "2024-12-19T21:56:59Z", "body": "You are totally correct\r\nI messed up my docker compose,\r\nIts fixed now, see #13"}, {"user": "necromancyr", "created_at": "2024-12-19T22:49:03Z", "body": "To map the ports for cloudflarebypassforscraping service should just add port maps under there, correct?  That's the other part - both ports need to be modifiable.  "}, {"user": "calibrain", "created_at": "2024-12-19T22:53:29Z", "body": "Sadly, cloudflarebypassforscraping port can't be changed, it's hardcoded from their service\r\nBut it shouldn't matter, since we are not exposing it\r\nIts only used internally, and by hostname so you can have another service in your compose using the same port and it will work fine\r\n\r\nWhy are you trying to change the port ?"}, {"user": "necromancyr", "created_at": "2024-12-20T01:40:53Z", "body": "Thanks - you answered my question.  I was overthinking it and trying to remap more than I needed to.  Thanks!  This is addressed and working great!  (Now I just need a UI element in CWA to link to this! :))\r\n"}, {"user": "calibrain", "created_at": "2024-12-20T02:24:32Z", "body": "I am talking with the creator of CWA about that :P \r\nFor now I have a hack where I apply inject the button in the HTML before spinning the CWA docker, I might clean it up a bit and share it next week"}], "satisfaction_conditions": ["Clarification on which ports need to be remapped and which cannot be changed", "Understanding of how the internal port mapping works with the cloudflare proxy service", "A working configuration that allows the user to run the service without port conflicts"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:16:06"}, "dockerfile": null, "language": "python"}
{"number": 94, "title": "fix the missing Chinese voices", "created_at": "2025-02-06T03:16:09Z", "closed_at": "2025-02-09T09:03:41Z", "commit_id": "ad352fe8f542799830d9ecc65999ad1e2684618b", "labels": [], "url": "https://github.com/thewh1teagle/kokoro-onnx/pull/94", "body": "## Description\r\n\r\nThe voices page can not show all of the voices, so the fetch_voices.py can not fetch all voices.\r\nThe four Chinese male voices are missing.\r\nSo I change another page to fetch the voices.\r\n", "comments_url": "https://api.github.com/repos/thewh1teagle/kokoro-onnx/issues/94/comments", "author": "qt06", "comments": [{"user": "thewh1teagle", "created_at": "2025-02-07T14:38:23Z", "body": "The regex looks too sensitive "}, {"user": "thewh1teagle", "created_at": "2025-02-09T09:03:41Z", "body": "Fixed by using huggingface API\r\nThanks!"}], "satisfaction_conditions": ["A solution that successfully retrieves all Chinese voices that were previously missing", "A more reliable method for fetching voice data that doesn't depend on regex parsing of web pages", "A complete solution that ensures all available voices can be fetched"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:34"}, "dockerfile": null, "language": "python"}
{"number": 71, "title": "Switch from json file to bin clarification ", "created_at": "2025-01-24T08:36:09Z", "closed_at": "2025-01-24T11:07:35Z", "commit_id": "57f8393345c05e9ba08572db8ed5d04ee0e1f7da", "labels": [], "url": "https://github.com/thewh1teagle/kokoro-onnx/issues/71", "body": "Hi,\n\nwhat is the reason of switching to binary file? (voices.bin) you are loading it also without stating `allow_pickle=False`, this is a security concern. ", "comments_url": "https://api.github.com/repos/thewh1teagle/kokoro-onnx/issues/71/comments", "author": "stavsap", "comments": [{"user": "thewh1teagle", "created_at": "2025-01-24T09:56:23Z", "body": "I changed it to numpy npz file which is key value pairs where the values are numpy arrays since the json file weighted ~30-50MB and the bin file weight just ~5MB.\nallow_pickle=False is set to False by default in np.load"}, {"user": "stavsap", "created_at": "2025-01-24T11:07:35Z", "body": "thanks for the explanation"}], "satisfaction_conditions": ["An explanation of the technical rationale for switching from JSON to binary format", "Clarification about security considerations related to the file format change", "Information about the efficiency benefits of the new file format"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:41"}, "dockerfile": null, "language": "python"}
{"number": 22, "title": "Add create_stream_from_session please", "created_at": "2025-01-12T01:19:03Z", "closed_at": "2025-01-12T01:50:30Z", "commit_id": "303b054953f006356019f45e0f1e2f33aacbaf19", "labels": ["feature"], "url": "https://github.com/thewh1teagle/kokoro-onnx/issues/22", "body": "Since CPU is preferred over CUDA, a session is required to utilize GPU.\r\nBut then we can't create a stream with a session, as it's not a parameter for \"from_session\"\r\n\r\nStream support is a killer feature. It allows connecting Kokoro to the model output, streamlining the whole interaction.\r\n\r\nCan you please add \"session\" option to create_stream function?\r\nAlternatively, perhaps it is easier to implement an override to the session on 'self' or configure it on class instantiation.\r\n", "comments_url": "https://api.github.com/repos/thewh1teagle/kokoro-onnx/issues/22/comments", "author": "OriNachum", "comments": [{"user": "thewh1teagle", "created_at": "2025-01-12T01:24:35Z", "body": "The from_session is just a constructor / class method same as init.\r\nYou should create the instance with it\r\nThen you can create the stream with the create_stream method from the instance \r\nAlso you should call the constructor only once in the code"}, {"user": "OriNachum", "created_at": "2025-01-12T01:50:30Z", "body": "Got it - already supported. Thank you!"}], "satisfaction_conditions": ["Clarification on how to properly use stream functionality with a session", "Confirmation that the existing API already supports their use case", "Understanding the proper sequence of operations for creating and using streams"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:02:46"}, "dockerfile": null, "language": "python"}
{"number": 102, "title": "Ganerate multiple structures", "created_at": "2024-12-12T14:39:16Z", "closed_at": "2024-12-12T21:22:59Z", "commit_id": "c59aa53815c02f0028c812732a174b8422df6fa6", "labels": [], "url": "https://github.com/jwohlwend/boltz/issues/102", "body": "How can we generate more than one output prediction result?", "comments_url": "https://api.github.com/repos/jwohlwend/boltz/issues/102/comments", "author": "aggelos-michael-papadopoulos", "comments": [{"user": "jwohlwend", "created_at": "2024-12-12T18:48:38Z", "body": "Hi, yes there’s an option for this called --diffusion_samples"}, {"user": "aggelos-michael-papadopoulos", "created_at": "2024-12-12T19:18:27Z", "body": "Yep, discovered it earlier today. Many thanks!!!"}], "satisfaction_conditions": ["Information about a command-line option that allows generating multiple prediction outputs", "A concise, direct answer pointing to the specific parameter or flag needed"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:13:13"}, "dockerfile": null, "language": "python"}
{"number": 85, "title": "How to increase solution diversity?", "created_at": "2024-12-03T19:33:54Z", "closed_at": "2024-12-11T04:29:01Z", "commit_id": "0852c65300b7952fe9284beb5f3a62c6a8327ec0", "labels": [], "url": "https://github.com/jwohlwend/boltz/issues/85", "body": "Thank you making this tool open source. I am currently using it to predict protein-protein complex structures. For a particular pair of protein sequences and diffusion samples=25, I am getting 25 models that are very similar to each other. I already know roughly where the binding interface is and models made in the 25 solutions do not have the correct binding interface. Will changing the sampling_steps and/or recycling_steps be helpful?\r\n", "comments_url": "https://api.github.com/repos/jwohlwend/boltz/issues/85/comments", "author": "varunmc92", "comments": [{"user": "gcorso", "created_at": "2024-12-11T02:56:45Z", "body": "i've exposed the `step_scale` parameter for inference. By setting it lower (probably I would stay above 1) you can increase the diversity of the samples."}, {"user": "varunmc92", "created_at": "2024-12-11T04:29:01Z", "body": "Thank you so much"}], "satisfaction_conditions": ["A parameter or method to increase the diversity of predicted protein-protein complex structures", "A practical adjustment that can be made to the existing model parameters", "Guidance on appropriate parameter values to achieve the desired diversity"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:13:26"}, "dockerfile": null, "language": "python"}
{"number": 82, "title": "ModuleNotFoundError: No module named 'torch._six'", "created_at": "2024-12-03T03:01:25Z", "closed_at": "2024-12-05T02:37:06Z", "commit_id": "b62d7f7dbe83c0550ce293c6b9c76372a9390f9f", "labels": [], "url": "https://github.com/jwohlwend/boltz/issues/82", "body": "Hi @jwohlwend Great work! Becoming something big.\r\nDo you know the potential reason for No module named 'torch._six'?\r\n\r\n```\r\n(boltz_AF3) me@server:/data/123456/test/test_boltz-AF3/boltz$ boltz predict test.fasta --use_msa_server\r\nTraceback (most recent call last):\r\n  File \"/data/123456/test/test_localcolabfold/localcolabfold/colabfold-conda/bin/boltz\", line 5, in <module>\r\n    from boltz.main import cli\r\n  File \"/data/123456/test/test_boltz-AF3/boltz/src/boltz/main.py\", line 23, in <module>\r\n    from boltz.model.model import Boltz1\r\n  File \"/data/123456/test/test_boltz-AF3/boltz/src/boltz/model/model.py\", line 27, in <module>\r\n    from boltz.model.modules.confidence import ConfidenceModule\r\n  File \"/data/123456/test/test_boltz-AF3/boltz/src/boltz/model/modules/confidence.py\", line 12, in <module>\r\n    from boltz.model.modules.trunk import (\r\n  File \"/data/123456/test/test_boltz-AF3/boltz/src/boltz/model/modules/trunk.py\", line 13, in <module>\r\n    from boltz.model.layers.triangular_attention.attention import (\r\n  File \"/data/123456/test/test_boltz-AF3/boltz/src/boltz/model/layers/triangular_attention/attention.py\", line 22, in <module>\r\n    from boltz.model.layers.triangular_attention.primitives import (\r\n  File \"/data/123456/test/test_boltz-AF3/boltz/src/boltz/model/layers/triangular_attention/primitives.py\", line 30, in <module>\r\n    and importlib.util.find_spec(\"deepspeed.ops.deepspeed4science\") is not None\r\n  File \"/data/123456/test/test_localcolabfold/localcolabfold/colabfold-conda/lib/python3.10/importlib/util.py\", line 94, in find_spec\r\n    parent = __import__(parent_name, fromlist=['__path__'])\r\n  File \"/data/123456/test/test_localcolabfold/localcolabfold/colabfold-conda/lib/python3.10/site-packages/deepspeed/__init__.py\", line 16, in <module>\r\n    from .runtime.engine import DeepSpeedEngine, DeepSpeedOptimizerCallable, DeepSpeedSchedulerCallable\r\n  File \"/data/123456/test/test_localcolabfold/localcolabfold/colabfold-conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 24, in <module>\r\n    from deepspeed.runtime.utils import see_memory_usage, get_ma_status, DummyOptim\r\n  File \"/data/123456/test/test_localcolabfold/localcolabfold/colabfold-conda/lib/python3.10/site-packages/deepspeed/runtime/utils.py\", line 18, in <module>\r\n    from torch._six import inf\r\nModuleNotFoundError: No module named 'torch._six'\r\n\r\n```", "comments_url": "https://api.github.com/repos/jwohlwend/boltz/issues/82/comments", "author": "johnnytam100", "comments": [{"user": "jwohlwend", "created_at": "2024-12-04T17:17:49Z", "body": "Probably happens because you have an older deepspeed installed (note that deepspeed is not a requirement for boltz). I would recommend that you install boltz in a fresh environment. You can also try upgrading deepspeed and see if that helps."}, {"user": "johnnytam100", "created_at": "2024-12-05T02:37:03Z", "body": "Hi @jwohlwend , thanks for the comment and I confirmed it's a problem of my env."}], "satisfaction_conditions": ["Identification of the root cause of the 'torch._six' module error", "Practical suggestions for resolving the dependency conflict", "Clarification about package dependencies for the boltz tool"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:13:36"}, "dockerfile": null, "language": "python"}
{"number": 32, "title": "mmseqs2 server use and disclaimer", "created_at": "2024-11-20T19:22:52Z", "closed_at": "2024-11-21T03:50:47Z", "commit_id": "c9c271067899c2d343b9cdb0d8721ebb86c02836", "labels": [], "url": "https://github.com/jwohlwend/boltz/issues/32", "body": "Can you put a warning somewhere that the automatic MSA generation uses a server?  This is a pretty big gotcha for a lot of groups with sensitive information and companies. \r\n\r\nAlso, as an alternative, can you add the commands you used to generate the MSAs for command line use or have some additional documentation here?  I've seen some of it in the issues (and thank you for trying to keep up with them!), but I think having this explicitly spelled out and alternatives would be needed here. \r\n\r\nThanks. ", "comments_url": "https://api.github.com/repos/jwohlwend/boltz/issues/32/comments", "author": "jadolfbr", "comments": [{"user": "jwohlwend", "created_at": "2024-11-20T19:43:14Z", "body": "Hmm this is a good point, maybe the MSA construction should be a separate command to avoid this type of surprise. I'll think about the best approach. An explicit flag might make sense as well\r\n\r\nAnd sure, we use colabfold to create our MSA's for both training and inference. I'll make sure to add docs on that!"}, {"user": "jwohlwend", "created_at": "2024-11-20T19:50:47Z", "body": "Ok I've made it an opt-in feature!"}, {"user": "jadolfbr", "created_at": "2024-11-20T20:14:08Z", "body": "Great, thanks!  Definitely a surprise that is not a fun one.  This would be great.  We are also working on a cmd-line implantation and can send it when we have it. "}, {"user": "jwohlwend", "created_at": "2024-11-21T03:50:47Z", "body": "This is now the behavior in the v0.2.1 release."}, {"user": "jadolfbr", "created_at": "2024-11-22T23:07:49Z", "body": "Thank you!"}], "satisfaction_conditions": ["Clear warning or disclosure about server usage for MSA generation", "Making server-based MSA generation optional rather than automatic", "Documentation of commands used for MSA generation"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:13:54"}, "dockerfile": null, "language": "python"}
{"number": 92, "title": "微调后的模型推理结果与微调训练时的验证结果差异问题", "created_at": "2025-02-26T03:15:56Z", "closed_at": "2025-02-27T02:26:47Z", "commit_id": "0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b", "labels": [], "url": "https://github.com/Francis-Rings/StableAnimator/issues/92", "body": "你好，我想知道保存下来的其他模型比如model.safetensors、model_1、model_2等怎么用呢？\n还是说这些都是不需要的，重点只是checkpoint-{global_step}这三个模型？\n我在用微调后的三个模型进行推理时发现生成的动作与微调训练时同步数生成的验证gif文件的动作不一致。\n下述视频是运行推理代码得到的，大概由于gif文件太大，上传失败，我想告诉你的是微调训练时同步数生成的验证gif得到的手部是正常的，与下述视频不同。\n", "comments_url": "https://api.github.com/repos/Francis-Rings/StableAnimator/issues/92/comments", "author": "YxY-HK", "comments": [{"user": "Francis-Rings", "created_at": "2025-02-27T01:50:15Z", "body": "Hi, you only need to load `checkpoint-{global_step}/pose_net-{global_step}.pth`, `checkpoint-{global_step}/face_encoder-{global_step}.pth`, `checkpoint-{global_step}/unet-{global_step}.pth` into the StableAnimator for conducting inference. For more details, please refer to the README file."}, {"user": "YxY-HK", "created_at": "2025-02-27T02:26:47Z", "body": "Ok，Thx"}], "satisfaction_conditions": ["Clear guidance on which model files are necessary for inference", "Instructions on how to properly load the fine-tuned models for inference", "Reference to documentation for additional details"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:09:57"}, "dockerfile": "FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nENV DEBIAN_FRONTEND=noninteractive\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wget \\\n    curl \\\n    python3 \\\n    python3-pip \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1 \\\n    libglib2.0-0 \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN git clone https://github.com/Francis-Rings/StableAnimator.git . && \\\n    git checkout 0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b\n\n# Install Python dependencies with a timeout to avoid build hanging\nRUN pip3 install --no-cache-dir --timeout 100 torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 && \\\n    pip3 install --no-cache-dir --timeout 100 -r requirements.txt\n\n# Create necessary directories\nRUN mkdir -p checkpoints/DWPose checkpoints/Animation checkpoints/SVD models/antelopev2 && \\\n    mkdir -p animation_data/rec animation_data/vec validation/ground_truth validation/poses\n\n# Set environment variables\nENV PYTHONPATH=\"${PYTHONPATH}:/app\"\n\n# Set the default command\nCMD [\"echo\", \"StableAnimator environment is ready. You can run inference, training, or finetuning using the provided scripts.\"]", "language": "python"}
{"number": 85, "title": "Error Loading Model State Dict: Missing Keys in UNetSpatioTemporalConditionModel", "created_at": "2025-02-03T02:34:48Z", "closed_at": "2025-02-05T08:59:50Z", "commit_id": "0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b", "labels": [], "url": "https://github.com/Francis-Rings/StableAnimator/issues/85", "body": "**Description:**  \nAfter training the model using the provided training script, I encountered an error when trying to load the model for inference. The error indicates that several keys are missing from the state dict of the `UNetSpatioTemporalConditionModel`. It appears that there might be a mismatch between the trained model and the expected state dict keys during loading.\n\n**Error Message:**  \n```python\nunet_state_dict = torch.load(args.unet_model_name_or_path, map_location=\"cpu\")\nTraceback (most recent call last):\n  File \"/workspace/StableAnimator/inference_basic.py\", line 319, in <module>\n    unet.load_state_dict(unet_state_dict, strict=True)\n  File \"/workspace/StableAnimator/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\n    raise RuntimeError(\nRuntimeError: Error(s) in loading state_dict for UNetSpatioTemporalConditionModel:\n        Missing key(s) in state_dict: \"down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.id_to_v.weight\", \"mid_block.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\", \"mid_block.attentions.0.transformer_blocks.0.attn2.processor.id_to_v.weight\".\n```\n\n**Reproduction Steps:**  \n1. **Training:**  \n   The model was trained using the following bash command:\n   ```bash\n   CUDA_VISIBLE_DEVICES=3,7,6,5,4,2,1,0 accelerate launch train_single.py \\\n    --pretrained_model_name_or_path=\"path/checkpoints/stable-video-diffusion-img2vid-xt\" \\\n    --finetune_mode=True \\\n    --posenet_model_finetune_path=\"path/checkpoints/Animation/pose_net.pth\" \\\n    --face_encoder_finetune_path=\"path/checkpoints/Animation/face_encoder.pth\" \\\n    --unet_model_finetune_path=\"path/checkpoints/Animation/unet.pth\" \\\n    --output_dir=\"path/checkpoints/Animation2\" \\\n    --data_root_path=\"path/preprocess/\" \\\n    --data_path=\"path/preprocess/video_path.txt\" \\\n    --dataset_width=576 \\\n    --dataset_height=1024 \\\n    --validation_image_folder=\"path/validation/images\" \\\n    --validation_control_folder=\"path/validation/poses\" \\\n    --validation_image=\"path/validation/reference.png\" \\\n    --num_workers=8 \\\n    --lr_warmup_steps=500 \\\n    --sample_n_frames=8 \\\n    --learning_rate=5e-6 \\\n    --per_gpu_batch_size=1 \\\n    --num_train_epochs=600 \\\n    --mixed_precision=\"fp16\" \\\n    --gradient_accumulation_steps=1 \\\n    --checkpointing_steps=3000 \\\n    --validation_steps=9999999 \\\n    --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --enable_xformers_memory_efficient_attention \\\n    --checkpoints_total_limit=90000 \\\n    --resume_from_checkpoint=\"latest\"\n   ```\n\n2. **Loading:**  \n   After training, I attempted to load the model with the following code:\n   ```python\n   unet_state_dict = torch.load(args.unet_model_name_or_path, map_location=\"cpu\")\n   unet.load_state_dict(unet_state_dict, strict=True)\n   ```\n   This resulted in the error shown above.\n\n**Environment:**  \n- **Python:** 3.12.3\n- **PyTorch:** 2.5.1+cu124 \n- **Diffusers:** 0.32.1\n\n**Additional Context:**  \n- The error lists several missing keys in the state dict (e.g., `\"down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.id_to_k.weight\"`, etc.).\n- This issue may indicate a mismatch between the model architecture used during training and the one expected during inference.  \n- Has there been any recent change in the model structure or naming conventions that could lead to this issue?\n\nAny help or guidance in resolving this issue would be greatly appreciated.", "comments_url": "https://api.github.com/repos/Francis-Rings/StableAnimator/issues/85/comments", "author": "cvecve147", "comments": [{"user": "Francis-Rings", "created_at": "2025-02-04T13:12:13Z", "body": "Hi, please check whether AnimationIDAttnNormalizedProcessor is activated. It seems that the weights of AnimationIDAttnNormalizedProcessor were not saved during training."}, {"user": "cvecve147", "created_at": "2025-02-05T08:59:51Z", "body": "Thank you for your prompt response and valuable guidance. Upon further investigation, I discovered that the root cause of the issue was the use of the --enable_xformers_memory_efficient_attention parameter during training, which resulted in the AnimationIDAttnNormalizedProcessor weights not being saved correctly. After removing this parameter, the model weights are now saved and loaded properly. I greatly appreciate your support and insights in resolving this matter!"}], "satisfaction_conditions": ["Identification of the root cause of the missing keys in the model state dict", "A specific parameter or configuration causing the model loading issue", "A practical solution to resolve the model loading error"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:10:10"}, "dockerfile": "FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nENV DEBIAN_FRONTEND=noninteractive\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wget \\\n    curl \\\n    python3 \\\n    python3-pip \\\n    python3-dev \\\n    ffmpeg \\\n    libsm6 \\\n    libxext6 \\\n    libgl1 \\\n    libglib2.0-0 \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/Francis-Rings/StableAnimator.git . && \\\n    git checkout 0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b\n\n# Install PyTorch first\nRUN pip3 install --no-cache-dir torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install diffusers separately\nRUN pip3 install --no-cache-dir diffusers==0.32.1\n\n# Install other dependencies in batches\nRUN pip3 install --no-cache-dir numpy opencv-python pillow matplotlib tqdm scikit-image && \\\n    pip3 install --no-cache-dir transformers accelerate einops omegaconf && \\\n    pip3 install --no-cache-dir onnxruntime onnx insightface && \\\n    pip3 install --no-cache-dir ninja gradio==4.19.2 && \\\n    pip3 install --no-cache-dir bitsandbytes==0.41.3 xformers==0.0.23.post1\n\n# Create necessary directories for model checkpoints and data\nRUN mkdir -p checkpoints/DWPose \\\n    checkpoints/Animation \\\n    checkpoints/SVD/feature_extractor \\\n    checkpoints/SVD/image_encoder \\\n    checkpoints/SVD/scheduler \\\n    checkpoints/SVD/unet \\\n    checkpoints/SVD/vae \\\n    models/antelopev2 \\\n    animation_data/rec \\\n    animation_data/vec \\\n    validation/ground_truth \\\n    validation/poses\n\n# Create a file with guidance for the UNetSpatioTemporalConditionModel issue\nRUN echo \"To fix the UNetSpatioTemporalConditionModel state dict loading issue, try loading the model with strict=False or update the model architecture to match the trained weights. The missing keys are related to the transformer attention processors.\" > model_loading_fix.txt\n\nENV PYTHONPATH=\"${PYTHONPATH}:/app\"\n\nCMD [\"echo\", \"StableAnimator environment is ready. To address the model state dict loading issue, check model_loading_fix.txt for guidance.\"]", "language": "python"}
{"number": 64, "title": "关于微调", "created_at": "2025-01-02T02:28:01Z", "closed_at": "2025-01-02T07:26:50Z", "commit_id": "0f3d85ad217c0d3edec89e310bb34c3ecb9eaf9b", "labels": [], "url": "https://github.com/Francis-Rings/StableAnimator/issues/64", "body": "请问大佬\r\n使用微调后的 pose_net.pth、face_encoder.pth、unet.pth 去做推理，推理时参考图片随便选择一张人物图，为啥推理出来生成的动画/视频都还是微调时候的人物，似乎参考图片的人物图并没有生效\r\n请问这是为啥呢？", "comments_url": "https://api.github.com/repos/Francis-Rings/StableAnimator/issues/64/comments", "author": "Jeremy-J-J", "comments": [{"user": "Francis-Rings", "created_at": "2025-01-02T03:34:38Z", "body": "Hi, the plausible reason is the limited quality and diversity of your training dataset, as well as the potential for overfitting. You can use SVD to initialize StableAnimator and train it on your dataset to check whether the issue is related to dataset quality or overfitting."}, {"user": "Jeremy-J-J", "created_at": "2025-01-02T06:05:33Z", "body": "> Hi, the plausible reason is the limited quality and diversity of your training dataset, as well as the potential for overfitting. You can use SVD to initialize StableAnimator and train it on your dataset to check whether the issue is related to dataset quality or overfitting.\r\n\r\nHow can I implement the initialization of StableAnimator using SVD?"}, {"user": "Francis-Rings", "created_at": "2025-01-02T06:32:25Z", "body": "Please refer to the training tutorial in the README file.\r\n```\r\nbash command_train.sh\r\n```"}, {"user": "Jeremy-J-J", "created_at": "2025-01-02T06:41:54Z", "body": "> Please refer to the training tutorial in the README file.\r\n> \r\n> ```\r\n> bash command_train.sh\r\n> ```\r\n\r\nI compared `command_finetune.sh` and `command_train.sh`, the difference in using SVD initialization is only that the parameters `--posenet_model_finetune_path`, `--face_encoder_finetune_path`, `--unet_model_finetune_path`, and `--finetune_mode` are not passed. Is that all there is to it?\r\n"}, {"user": "Francis-Rings", "created_at": "2025-01-02T06:52:40Z", "body": "Yep."}, {"user": "Jeremy-J-J", "created_at": "2025-01-02T07:14:15Z", "body": "感谢大佬"}], "satisfaction_conditions": ["Confirmation of the correct approach to initialize StableAnimator using SVD", "Clear explanation of the difference between fine-tuning and training from scratch", "Guidance that addresses the overfitting problem in their model"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:10:18"}, "dockerfile": null, "language": "python"}
{"number": 45, "title": "the file \"inference\" cannot be opened", "created_at": "2024-12-17T02:33:53Z", "closed_at": "2024-12-17T03:17:36Z", "commit_id": "65a72f8702cb08160bcd1a23b4bbd8a6042487aa", "labels": [], "url": "https://github.com/Francis-Rings/StableAnimator/issues/45", "body": null, "comments_url": "https://api.github.com/repos/Francis-Rings/StableAnimator/issues/45/comments", "author": "sunjing1123", "comments": [{"user": "Francis-Rings", "created_at": "2024-12-17T02:48:33Z", "body": "Hi, I just downloaded the `inference.zip` file from the Hugging Face model and unzipped it locally. I was able to open the unzipped files successfully and have checked all the contents to ensure that no files are corrupted."}, {"user": "Francis-Rings", "created_at": "2024-12-17T02:51:36Z", "body": "Please check you have successfully downloaded the entire `inference.zip` file from the Hugging Face model."}, {"user": "sunjing1123", "created_at": "2024-12-17T03:32:01Z", "body": "Hi, yes, I've tried it and succeeded . thanks a lot \r\n"}], "satisfaction_conditions": ["Guidance on properly downloading and accessing the inference.zip file", "Verification that the inference files are accessible when properly downloaded", "Troubleshooting steps for file access problems"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:10:24"}, "dockerfile": null, "language": "python"}
{"number": 29, "title": "Training failed with error _pickle.UnpicklingError: pickle data was truncated", "created_at": "2024-12-13T03:47:00Z", "closed_at": "2024-12-16T02:59:49Z", "commit_id": "6b00adae112001e8f02cb673856585a4b4fcf8e5", "labels": [], "url": "https://github.com/Francis-Rings/StableAnimator/issues/29", "body": "I tried to run the training scripts but it failed with error\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/home/yaqing/miniconda3/envs/stableanimator/bin/python', 'train.py', '--pretrained_model_name_or_path=stabilityai/stable-video-diffusion-img2vid-xt', '--output_dir=/home/yaqing/ai/StableAnimator/checkpoints/Animation', '--data_root_path=/home/yaqing/ai/StableAnimator/animation_data', '--rec_data_path=/home/yaqing/ai/StableAnimator/animation_data/video_rec_path.txt', '--vec_data_path=/home/yaqing/ai/StableAnimator/animation_data/video_vec_path.txt', '--validation_image_folder=/home/yaqing/ai/StableAnimator/validation/ground_truth', '--validation_control_folder=/home/yaqing/ai/StableAnimator/validation/poses', '--validation_image=/home/yaqing/ai/StableAnimator/validation/reference.png', '--num_workers=8', '--lr_warmup_steps=500', '--sample_n_frames=16', '--learning_rate=1e-5', '--per_gpu_batch_size=1', '--num_train_epochs=6000', '--mixed_precision=fp16', '--gradient_accumulation_steps=1', '--checkpointing_steps=2000', '--validation_steps=500', '--gradient_checkpointing', '--checkpoints_total_limit=5000', '--resume_from_checkpoint=latest']' died with <Signals.SIGKILL: 9>.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/yaqing/miniconda3/envs/stableanimator/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"/home/yaqing/miniconda3/envs/stableanimator/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\n_pickle.UnpicklingError: pickle data was truncated\r\nAny idea what may be wrong?\r\n", "comments_url": "https://api.github.com/repos/Francis-Rings/StableAnimator/issues/29/comments", "author": "Yaqing2023", "comments": [{"user": "Francis-Rings", "created_at": "2024-12-13T05:17:46Z", "body": "Hi, I’ve never encountered this issue before. Based on the error message, it might be related to `spawn`. You could try modifying the multiprocessing method at Line 822 in `train.py`."}, {"user": "Yaqing2023", "created_at": "2024-12-13T06:06:04Z", "body": "yes I tried to change spawn to fork, the error is gone; also in the shell script  it has CUDA_VISIBLE_DEVICES=3,2,1,0, i suppose you have 4 GPU for training. this needs to be updated for actual GPU numbers user has?\r\nbut i still can not run the training on my single GPU machine with 16G memory, even though i have only 2 sub-dir to train 00001 and 00002. It still runs OOM"}, {"user": "Francis-Rings", "created_at": "2024-12-13T06:12:07Z", "body": "> yes I tried to change spawn to fork, the error is gone; also in the shell script it has CUDA_VISIBLE_DEVICES=3,2,1,0, i suppose you have 4 GPU for training. this needs to be updated for actual GPU numbers user has? but i still can not run the training on my single GPU machine with 16G memory, even though i have only 2 sub-dir to train 00001 and 00002. It still runs OOM\r\n\r\nI use 4 NVIDIA A100 80GB GPUs to train StableAnimator. The CUDA_VISIBLE_DEVICES variable specifies which GPUs are available for use. For example, if your machine has a single GPU, you should set CUDA_VISIBLE_DEVICES=0. Furthermore, I recommend using GPUs with at least 40GB of VRAM for training StableAnimator."}], "satisfaction_conditions": ["A solution to the pickle data truncation error during training", "Guidance on configuring GPU settings for the training environment", "Information about hardware requirements for successful training"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:10:32"}, "dockerfile": null, "language": "python"}
{"number": 43, "title": "Global Rules instead of .windsurfrules", "created_at": "2025-02-04T14:38:01Z", "closed_at": "2025-02-09T18:23:06Z", "commit_id": "69abe65f61f29f1d8f01c3257e311a5f009865d2", "labels": [], "url": "https://github.com/grapeot/devin.cursorrules/issues/43", "body": "Can we use Global Rules within windsurf (instead of .windsurfrules) for referencing the scratchpad.md file? I have a project specific .windsurfrules instructions, and given the character limitation for this file, I am unable to update this.", "comments_url": "https://api.github.com/repos/grapeot/devin.cursorrules/issues/43/comments", "author": "TinkererInChief", "comments": [{"user": "grapeot", "created_at": "2025-02-04T17:08:15Z", "body": "I think it should work for Windsurf (to put parts of the file into the .windsurfrules). Because it's using the scratchpad to do the planning anyway."}, {"user": "grapeot", "created_at": "2025-02-09T18:23:06Z", "body": "Closing due to lack of activity. But feel free to reopen it."}, {"user": "TinkererInChief", "created_at": "2025-02-10T04:35:11Z", "body": "Thanks for your cment but that didn't answer the question raised. I already have a boilerplate repo which has it's own set of instructions. Given that .windsurfrules has character limitations, I was exploring if we can shift your rules to \"Global Rules\" section in windsurf without creating any negative impact. Hope it's clearer now.\n\n"}, {"user": "grapeot", "created_at": "2025-02-10T04:43:35Z", "body": "Yeah the info is helpful! I think the answer is, it depends. I think moving the rules to the \"Global Rules\" section is a good idea that would work for your project. I don't see any issues for your specific project for now. The issue is that these instructions will impact all your projects since anything in the global rules section affects the entire Windsurf. If your intention is to have these Windsurf rules apply to every project, that's perfectly fine. However, if you only want these additional Windsurf rules from my repo to affect certain projects, it could cause side effects.\n\nOne alternative is to rename the Windsurf rules in my repository to another name and manually include it (using mention) when launching new cascade requests. You likely won't need to do this often because, once you include the file in the initial cascade conversation, Windsurf keeps it in the context. This could be a useful workaround."}, {"user": "TinkererInChief", "created_at": "2025-02-10T05:23:15Z", "body": "Thanks, this is helpful!"}], "satisfaction_conditions": ["Clear explanation of whether Global Rules can be used instead of .windsurfrules for referencing scratchpad.md", "Information about potential impacts or side effects of using Global Rules", "Alternative solutions to overcome the character limitations in .windsurfrules", "Addressing the specific context of having a project with its own set of instructions"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:07:11"}, "dockerfile": null, "language": "python"}
{"number": 720, "title": "modify the explanation of MLA", "created_at": "2025-02-26T09:08:31Z", "closed_at": "2025-04-08T09:20:37Z", "commit_id": "d29a967601cc772ede6c475870e3b591f2f89c45", "labels": [], "url": "https://github.com/deepseek-ai/DeepSeek-V3/pull/720", "body": null, "comments_url": "https://api.github.com/repos/deepseek-ai/DeepSeek-V3/issues/720/comments", "author": "xiaokongkong", "comments": [{"user": "musvaage", "created_at": "2025-03-02T17:19:30Z", "body": "inference/model.py\r\n\r\nfeasibly these lines should read\r\n\r\n```diff\r\n-     Multi-Headed Attention Layer (MLA).\r\n+     Multi-head Latent Attention (MLA) layer.\r\n```\r\n\r\n```diff\r\n-         Forward pass for the Multi-Headed Attention Layer (MLA).\r\n+         Forward pass for the Multi-head Latent Attention (MLA) layer.\r\n```"}, {"user": "GeeeekExplorer", "created_at": "2025-04-08T09:20:03Z", "body": "Thank your fix!"}], "satisfaction_conditions": ["Correction of terminology in documentation to accurately reflect the proper name of the attention mechanism", "Consistent naming convention throughout the codebase documentation"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:08:53"}, "dockerfile": null, "language": "python"}
{"number": 221, "title": "[Bug]: Module not found: Can't resolve 'crypto'", "created_at": "2025-03-16T01:18:42Z", "closed_at": "2025-03-20T05:35:01Z", "commit_id": "b7d15971e563b5b4ebccffc35bee9666aa4c926e", "labels": ["question"], "url": "https://github.com/lynx-family/lynx-stack/issues/221", "body": "### System Info\n\n  System:\n    OS: Windows 11 10.0.26100\n    CPU: (8) x64 Intel(R) Core(TM) Ultra 7 258V\n    Memory: 17.39 GB / 31.48 GB\n  Binaries:\n    Node: 23.5.0 - C:\\Program Files\\nodejs\\node.EXE\n    npm: 10.9.2 - C:\\Program Files\\nodejs\\npm.CMD\n    pnpm: 9.15.0 - ~\\.bun\\bin\\pnpm.EXE\n    bun: 1.1.33 - ~\\.bun\\bin\\bun.EXE\n  npmPackages:\n    @lynx-js/qrcode-rsbuild-plugin: ^0.3.4 => 0.3.4\n    @lynx-js/react: ^0.105.2 => 0.105.2\n    @lynx-js/react-rsbuild-plugin: ^0.9.2 => 0.9.2\n    @lynx-js/rspeedy: ^0.8.4 => 0.8.4\n    @lynx-js/types: ^3.2.0 => 3.2.0\n\n### Details\n\nbun run build fails with the following error\n\n```Tip: \"crypto\" is a built-in Node.js module. It cannot be imported in client-side code.\nCheck if you need to import Node.js module. If needed, you can use \"@rsbuild/plugin-node-polyfill\" to polyfill it.\n\nerror   Rspack build failed.\n    at file:///C:/Users/ancie/OneDrive/Documents/GitHub/veme-lynx/node_modules/@rsbuild/core/dist/index.js:6374:87\n    at finalCallback (C:\\Users\\ancie\\OneDrive\\Documents\\GitHub\\veme-lynx\\node_modules\\@rspack\\core\\dist\\index.js:14973:9)       \n    at C:\\Users\\ancie\\OneDrive\\Documents\\GitHub\\veme-lynx\\node_modules\\@rspack\\core\\dist\\index.js:15005:16\n    at done (C:\\Users\\ancie\\OneDrive\\Documents\\GitHub\\veme-lynx\\node_modules\\@rspack\\lite-tapable\\dist\\index.js:473:13)\n    at promise.then.index (C:\\Users\\ancie\\OneDrive\\Documents\\GitHub\\veme-lynx\\node_modules\\@rspack\\lite-tapable\\dist\\index.js:493:25)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\nerror: script \"build\" exited with code 1```\n\n### Reproduce link\n\n_No response_\n\n### Reproduce Steps\n\n1. bun install @aws-sdk/cloudfront-signer\n\n2. use anywhere in the app\n\n// API utility functions for VEME Lynx\nimport { getSignedUrl } from \"@aws-sdk/cloudfront-signer\";\n\n3. bun run build", "comments_url": "https://api.github.com/repos/lynx-family/lynx-stack/issues/221/comments", "author": "yungyoda", "comments": [{"user": "upupming", "created_at": "2025-03-17T02:42:08Z", "body": "Hi @yungyoda, as the error message says `Check if you need to import Node.js module. If needed, you can use \"@rsbuild/plugin-node-polyfill\" to polyfill it.`, `crypto` is a Node.js module, maybe you should use polyfill to bundle it correctly!"}, {"user": "yungyoda", "created_at": "2025-03-20T05:34:42Z", "body": "thanks @upupming the unobvious answer, using his point, was to add this to my lynx.config.ts\n\n` import { defineConfig } from '@lynx-js/rspeedy'\n\nimport { pluginQRCode } from '@lynx-js/qrcode-rsbuild-plugin'\nimport { pluginReactLynx } from '@lynx-js/react-rsbuild-plugin'\nimport { pluginNodePolyfill } from '@rsbuild/plugin-node-polyfill'\n\nexport default defineConfig({\n  plugins: [\n    pluginQRCode({\n      schema(url) {\n        // We use `?fullscreen=true` to open the page in LynxExplorer in full screen mode\n        return `${url}?fullscreen=true`\n      },\n    }),\n    pluginReactLynx(),\n    pluginNodePolyfill({\n      globals: {\n        Buffer: true,\n        // process: true,\n      },\n      protocolImports: true,\n    }),\n  ],\n})`"}], "satisfaction_conditions": ["A solution that resolves the 'Module not found: Can't resolve crypto' error when building the application", "Guidance on how to properly handle Node.js built-in modules in a client-side application", "Instructions for configuring the application to use appropriate polyfills for Node.js modules", "A solution compatible with their existing tech stack (Lynx.js, RSBuild, Bun)"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:43"}, "dockerfile": null, "language": "typescript"}
{"number": 73, "title": "直接喂给sonnet 3.7 普模会报错，甚至无法回复消息", "created_at": "2025-03-01T03:49:49Z", "closed_at": "2025-03-10T03:47:32Z", "commit_id": "7fc9948ab6e839f9e8a37b6017dffbd49670b0ce", "labels": [], "url": "https://github.com/richards199999/Thinking-Claude/issues/73", "body": "拿claude thinking直接喂给sonnet 3.7 普模会报错，甚至无法回复消息，有解决方案么，还能继续更新么", "comments_url": "https://api.github.com/repos/richards199999/Thinking-Claude/issues/73/comments", "author": "weidoesa", "comments": [{"user": "charliez0", "created_at": "2025-03-01T15:01:35Z", "body": "我实测没有任何问题啊👀\n"}, {"user": "richards199999", "created_at": "2025-03-10T03:47:32Z", "body": "@weidoesa \n对不起，之前因为个人的一些事情没有更新。\n目前Claude-3.7已经是thinking model了，API和Web端都可开启extended thinking mode（前者可以滑动调节思考长度）。\n所以目前如果Thinking Claude不能很好地工作的话，建议可以直接使用extended thinking，或者是用老版的较短的instruction（我在测试时发现那种更有效）。"}, {"user": "weidoesa", "created_at": "2025-03-13T14:37:26Z", "body": "谢谢！"}], "satisfaction_conditions": ["Information about why Claude Thinking models might not work properly with Sonnet 3.7", "Alternative approaches to achieve similar functionality", "Current status information about Claude-3.7's capabilities"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:50"}, "dockerfile": null, "language": "typescript"}
{"number": 33, "title": "cursor可以用吗", "created_at": "2024-11-20T08:38:47Z", "closed_at": "2024-11-20T10:37:18Z", "commit_id": "8b488ef5f431c959e009febd642a10d1580224c2", "labels": [], "url": "https://github.com/richards199999/Thinking-Claude/issues/33", "body": "我应该怎么把这个thinking claude的promat部署到我的cursor上去呢？", "comments_url": "https://api.github.com/repos/richards199999/Thinking-Claude/issues/33/comments", "author": "sjocnjfjd", "comments": [{"user": "lumpinif", "created_at": "2024-11-20T08:42:09Z", "body": "可以的，在cursor settings->Rules for AI复制粘贴进去，或者在项目根目录创建 `.cursorrules` 存储prompt"}, {"user": "sjocnjfjd", "created_at": "2024-11-20T08:47:54Z", "body": "\r\n非常感谢，已经可以思考了"}, {"user": "z1073", "created_at": "2024-11-21T13:10:16Z", "body": "> 非常感谢，已经可以思考了\r\n\r\n怎么样，cursor加了这个提示词效果如何\r\n"}, {"user": "sjocnjfjd", "created_at": "2024-11-21T18:43:28Z", "body": "> > 非常感谢，已经可以思考了\r\n> \r\n> 怎么样，cursor加了这个提示词效果如何\r\n\r\n可以思考，就是感觉思考的有点死板，它是按步骤一步一步来思考的"}, {"user": "z1073", "created_at": "2024-11-30T11:51:47Z", "body": "> > > 非常感谢，已经可以思考了\r\n> > \r\n> > \r\n> > 怎么样，cursor加了这个提示词效果如何\r\n> \r\n> 可以思考，就是感觉思考的有点死板，它是按步骤一步一步来思考的\r\n\r\n感觉加不加效果差不多，还是不加效率些"}], "satisfaction_conditions": ["Instructions for adding the 'thinking Claude' prompt to Cursor", "A method that enables the 'thinking' functionality in Cursor", "A straightforward implementation process that doesn't require complex setup"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:54"}, "dockerfile": null, "language": "typescript"}
{"number": 122, "title": "Pug syntax in template", "created_at": "2025-03-20T07:02:56Z", "closed_at": "2025-03-20T08:33:45Z", "commit_id": "4f2dcbaffaf2c3ea3961ee0ffc74a554d3b35855", "labels": [], "url": "https://github.com/motiondivision/motion-vue/issues/122", "body": "It seems impossible use motion.div in template with pug syntax. I have tried various combinations but without success.\nCan you show me the right way of doing this, or implement this feature in the future?\nThanks! \n", "comments_url": "https://api.github.com/repos/motiondivision/motion-vue/issues/122/comments", "author": "emptyfortress", "comments": [{"user": "rick-hup", "created_at": "2025-03-20T08:33:39Z", "body": "hi! @emptyfortress  Since .div gets compiled to a class prop, you can work around this by doing:\n```\n<script setup>\nconst Div = motion.div\n</script>\n\n<template lang=\"pug\">\n  Div()\n</template>\n```"}, {"user": "emptyfortress", "created_at": "2025-03-20T09:10:14Z", "body": "Thank you for such a quick response! It works like a charm."}], "satisfaction_conditions": ["A working syntax for using motion.div with Pug templates in Vue", "A straightforward workaround that doesn't require complex code changes", "A solution that maintains Pug syntax in templates"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:28"}, "dockerfile": null, "language": "typescript"}
{"number": 91, "title": "请教一个问题，为什么用户上传文件后，点击了发送，文件没有出现在用户的消息栏，要等最后ai输出完，获取消息列表才出现文件？", "created_at": "2025-03-28T03:31:04Z", "closed_at": "2025-03-29T13:26:39Z", "commit_id": "c2567db9717f4a1ab79ba30b3ab7db32f1d39832", "labels": ["bug"], "url": "https://github.com/lexmin0412/dify-chat/issues/91", "body": null, "comments_url": "https://api.github.com/repos/lexmin0412/dify-chat/issues/91/comments", "author": "bin-bin-6", "comments": [{"user": "lexmin0412", "created_at": "2025-03-29T13:26:39Z", "body": "Fixed in  926655477cb74d242c2d604db3279cfc3c8538cc"}, {"user": "bin-bin-6", "created_at": "2025-03-31T01:33:14Z", "body": "亲测可以，非常感谢"}], "satisfaction_conditions": ["Fix for the file upload display issue where uploaded files don't immediately appear in the user's message area", "Immediate display of uploaded files in the user interface after sending"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:49"}, "dockerfile": null, "language": "typescript"}
{"number": 77, "title": "ESM Module Error: @dmitryrechkin/json-schema-to-zod compatibility with agent-kit", "created_at": "2025-02-27T00:13:03Z", "closed_at": "2025-02-27T13:00:01Z", "commit_id": "e73e07e908946e21261e9abbc03e853a01ac6774", "labels": [], "url": "https://github.com/inngest/agent-kit/issues/77", "body": "## Bug Description\nWhen using the latest version of @inngest/agent-kit (v0.3.0) with Next.js, I'm encountering an ESM/CommonJS compatibility issue. The agent-kit package tries to import @dmitryrechkin/json-schema-to-zod using CommonJS require(), but that package is an ESM module.\n\nWhen using @inngest/agent-kit v0.2.2:\n\n ⨯ [Error: require() of ES Module /Users/ruby/code/nexus-workflow/node_modules/@dmitryrechkin/json-schema-to-zod/dist/index.js from /Users/ruby/code/nexus-workflow/node_modules/@inngest/agent-kit/dist/agent.js not supported.\nInstead change the require of index.js in /Users/ruby/code/nexus-workflow/node_modules/@inngest/agent-kit/dist/agent.js to a dynamic import() which is available in all CommonJS modules.] {\n  code: 'ERR_REQUIRE_ESM'\n}\n\n\n\n## Environment\n- Next.js: 15.1.3\n- @inngest/agent-kit: Tested v0.1.2 (works), v0.2.2 and v0.3.0 (both fail)\n- inngest: 3.31.11\n- React: 19.0.0\n- Node.js version: 20.15.1\n\n## Reproduction Steps\n1. Set up a Next.js project with dependencies as listed above\n2. Install @inngest/agent-kit v0.1.2 (works correctly)\n3. Upgrade to @inngest/agent-kit v0.2.2 or v0.3.0\n4. Run the development server (npm run dev)\n5. The server fails with ESM/CommonJS compatibility errors\n\n## Attempted Solutions\nI've tried various workarounds including:\n- Adding transpilePackages: ['@inngest/agent-kit', '@dmitryrechkin/json-schema-to-zod'] to next.config.js\n- Setting experimental.esmExternals to 'loose' in next.config.js\n- Creating a bridge module that avoids using agent-kit directly and falls back to inngest.step.ai.infer()\n- Modifying webpack configuration to handle ESM modules\n- Downgrading from v0.3.0 to v0.2.2 (but encountered similar errors with pkce-challenge)\n\nNone of these solutions have fully resolved the issue. The only version that works correctly is v0.1.2, but it lacks the newer features I need.\n\n## Suggested Fix\nThe agent-kit package should be updated to:\n1. Use dynamic import() instead of require() when importing ESM modules\n2. Provide a compatibility layer for both ESM and CommonJS environments\n3. Update dependencies to versions that support dual module systems\n\nThis issue affects the usability of agent-kit in Next.js projects, which is a common use case for Inngest functions.", "comments_url": "https://api.github.com/repos/inngest/agent-kit/issues/77/comments", "author": "eraykeskinmac", "comments": [{"user": "eraykeskinmac", "created_at": "2025-02-27T00:13:56Z", "body": "now package.json \n\n```\n{\n  \"name\": \"nexus-workflow\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\",\n    \"ingest\": \"inngest dev\"\n  },\n  \"dependencies\": {\n    \"@deepgram/sdk\": \"^3.9.0\",\n    \"@inngest/agent-kit\": \"^0.2.2\",\n    \"@types/dotenv\": \"^8.2.3\",\n    \"@vercel/blob\": \"^0.27.0\",\n    \"axios\": \"^1.7.9\",\n    \"date-fns\": \"^4.1.0\",\n    \"dotenv\": \"^16.4.7\",\n    \"inngest\": \"^3.31.11\",\n    \"inngest-cli\": \"^1.4.8\",\n    \"libphonenumber-js\": \"^1.11.17\",\n    \"next\": \"15.1.3\",\n    \"react\": \"^19.0.0\",\n    \"react-dom\": \"^19.0.0\"\n  },\n  \"devDependencies\": {\n    \"@eslint/eslintrc\": \"^3\",\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^19\",\n    \"@types/react-dom\": \"^19\",\n    \"eslint\": \"^9\",\n    \"eslint-config-next\": \"15.1.3\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.4.1\",\n    \"typescript\": \"^5\"\n  }\n}\n```\n\nold package.json\n\n```\n{\n  \"name\": \"nexus-workflow\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\",\n    \"ingest\": \"inngest dev\"\n  },\n  \"dependencies\": {\n    \"@deepgram/sdk\": \"^3.9.0\",\n    \"@inngest/agent-kit\": \"^0.1.2\",\n    \"@types/dotenv\": \"^8.2.3\",\n    \"@vercel/blob\": \"^0.27.0\",\n    \"axios\": \"^1.7.9\",\n    \"date-fns\": \"^4.1.0\",\n    \"dotenv\": \"^16.4.7\",\n    \"inngest\": \"^3.28.0\",\n    \"inngest-cli\": \"^1.3.3\",\n    \"libphonenumber-js\": \"^1.11.17\",\n    \"next\": \"15.1.3\",\n    \"react\": \"^19.0.0\",\n    \"react-dom\": \"^19.0.0\"\n  },\n  \"devDependencies\": {\n    \"@eslint/eslintrc\": \"^3\",\n    \"@types/node\": \"^20\",\n    \"@types/react\": \"^19\",\n    \"@types/react-dom\": \"^19\",\n    \"eslint\": \"^9\",\n    \"eslint-config-next\": \"15.1.3\",\n    \"postcss\": \"^8\",\n    \"tailwindcss\": \"^3.4.1\",\n    \"typescript\": \"^5\"\n  }\n}\n```\n"}, {"user": "jpwilliams", "created_at": "2025-02-27T12:39:39Z", "body": "Thanks for the detailed report, @eraykeskinmac!\n\nCould you try using `@inngest/agent-kit@0.3.1`? This offers dual CJS and ESM builds so may resolve the issue immediately.\n\nIf not, we can ship another change to handle this case."}, {"user": "eraykeskinmac", "created_at": "2025-02-27T12:50:03Z", "body": "Thanks! I tried the @inngest/agent-kit@0.3.1 version and it worked perfectly without any issues. This solution was exactly what I needed. I was looking to use Agent Kit and Inngest's new features, especially the AI inference capability, so this fix was really valuable for me. Thank you for your help!"}, {"user": "jpwilliams", "created_at": "2025-02-27T13:00:01Z", "body": "Awesome! Thanks for testing and glad it's looking good, @eraykeskinmac! 🙌 \n\ncc @MonsterDeveloper - thanks for the PR 🙂 "}], "satisfaction_conditions": ["A version of @inngest/agent-kit that resolves the ESM/CommonJS compatibility issue", "Access to newer features of the agent-kit package", "Compatibility with Next.js projects", "A solution that doesn't require complex workarounds"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:19"}, "dockerfile": null, "language": "typescript"}
{"number": 5, "title": "Unable to open database at first run", "created_at": "2025-02-18T10:17:47Z", "closed_at": "2025-02-23T22:13:08Z", "commit_id": "326f7d44c52ae96e5199671fc06784acd052e674", "labels": [], "url": "https://github.com/dotnetfactory/fluid-calendar/issues/5", "body": "Hi,\n\nI try to install you software, but I have a problem at first start. I wanted to use it with docker.\nI followed the documentation, but at the step, where I have to migrate the database, I have an error:\n\nI run this command: \n\n```\ndocker run --rm \\\n  -v $(pwd)/data:/app/data \\\n  --env-file .env \\\n  eibrahim/fluid-calendar:latest \\\n  npx prisma migrate deploy\n\n```\n\nAnd get this error message: \n\n```\nid-calendar:latest   npx prisma migrate deploy\nPrisma schema loaded from prisma/schema.prisma\nDatasource \"db\": SQLite database \"dev.db\" at \"file:/app/data/dev.db\"\n\nError: Schema engine error:\nSQLite database error\nunable to open database file: /app/data/dev.db\n```\n\nI also tried to run: \n\n```\nrm -rf data/* && docker run --rm \\\n  -v $(pwd)/data:/app/data \\\n  --env-file .env \\\n  eibrahim/fluid-calendar:latest \\\n  npx prisma migrate deploy\n```\n\nBut I got same error above. \n\n\nUpdate1: I created an empty file in the data folder (touch dev.db), and re-run the database migration command, but still dont work:\n\n```\nPrisma schema loaded from prisma/schema.prisma\nDatasource \"db\": SQLite database \"dev.db\" at \"file:/app/data/dev.db\"\n\n8 migrations found in prisma/migrations\n\nError: SQLite database error\nattempt to write a readonly database\n   0: sql_schema_connector::sql_migration_persistence::initialize\n           with namespaces=None\n             at schema-engine/connectors/sql-schema-connector/src/sql_migration_persistence.rs:14\n   1: schema_core::state::ApplyMigrations\n             at schema-engine/core/src/state.rs:226\n```\n\nI tried everything, run docker with sudo and root user, change the permission of dev.db with chmod to 777, change the owner of the dev.db from user to root, but still read only. ", "comments_url": "https://api.github.com/repos/dotnetfactory/fluid-calendar/issues/5/comments", "author": "bttd", "comments": [{"user": "MooRogue", "created_at": "2025-02-20T00:23:43Z", "body": "I ran into the same problem and had to change the **directory** which would store dev.db to 777 to allow the initial dev.db file to be written"}, {"user": "bttd", "created_at": "2025-02-20T09:05:14Z", "body": "> I ran into the same problem and had to change the **directory** which would store dev.db to 777 to allow the initial dev.db file to be written\n\nThanks!\n\nThat's work. But I think this need to be addressed. \n"}, {"user": "Garougamesh", "created_at": "2025-02-23T09:14:10Z", "body": "Doesn't work for me, whatever I try the database can't be written to, or even created. Never had any problem like this with any other docker containers. Commands to reset db need to be changed too because it gets called from app folder while trying to use the .env file which is one folder higher. Directory structure makes no sense anyway, why not put everything in data. Why wouldn't I have permission to write a file to a folder I just created? Why do I have to convert a Docker run command when you could easily have written a compose file. Wasted 2 hours of my time on this."}, {"user": "eibrahim", "created_at": "2025-02-23T22:14:05Z", "body": "I made some updates.  It's a lot easier to run now... all you have to do is run `docker compose up` see readme for more details.  I also switched to postgresql, so you will lose your data... but you can run `node migrate.js` to move your data from sqlite to postgres"}], "satisfaction_conditions": ["A solution that resolves the database permission issues when running the application in Docker", "A simpler, more streamlined Docker setup process", "Clear documentation on Docker deployment requirements", "A solution that maintains data persistence across Docker container restarts"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 00:59:19"}, "dockerfile": "FROM node:20-alpine\n\n# Set working directory\nWORKDIR /app\n\n# Install git and other dependencies\nRUN apk add --no-cache git\n\n# Clone the repository at the specific commit\nRUN git clone https://github.com/dotnetfactory/fluid-calendar.git . && \\\n    git checkout 326f7d44c52ae96e5199671fc06784acd052e674\n\n# Install dependencies\nRUN npm ci\n\n# Generate Prisma Client\nRUN npx prisma generate\n\n# Create data directory with proper permissions\nRUN mkdir -p /app/data && \\\n    chmod -R 777 /app/data\n\n# Build the application\nRUN npm run build\n\n# Ensure SQLite database directory has correct permissions\nRUN touch /app/data/dev.db && \\\n    chmod 666 /app/data/dev.db && \\\n    chmod -R 777 /app/data\n\n# Expose port 3000\nEXPOSE 3000\n\n# Define the command to run the application\nCMD [\"npm\", \"start\"]", "language": "typescript"}
{"number": 6, "title": "Not starting", "created_at": "2025-02-06T12:41:06Z", "closed_at": "2025-02-07T16:46:24Z", "commit_id": "b1b26a8ab940d4a9a5134e84b8dc733a609c6070", "labels": [], "url": "https://github.com/dzhng/deep-research/issues/6", "body": "Hi, I get \n`> open-deep-research@0.0.1 start\n> tsx --env-file=.env.local src/run.ts` on start and it exits (on Windows)", "comments_url": "https://api.github.com/repos/dzhng/deep-research/issues/6/comments", "author": "korzen", "comments": [{"user": "dzhng", "created_at": "2025-02-06T17:37:57Z", "body": "what environment are you running this in?"}, {"user": "UOW37", "created_at": "2025-02-07T14:30:40Z", "body": "You may want to upgrade your Node.js to the latest version or to a version that supports dotenv out of the box."}, {"user": "dzhng", "created_at": "2025-02-07T16:46:38Z", "body": "yea check you're running >node 22 pls"}, {"user": "korzen", "created_at": "2025-02-07T20:12:08Z", "body": "OK, it worked! However I see that the code is hardcoded to o3-mini and, for some reason, I don't have access to it in  OpenAI's API."}], "satisfaction_conditions": ["Information about Node.js version requirements for the application", "A solution that allows the application to properly start and run on Windows", "Guidance on environment configuration for the application"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:25"}, "dockerfile": null, "language": "typescript"}
{"number": 95, "title": "节点不使用表单的话，应该怎么设置和调整，base-node该如何调整", "created_at": "2025-03-26T08:26:11Z", "closed_at": "2025-03-26T08:58:21Z", "commit_id": "62e53350f1ff60b12ca3a0d6660d3bf463e7f8b0", "labels": [], "url": "https://github.com/bytedance/flowgram.ai/issues/95", "body": "文档只写了进阶的表单调用，但并没有写不使用表单的写法，希望能补充", "comments_url": "https://api.github.com/repos/bytedance/flowgram.ai/issues/95/comments", "author": "brysonLin247", "comments": [{"user": "xiamidaxia", "created_at": "2025-03-26T08:38:13Z", "body": "表单目前主要用来处理 节点的 data 字段，可以看下 demo-free-layout-simple 这个例子\n\n如果不想用表单\n1, formMeta.render 配置改成 null，因为不想用渲染\n2，可以在 节点渲染里。不调用 form.render,  然后通过 form.getValueIn 和 form.setValueIn 去修改节点的 data 数据（看你们是否有 data 字段）\n\n```\n      getNodeDefaultRegistry(type) {\n        return {\n          type,\n          formMeta: {\n            render: () => null, // 改成 null\n          },\n        };\n        /**\n         * Render Node\n         */\n        renderDefaultNode: (props: WorkflowNodeProps) => {\n          const { form } = useNodeRender();\n          return (\n            <WorkflowNodeRenderer className=\"demo-free-node\" node={props.node}>\n              {form?.render()} // 不用这个\n            </WorkflowNodeRenderer>\n          );\n        },\n\n```"}, {"user": "brysonLin247", "created_at": "2025-03-26T08:49:04Z", "body": "十分感谢！"}], "satisfaction_conditions": ["Instructions on how to configure nodes without using forms", "Clear steps for modifying the node configuration to disable form rendering", "Alternative approaches for handling node data without forms", "Code examples or references to demonstrate the non-form approach"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:29"}, "dockerfile": null, "language": "typescript"}
{"number": 63, "title": "Troubles using new SSE", "created_at": "2025-03-27T23:34:38Z", "closed_at": "2025-03-27T23:56:46Z", "commit_id": "b477b7c26f35b67a76459ad7e98e95c6946aa28f", "labels": [], "url": "https://github.com/microsoft/playwright-mcp/issues/63", "body": "Can't think you guys enough for this work !! \n\nI tried the new SSE support and it fails because --port is unknown\n\n```\nnpx @playwright/mcp:latest _V\nVersion 0.0.5\nnpx @playwright/macp:latest --port 8931\nerror: unknown option '--port'\n```\n\nI looked into the feature, and tried to replicate the cli.js -- module not found\n\n```\nnode cli.js --port 1234\nError: Cannot find module './lib/program'\nRequire stack:\n- /Users/ajoslin/Desktop/Development/playwright-mcp/cli.js\n    at Function._resolveFilename (node:internal/modules/cjs/loader:1225:15)\n    at Function._load (node:internal/modules/cjs/loader:1055:27)\n    at TracingChannel.traceSync (node:diagnostics_channel:322:14)\n    at wrapModuleLoad (node:internal/modules/cjs/loader:220:24)\n    at Module.require (node:internal/modules/cjs/loader:1311:12)\n    at require (node:internal/modules/helpers:136:16)\n    at Object.<anonymous> (/Users/ajoslin/Desktop/Development/playwright-mcp/cli.js:18:1)\n    at Module._compile (node:internal/modules/cjs/loader:1554:14)\n    at Object..js (node:internal/modules/cjs/loader:1706:10)\n    at Module.load (node:internal/modules/cjs/loader:1289:32) {\n  code: 'MODULE_NOT_FOUND',\n  requireStack: [ '/Users/ajoslin/Desktop/Development/playwright-mcp/cli.js' ]\n}\n\nNode.js v22.14.0\n```\nI'm off the try that `mcp-playwright-cdp`\n\nCheers!\nAl;\n", "comments_url": "https://api.github.com/repos/microsoft/playwright-mcp/issues/63/comments", "author": "ajoslin103", "comments": [{"user": "medioxor", "created_at": "2025-03-27T23:48:38Z", "body": "i am also running into this issue"}, {"user": "pavelfeldman", "created_at": "2025-03-27T23:56:46Z", "body": "You folks are too quick, I just published v0.0.6 that has it."}, {"user": "medioxor", "created_at": "2025-03-27T23:58:17Z", "body": "amazing work @pavelfeldman !!!\n"}, {"user": "ajoslin103", "created_at": "2025-03-28T00:11:38Z", "body": "I've been trying to get it running in n8n\n\nif you `cd ~/.n8n/nodes/` and `npm install playwright-mcp`\n\nThen you can connect in and STDIO session using \n\nnpx & @playwright/macp:latest\n\nAnd then is Glorious !!\n\n(the mcp-playwright-cdp didn;t work for me)"}, {"user": "ajoslin103", "created_at": "2025-03-28T00:13:36Z", "body": "I love this new world of mcp !!!  (or I will 15mins ago)  🤣\n"}, {"user": "ajoslin103", "created_at": "2025-03-28T13:54:04Z", "body": "The new SSE is working Perfectly !!\n\nYou guys Rock!!"}, {"user": "ajoslin103", "created_at": "2025-04-04T03:26:35Z", "body": "Closing with complete success!!"}], "satisfaction_conditions": ["Access to a working version of the SSE feature in the Playwright MCP tool", "Clear command-line instructions for using the SSE functionality", "Compatibility with their specific environment (n8n)", "A timely update or fix to the reported issue"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:43"}, "dockerfile": null, "language": "typescript"}
{"number": 7, "title": "Playwright Version Mismatch Error `(428 Precondition Required)`", "created_at": "2025-03-24T01:46:41Z", "closed_at": "2025-03-28T18:41:00Z", "commit_id": "dc7a449e8a0ebe8726213e617f143f5a3163c2fe", "labels": [], "url": "https://github.com/microsoft/playwright-mcp/issues/7", "body": "**Description:**\nWhen connecting to the Playwright MCP server, I encountered a `428 Precondition Required` error due to a version mismatch between the server and client:\n\n**Error details:**\n```\nError: browserType.connect: WebSocket error: ws://localhost:59985/ 428 Precondition Required\n╔════════════════════════════════════════════════════╗\n║ Playwright version mismatch:                       ║\n║   - server version: v1.51                          ║\n║   - client version: v1.52                          ║\n║                                                    ║\n║ If you are using VSCode extension, restart VSCode. ║\n║                                                    ║\n║ If you are connecting to a remote service,         ║\n║ keep your local Playwright version in sync         ║\n║ with the remote service version.                   ║\n║                                                    ║\n║ <3 Playwright Team                                 ║\n╚════════════════════════════════════════════════════╝\n```\n\n**Steps to Reproduce:**\n1. Start Playwright server using:\n   ```\n   npx playwright@latest run-server\n   ```\n   Output:\n   ```\n   Listening on ws://localhost:59985/\n   ```\n\n2. Configure MCP client with the following:\n   ```json\n   {\n     \"mcpServers\": {\n       \"playwright\": {\n         \"command\": \"npx\",\n         \"args\": [\"@playwright/mcp@latest\"],\n         \"env\": {\n           \"PLAYWRIGHT_WS_ENDPOINT\": \"ws://localhost:59985/\"\n         }\n       }\n     }\n   }\n   ```\n\n3. Attempt connection; observe the version mismatch error.\n\n**Expected behavior:**\nSuccessful connection without version mismatch error.\n\n**Workaround Attempted:**\nPinning both server and client explicitly to the same version (`v1.51` or `v1.52`) does **not** resolve the issue.\n\n**Environment:**\n- Playwright MCP client version: `v1.52`\n- Playwright server version: `v1.51`\n- OS/environment details (optional): [Add if relevant]\n\n**Suggested Fix:**\nInvestigate internal compatibility handling or provide explicit documentation on resolving server-client mismatches beyond simple version pinning.\n\nThank you!\n\n", "comments_url": "https://api.github.com/repos/microsoft/playwright-mcp/issues/7/comments", "author": "yottahmd", "comments": [{"user": "hanchuanjun", "created_at": "2025-03-24T05:30:52Z", "body": "I'm experiencing the same problem."}, {"user": "Skn0tt", "created_at": "2025-03-24T07:33:36Z", "body": "Instead of `npx playwright@latest run-server`, try `npx playwright@1.51.0 run-server`."}, {"user": "yottahmd", "created_at": "2025-03-24T10:36:28Z", "body": "I'm running the server on version v1.51.0, but the client is using v1.52.0."}, {"user": "yottahmd", "created_at": "2025-03-24T10:40:07Z", "body": "Playwright hasn't released version 1.52.0 yet.\n\nWorkaround:\n```sh\nnpx playwright@1.52.0-alpha-2025-03-21 run-server\n```"}, {"user": "pavelfeldman", "created_at": "2025-03-28T18:41:00Z", "body": "Check out the new README, we now recommend using MCP SSE to run browser remotely. Happy this works though!"}], "satisfaction_conditions": ["A solution that resolves the version mismatch error between Playwright server and client", "A specific command or configuration that allows the server and client to work together despite version differences", "Information about compatible version combinations for Playwright server and client", "A workaround that doesn't require downgrading the client version"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:50"}, "dockerfile": null, "language": "typescript"}
{"number": 45, "title": "--add-ons flag not working", "created_at": "2025-03-08T17:46:01Z", "closed_at": "2025-03-08T18:07:49Z", "commit_id": "a117c4d35395c361923df196c842793675fff4f1", "labels": [], "url": "https://github.com/TanStack/create-tsrouter-app/issues/45", "body": "### Which project does this relate to?\n\nCreate Tanstack App\n\n### Describe the bug\n\nHere's a few different logs showing the add ons flag not working quite right. I recall using it before recently and it worked so not sure what happened.\n\n```\nbunx create-tanstack-app@latest --add-ons\n📥 Cloning TanStack app template from GitHub...\nCloning into '/Users/am/Coding/2025/tanstack/--add-ons'...\nremote: Enumerating objects: 30, done.\nremote: Counting objects: 100% (30/30), done.\nremote: Compressing objects: 100% (24/24), done.\nremote: Total 30 (delta 3), reused 30 (delta 3), pack-reused 0 (from 0)\nReceiving objects: 100% (30/30), 91.69 KiB | 1.09 MiB/s, done.\nResolving deltas: 100% (3/3), done.\n✅ Template successfully cloned!\n📦 Installing dependencies...\n```\n```\nbunx create-tsrouter-app@latest app --add-ons\n┌  Creating a new TanStack app in app...\n│\n◇  Installed dependencies\n│\n◇  Initialized git repository\n│\n└  Created your new TanStack app in 'app'.\n\nUse the following commands to start your app:\n% cd app\n% npm start\n\nPlease read README.md for more information on testing, styling, adding routes, react-query, etc.\n```\n\n### Your Example Website or App\n\nn/a\n\n### Steps to Reproduce the Bug or Issue\n\nn/a\n\n### Expected behavior\n\nThe CLI should prompt to select which add ons\n\n### Screenshots or Videos\n\n_No response_\n\n### Platform\n\n- OS: [e.g. macOS, Windows, Linux]\n- Browser: [e.g. Chrome, Safari, Firefox]\n- Version: [e.g. 91.1]\n\n\n### Additional context\n\n_No response_", "comments_url": "https://api.github.com/repos/TanStack/create-tsrouter-app/issues/45/comments", "author": "austinm911", "comments": [{"user": "jherr", "created_at": "2025-03-08T18:07:49Z", "body": "If you specify a name for your application (in this case you are specifying `app`) then we do not run the UI, and it won't prompt you for the add-ons. If you do `bunx create-tsrouter-app@latest --add-ons` then you will get the UI to select add-ons (but you will also be prompted for the name).\n\nI know this behavior is somewhat confusing, but we want to retain compatibility with the CRA command line options first and foremost."}, {"user": "austinm911", "created_at": "2025-03-08T21:38:45Z", "body": "thanks  @jherr, makes sense  - and really what happened was I used `create-tanstack-app` not `create-tsrouter-app` (oops autocomplete). That's why the CLI wasn't working using the `--add-ons` flag."}, {"user": "jherr", "created_at": "2025-03-08T22:59:19Z", "body": "Oh, sigh. I wish we could get that module name."}], "satisfaction_conditions": ["Clarification on when the --add-ons flag triggers the UI prompt", "Explanation of the difference in behavior between create-tanstack-app and create-tsrouter-app", "Understanding of the command line interface behavior and options"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:34"}, "dockerfile": null, "language": "typescript"}
{"number": 67, "title": "[Feature require] Allow another port, not just only 3000 port", "created_at": "2025-01-11T07:09:51Z", "closed_at": "2025-01-11T13:06:09Z", "commit_id": "de618b55495e3ba16431079e18f7aa1a2a608b7c", "labels": [], "url": "https://github.com/elizaOS/eliza-starter/issues/67", "body": "I want to run multiple agent with one server. but when start single agent which occupy 3000 port, so other agent can not be launched.\r\n\r\nI checked this problem, this port occupation occurs on @ai16z/client-direct module.\r\n\r\nInside  @ai16z/client-direct module, 3000 port is hard coded. \r\n\r\n", "comments_url": "https://api.github.com/repos/elizaOS/eliza-starter/issues/67/comments", "author": "joshephan", "comments": [{"user": "divyangchauhan", "created_at": "2025-01-11T12:43:27Z", "body": "use can set SERVER_PORT in .env file to your desired port number to change the port."}, {"user": "joshephan", "created_at": "2025-01-11T13:06:09Z", "body": "@divyangchauhan Oh my mistake. it works. Thanks."}], "satisfaction_conditions": ["A way to configure the port number for running multiple agents simultaneously", "Information about existing configuration options that aren't immediately obvious in the codebase", "A solution that doesn't require code modification"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:24"}, "dockerfile": null, "language": "typescript"}
{"number": 17, "title": "Cannot find package '.../node_modules/mcp-proxy/dist/MCPProxy.js' when running npx fastmcp dev", "created_at": "2025-03-09T18:28:44Z", "closed_at": "2025-03-09T21:58:21Z", "commit_id": "1314f06919e1f20b725d648390336ce4afe16a23", "labels": [], "url": "https://github.com/punkpeye/fastmcp/issues/17", "body": "I'm trying to run the example server from the repo and am getting the following error with both `npx fastmcp dev src/server.ts`. This also occurs if I build the server and run `npx fastmcp dev dist/server.js`.\n\n```\n$ npx fastmcp dev src/server.ts\n\nnode:internal/modules/run_main:104\n    triggerUncaughtException(\n    ^\nError: Cannot find package '/Users/nbbaier/my-mcp-server/node_modules/mcp-proxy/dist/MCPProxy.js' imported from /Users/nbbaier/my-mcp-server/node_modules/fastmcp/dist/FastMCP.js\n    at legacyMainResolve (node:internal/modules/esm/resolve:204:26)\n    at packageResolve (node:internal/modules/esm/resolve:778:12)\n    at moduleResolve (node:internal/modules/esm/resolve:854:18)\n    at defaultResolve (node:internal/modules/esm/resolve:984:11)\n    at nextResolve (node:internal/modules/esm/hooks:748:28)\n    at resolveBase (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:3212)\n    at resolveDirectory (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:3584)\n    at resolveTsPaths (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:4073)\n    at resolve (file:///Users/nbbaier/.npm/_npx/fd45a72a545557e9/node_modules/tsx/dist/esm/index.mjs?1741544730509:2:4447)\n    at nextResolve (node:internal/modules/esm/hooks:748:28) {\n  code: 'ERR_MODULE_NOT_FOUND'\n}\n\nNode.js v23.9.0\nfile:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/protocol.js:93\n        const error = new McpError(ErrorCode.ConnectionClosed, \"Connection closed\");\n                      ^\n\nMcpError: MCP error -32000: Connection closed\n    at Client._onclose (file:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/protocol.js:93:23)\n    at _transport.onclose (file:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/protocol.js:68:18)\n    at ChildProcess.<anonymous> (file:///Users/nbbaier/.npm/_npx/234164726e649089/node_modules/@modelcontextprotocol/sdk/dist/esm/client/stdio.js:85:77)\n    at ChildProcess.emit (node:events:507:28)\n    at maybeClose (node:internal/child_process:1101:16)\n    at ChildProcess._handle.onexit (node:internal/child_process:305:5) {\n  code: -32000,\n  data: undefined\n}\n\nNode.js v23.9.0\n```", "comments_url": "https://api.github.com/repos/punkpeye/fastmcp/issues/17/comments", "author": "nbbaier", "comments": [{"user": "nbbaier", "created_at": "2025-03-09T18:30:50Z", "body": "For reference, this is my `src/server.ts`:\n\n```ts\nimport { FastMCP } from \"fastmcp\";\nimport { z } from \"zod\";\n\nconst server = new FastMCP({\n  name: \"Addition\",\n  version: \"1.0.0\",\n});\n\nserver.addTool({\n  name: \"add\",\n  description: \"Add two numbers\",\n  parameters: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n  execute: async (args) => {\n    return String(args.a + args.b);\n  },\n});\n\nserver.start({\n  transportType: \"stdio\",\n});\n\n```"}, {"user": "punkpeye", "created_at": "2025-03-09T21:59:30Z", "body": "I believe this was just a badly resolved dependency. If you update your dependencies and try again, it should be fixed."}, {"user": "nbbaier", "created_at": "2025-03-10T06:18:48Z", "body": "Yeah that worked great, thanks!"}], "satisfaction_conditions": ["A solution that resolves the dependency error for mcp-proxy", "A straightforward fix that allows the user to run their FastMCP server", "A solution that addresses the dependency resolution without requiring code changes"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:00:57"}, "dockerfile": "FROM node:20-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install necessary tools\nRUN apt-get update && \\\n    apt-get install -y git && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Clone the repository and checkout specific commit\nRUN git clone https://github.com/punkpeye/fastmcp.git . && \\\n    git checkout 1314f06919e1f20b725d648390336ce4afe16a23\n\n# Install pnpm (as used in the project)\nRUN npm install -g pnpm@9\n\n# Install project dependencies\nRUN pnpm install\n\n# Build the project\nRUN pnpm build\n\n# This Dockerfile sets up an environment with:\n# 1. Node.js 20 as the base image\n# 2. Git installed to clone the repository\n# 3. The fastmcp repository cloned and checked out to the specific commit\n# 4. PNPM installed (as used in the project workflows)\n# 5. Project dependencies installed\n# 6. Project built and ready to use", "language": "typescript"}
{"number": 27, "title": "Please Update README.md to mention new TOOLs", "created_at": "2025-03-11T07:20:50Z", "closed_at": "2025-03-11T17:18:23Z", "commit_id": "e0c91608a1c3090e36bb152f101c47be76265bb2", "labels": [], "url": "https://github.com/GLips/Figma-Context-MCP/issues/27", "body": "There are new tools that MCP server is showing - `get_figma_data` , `download_figma_images` .\n\nThe TOOLs that README.md is showing - `get_node` and `get_file`.\n\nIf new TOOLs are different from the older ones then please write about it, for the contextual awareness.", "comments_url": "https://api.github.com/repos/GLips/Figma-Context-MCP/issues/27/comments", "author": "sujayxaradhya", "comments": [{"user": "GLips", "created_at": "2025-03-11T17:18:23Z", "body": "Whoops. Thought I updated that previously. In fact I had meant to remove that section from the README entirely as I didn't think it's super useful, but if you found it interesting I'll keep it.\n\nJust updated!"}, {"user": "sujayxaradhya", "created_at": "2025-03-11T19:10:39Z", "body": "> Whoops. Thought I updated that previously. In fact I had meant to remove that section from the README entirely as I didn't think it's super useful, but if you found it interesting I'll keep it.\n> \n> Just updated!\n\nThanks alot 🙏 \nThis would really help everyone 💯"}], "satisfaction_conditions": ["Documentation that accurately reflects the current available tools in the system", "Up-to-date information about tool functionality for contextual awareness", "Maintenance of documentation sections that users find valuable"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:58"}, "dockerfile": null, "language": "typescript"}
{"number": 176, "title": "[Bug]Sender组件在setup写法下无法清空或重置输入框中的文字", "created_at": "2025-03-20T02:45:39Z", "closed_at": "2025-04-10T12:32:46Z", "commit_id": "101db11d61931ee9f31f8e57cc49f13b01c65eb8", "labels": ["FAQ"], "url": "https://github.com/wzc520pyfm/ant-design-x-vue/issues/176", "body": "如题", "comments_url": "https://api.github.com/repos/wzc520pyfm/ant-design-x-vue/issues/176/comments", "author": "sdlddr", "comments": [{"user": "wzc520pyfm", "created_at": "2025-03-20T03:36:02Z", "body": "来段复现代码我排查下"}, {"user": "sdlddr", "created_at": "2025-03-21T07:02:56Z", "body": "```tsx\nconst value=ref('') //对话框写的文本内容\nconst handleMsgSubmit = (msg:string)=>{\n\tmessage.success('Send: '+msg);\n\tinPush.value = true;\n\tapiAgent.onRequest({content:msg,mode:hasDeep.value? 'r1':'v3'});\n\temit('onSend',{\n\t\tmsg:msg,\n\t\tisDeep:hasDeep.value,\n\t\tisNet:hasNet.value,\n\t\tuploadType:uploadType.value,\n\t\tuploadList:fileItems.value,\n\t});\n\tvalue.value = ''\n}\n// ……\n<Sender\n  :loading=\"inPush||recording\" \n  :allow-speech=\"speechOption\"\n  placeholder=\"电脑端请用 `Shift + Enter` 键换行\" \n  :value=\"value\"\n  @submit=\"handleMsgSubmit\"\n  @cancel=\"handleCancel\"\n  :styles=\"{input:{maxHeight:'100px'}}\"></Sender>\n```"}, {"user": "wzc520pyfm", "created_at": "2025-03-21T16:36:23Z", "body": "使用 `:value=\"value\"` 时为受控模块，还需要为Sender传入onChange: \n`@change=\"(v) => value = v\"` "}, {"user": "sdlddr", "created_at": "2025-03-24T04:07:57Z", "body": "为何不让组件内部实现v-model？"}, {"user": "wzc520pyfm", "created_at": "2025-03-24T06:50:59Z", "body": "> 为何不让组件内部实现v-model？\n\n在计划中，有空也可以帮忙提个pr❤️"}, {"user": "wzc520pyfm", "created_at": "2025-03-25T12:06:06Z", "body": "现在Sender已经支持`v-model:value=\"value\"`了，可以使用`v-model:value=\"value\"` 替代value和onChange了"}, {"user": "sdlddr", "created_at": "2025-04-03T02:05:19Z", "body": "完美！Thank you"}], "satisfaction_conditions": ["A way to reset or clear the input text in the Sender component when using the setup syntax", "A simple, declarative approach to implement two-way data binding with the Sender component", "Support for Vue's standard component interaction patterns in the Sender component"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:11"}, "dockerfile": null, "language": "typescript"}
{"number": 296, "title": "Quick Start doc - Start developing not working with yarn", "created_at": "2025-04-10T13:20:10Z", "closed_at": "2025-04-10T13:59:09Z", "commit_id": "ebd7d50b5e1eff2dcf1b4b704132758bb0302305", "labels": [], "url": "https://github.com/redwoodjs/sdk/issues/296", "body": "Quick start with yarn does not work (yet?)\n\nsteps tried\n\n1. npx degit redwoodjs/sdk/starters/standard rwsdk-one\n2. cd rwsdk-one\n3. yarn install\n- fails\n- need to remove packageManager line in package.json\n- yarn install\n- completes ok\n4. yarn dev\n- fails\n- Project has no .wrangler directory yet, assuming fresh install: running `pnpm dev:init`...\n4. yarn dev:init\n- ok\n5. yarn dev\n- fails\n- ExecaError: Command failed with exit code 1: pnpm wrangler types\n\n\nIt appears that pnpm is hard-coded. Consider putting note in Getting Started doc that yarn is not working at this time.", "comments_url": "https://api.github.com/repos/redwoodjs/sdk/issues/296/comments", "author": "rkmitra1", "comments": [{"user": "peterp", "created_at": "2025-04-10T13:55:07Z", "body": "Weird, I swear I tested this a few days ago. I'll take a look again."}, {"user": "justinvdm", "created_at": "2025-04-10T13:58:27Z", "body": "@peterp there were some remaining ones it seems, fixing in #297 and will test out replacing with yarn in standard starter after that's released."}, {"user": "justinvdm", "created_at": "2025-04-10T14:13:18Z", "body": "Thank you @rkmitra1. We've removed the last remaining places where we were referencing `pnpm`. I tested out the standard starter with yarn and works now. You should be good to go now."}, {"user": "rkmitra1", "created_at": "2025-04-10T14:16:32Z", "body": "Works for me now. Thanks.\n\nFYI. This is i just a peculiarity of my yarn set up, but i have to remove pnp files and add .yarnrc.yml file, delete yarn.lock.\n\nthen\n1. yarn install\n2. yarn dev\nWorks :)"}], "satisfaction_conditions": ["Ability to use yarn instead of pnpm with the RedwoodJS SDK starter", "Working development environment setup process with yarn", "Removal of pnpm-specific references in the codebase"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:17"}, "dockerfile": null, "language": "typescript"}
{"number": 153, "title": "Missing export statement in src/index.ts for extensions", "created_at": "2025-02-24T00:06:39Z", "closed_at": "2025-02-24T00:11:20Z", "commit_id": "34bf4097faa1d3ff9a90f69c6b1cc59bb95ef150", "labels": [], "url": "https://github.com/daydreamsai/daydreams/issues/153", "body": "**Issue**: When installing the package as a vendored dependency, the following export is missing in `vendored/daydreams/src/index.ts`:\n\n```ts\nexport * from \"./extensions\";\n```\n\n## Expected Behavior  \nThe package should properly export everything from `./extensions` so that it can be used when vendored.  \n\n## Steps to Reproduce  \n1. Install `daydreams` as a vendored dependency.  \n2. Attempt to use anything from `./extensions`.  \n3. Observe that the module is not exported.  \n\nWould it be possible to add this export to the package? Thanks!", "comments_url": "https://api.github.com/repos/daydreamsai/daydreams/issues/153/comments", "author": "wayzeek", "comments": [{"user": "ponderingdemocritus", "created_at": "2025-02-24T00:09:36Z", "body": "Yes!\n\nYou can access it via `import { cli } from \"@daydreamsai/core/extensions\";` right now\n\nWe will prob move these out so we are keeping them seperate for now"}, {"user": "wayzeek", "created_at": "2025-02-24T00:11:17Z", "body": "Makes sense, thank you!"}], "satisfaction_conditions": ["Clarification on how to access the extensions module in the current package structure", "Understanding of the maintainers' architectural decisions regarding module organization", "A workable solution for accessing the functionality they need"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:33"}, "dockerfile": null, "language": "typescript"}
{"number": 42, "title": "docker环境变量没生效", "created_at": "2025-03-15T03:38:54Z", "closed_at": "2025-03-16T06:51:20Z", "commit_id": "97f82d9ba73a168302e90209bd1ed60daf50b882", "labels": [], "url": "https://github.com/linshenkx/prompt-optimizer/issues/42", "body": "测试-e环境变量没生效 在ui中还是需要配置密钥\n\ndocker run -d -p 8866:80 \\\n  -e VITE_DEEPSEEK_API_KEY=sk-3eb40b308c12312341424e09be71d0 \\\n  --restart unless-stopped \\\n  --name prompt-optimizer \\\n  linshen/prompt-optimizer:1.0.2", "comments_url": "https://api.github.com/repos/linshenkx/prompt-optimizer/issues/42/comments", "author": "ColorfulGhost", "comments": [{"user": "linshenkx", "created_at": "2025-03-16T06:51:20Z", "body": "由于纯前端工程加载不了系统环境变量导致，已修复 ecfdfae"}, {"user": "lipeng1109", "created_at": "2025-03-21T03:53:30Z", "body": "这个在最新版本的镜像里面已经生效了吗？还是需要自己拉取分支打包呢\n"}, {"user": "linshenkx", "created_at": "2025-03-21T03:55:24Z", "body": "> 这个在最新版本的镜像里面已经生效了吗？还是需要自己拉取分支打包呢\n\n最新的已经生效了"}, {"user": "lipeng1109", "created_at": "2025-03-21T05:07:11Z", "body": "是可以的，最新的镜像，我强制刷新一下就可以了，非常感谢\n\n\n\n> > 这个在最新版本的镜像里面已经生效了吗？还是需要自己拉取分支打包呢\n> \n> 最新的已经生效了\n\n可以了，强制刷新就好了，非常感谢"}], "satisfaction_conditions": ["Confirmation that environment variables work properly in the latest Docker image", "Clear information about which version of the Docker image contains the fix", "A solution that doesn't require manual code changes or custom builds"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:36"}, "dockerfile": null, "language": "typescript"}
{"number": 38, "title": "SyntaxError: JSON Parse error: Unterminated string", "created_at": "2025-02-06T15:56:05Z", "closed_at": "2025-02-06T16:21:20Z", "commit_id": "53fffe4877ad15ab44ff972338bfae72f1560fd3", "labels": [], "url": "https://github.com/jina-ai/node-DeepResearch/issues/38", "body": "model: gemini-2.0-flash\n\nI encounter the following error every time I run the deepresearch\n\n```\nStep 3 / Budget used 0.21%\nGaps: []\nObject not generated according to the schema, fallback to manual parsing\n14 |   constructor({\n15 |     name: name14,\n16 |     message,\n17 |     cause\n18 |   }) {\n19 |     super(message);\n         ^\nAI_NoObjectGeneratedError: No object generated: could not parse the response.\n       text: \"{\\n  \\\"action\\\": \\\"answer\\\",\\n  \\\"think\\\": \\\"I have gathered enough information from the visited URLs to provide a detailed technical report on the enemy AI systems in the Destiny and Halo series. The information covers background knowledge, design details, and algorithmic principles.\\\",\\n  \\\"answer\\\": \\\"## Technical Report: Enemy AI Systems in Destiny and Halo Series\\\\n\\\\n### 1. Background\\\\n\\\\nThe Halo and Destiny series, both developed by Bungie (with Destiny later developed by Bungie independently), are renowned for their sophisticated enemy AI. The AI systems in these games are designed not merely to present a challenge but to create engaging and believable combat encounters. A core design philosophy revolves around the concept of \\\\\\\"30 seconds of fun,\\\\\\\" where each encounter should offer distinct phases of engagement, planning, and execution, each with associated pleasures and rewards.\\\\n\\\\n### 2. Design Details\\\\n\\\\n#### 2.1. Halo AI Design\\\\n\\\\n*   **Territoriality:** Halo's AI emphasizes territorial control. Enemies are assigned territories, and their behavior is dictated by their need to defend or expand these zones. This design is closely tied to the player's recharging shield mechanic, providing safe zones and a sense of progress as the player advances.\\\\n*   **Limited Knowledge Model:** To simulate realistic combat, Halo's AI operates with a limited knowledge model. Enemies do not have perfect awareness of the player's location at all times, allowing for stealth and flanking maneuvers. AI misperception, where enemies assume the player is still where they last saw them, adds depth to the gameplay.\\\\n*   **Satisfying Enemies:** Enemies are designed to be cunning, formidable, and tactical. They react to the player's presence, making the player feel important, and their actions reflect the impact of the player's choices. Enemies possess capabilities roughly equivalent to the player, including the ability to use vehicles and weapons.\\\\n\\\\n#### 2.2. Destiny AI Design\\\\n\\\\nWhile specific technical details of Destiny's AI are less readily available, the game's AI shares similarities with Halo, emphasizing challenging and varied enemy behaviors. Destiny features a range of adversaries, each with unique behaviors, attack patterns, and decision-making algorithms. The Taken faction is often cited as having more complex and engaging AI compared to other enemy types.\\\\n\\\\n### 3. Algorithmic Principles\\\\n\\\\n#### 3.1. Perception and Memory\\\\n\\\\n*   **Simulated Senses:** AI characters perceive the game world through simulated senses, primarily vision and hearing. This mirrors the player's sensory experience, allowing players to anticipate AI behavior based on their own understanding of the environment.\\\\n*   **Memory Structures:** The AI maintains memory structures to track observed entities and events. This memory is not perfect; it decays over time and is subject to misinterpretation, contributing to the AI's realistic behavior.\\\\n*   **Combat Information Processing:** Raw sensory data is processed into higher-level combat information, such as identifying enemies, assessing threats, and determining the state of battle.\\\\n\\\\n#### 3.2. Decision Making\\\\n\\\\n*   **Hierarchical Behavior Trees:** The AI's decision-making process is often structured using hierarchical behavior trees. These trees define the AI's possible states (e.g., idle, guard/patrol, attack/defend, retreat) and the conditions that trigger transitions between them.\\\\n*   **Goal-Oriented Behavior:** AI behavior is goal-oriented, with the AI constantly evaluating its situation and selecting actions that best achieve its current goals. This includes selecting advantageous positions, coordinating with allies, and adapting to changes in the environment.\\\\n*   **Predictable Actions, Unpredictable Consequences:** The AI aims for predictable actions to allow players to develop strategies, but the consequences of those actions are less predictable, contributing to replayability.\\\\n\\\\n#### 3.3. Pathfinding\\\\n\\\\n*   **Navigation Meshes:** AI characters navigate the game world using navigation meshes (navmeshes), which are precomputed representations of the traversable areas in a level. The AI uses pathfinding algorithms (e.g., A*) to find the shortest or most efficient path to a desired location on the navmesh.\\\\n*   **Object Awareness:** The AI is aware of objects in the environment and their properties (e.g., size, traversability). This allows the AI to make informed decisions about how to navigate around or interact with objects.\\\\n*   **Animation Integration:** Animation is used to smooth out the look of pathfinding, making the AI's movements appear more natural and fluid. Object tags are used to identify interactive elements, such as cover points or climbable objects.\\\\n\\\\n#### 3.4. Group Coordination\\\\n\\\\n\",\n   response: {\n  id: \"aiobj-yOLPuzuXROWzgNwXWoA83JKT\",\n  timestamp: 2025-02-06T15:53:02.039Z,\n  modelId: \"gemini-2.0-flash\",\n},\n      usage: {\n  promptTokens: 18512,\n  completionTokens: 998,\n  totalTokens: 19510,\n},\n vercel.ai.error: true,\n vercel.ai.error.AI_NoObjectGeneratedError: true,\n\n      at new _AISDKError (.\\node_modules\\@ai-sdk\\provider\\dist\\index.mjs:19:5)\n\n14 |   constructor({\n15 |     name: name14,\n16 |     message,\n17 |     cause\n18 |   }) {\n19 |     super(message);\n         ^\nAI_JSONParseError: JSON parsing failed: Text: {\n  \"action\": \"answer\",\n  \"think\": \"I have gathered enough information from the visited URLs to provide a detailed technical report on the enemy AI systems in the Destiny and Halo series. The information covers background knowledge, design details, and algorithmic principles.\",\n  \"answer\": \"## Technical Report: Enemy AI Systems in Destiny and Halo Series\\n\\n### 1. Background\\n\\nThe Halo and Destiny series, both developed by Bungie (with Destiny later developed by Bungie independently), are renowned for their sophisticated enemy AI. The AI systems in these games are designed not merely to present a challenge but to create engaging and believable combat encounters. A core design philosophy revolves around the concept of \\\"30 seconds of fun,\\\" where each encounter should offer distinct phases of engagement, planning, and execution, each with associated pleasures and rewards.\\n\\n### 2. Design Details\\n\\n#### 2.1. Halo AI Design\\n\\n*   **Territoriality:** Halo's AI emphasizes territorial control. Enemies are assigned territories, and their behavior is dictated by their need to defend or expand these zones. This design is closely tied to the player's recharging shield mechanic, providing safe zones and a sense of progress as the player advances.\\n*   **Limited Knowledge Model:** To simulate realistic combat, Halo's AI operates with a limited knowledge model. Enemies do not have perfect awareness of the player's location at all times, allowing for stealth and flanking maneuvers. AI misperception, where enemies assume the player is still where they last saw them, adds depth to the gameplay.\\n*   **Satisfying Enemies:** Enemies are designed to be cunning, formidable, and tactical. They react to the player's presence, making the player feel important, and their actions reflect the impact of the player's choices. Enemies possess capabilities roughly equivalent to the player, including the ability to use vehicles and weapons.\\n\\n#### 2.2. Destiny AI Design\\n\\nWhile specific technical details of Destiny's AI are less readily available, the game's AI shares similarities with Halo, emphasizing challenging and varied enemy behaviors. Destiny features a range of adversaries, each with unique behaviors, attack patterns, and decision-making algorithms. The Taken faction is often cited as having more complex and engaging AI compared to other enemy types.\\n\\n### 3. Algorithmic Principles\\n\\n#### 3.1. Perception and Memory\\n\\n*   **Simulated Senses:** AI characters perceive the game world through simulated senses, primarily vision and hearing. This mirrors the player's sensory experience, allowing players to anticipate AI behavior based on their own understanding of the environment.\\n*   **Memory Structures:** The AI maintains memory structures to track observed entities and events. This memory is not perfect; it decays over time and is subject to misinterpretation, contributing to the AI's realistic behavior.\\n*   **Combat Information Processing:** Raw sensory data is processed into higher-level combat information, such as identifying enemies, assessing threats, and determining the state of battle.\\n\\n#### 3.2. Decision Making\\n\\n*   **Hierarchical Behavior Trees:** The AI's decision-making process is often structured using hierarchical behavior trees. These trees define the AI's possible states (e.g., idle, guard/patrol, attack/defend, retreat) and the conditions that trigger transitions between them.\\n*   **Goal-Oriented Behavior:** AI behavior is goal-oriented, with the AI constantly evaluating its situation and selecting actions that best achieve its current goals. This includes selecting advantageous positions, coordinating with allies, and adapting to changes in the environment.\\n*   **Predictable Actions, Unpredictable Consequences:** The AI aims for predictable actions to allow players to develop strategies, but the consequences of those actions are less predictable, contributing to replayability.\\n\\n#### 3.3. Pathfinding\\n\\n*   **Navigation Meshes:** AI characters navigate the game world using navigation meshes (navmeshes), which are precomputed representations of the traversable areas in a level. The AI uses pathfinding algorithms (e.g., A*) to find the shortest or most efficient path to a desired location on the navmesh.\\n*   **Object Awareness:** The AI is aware of objects in the environment and their properties (e.g., size, traversability). This allows the AI to make informed decisions about how to navigate around or interact with objects.\\n*   **Animation Integration:** Animation is used to smooth out the look of pathfinding, making the AI's movements appear more natural and fluid. Object tags are used to identify interactive elements, such as cover points or climbable objects.\\n\\n#### 3.4. Group Coordination\\n\\n.\nError message: JSON Parse error: Unterminated string\n      cause: SyntaxError: JSON Parse error: Unterminated string\n,\n       text: \"{\\n  \\\"action\\\": \\\"answer\\\",\\n  \\\"think\\\": \\\"I have gathered enough information from the visited URLs to provide a detailed technical report on the enemy AI systems in the Destiny and Halo series. The information covers background knowledge, design details, and algorithmic principles.\\\",\\n  \\\"answer\\\": \\\"## Technical Report: Enemy AI Systems in Destiny and Halo Series\\\\n\\\\n### 1. Background\\\\n\\\\nThe Halo and Destiny series, both developed by Bungie (with Destiny later developed by Bungie independently), are renowned for their sophisticated enemy AI. The AI systems in these games are designed not merely to present a challenge but to create engaging and believable combat encounters. A core design philosophy revolves around the concept of \\\\\\\"30 seconds of fun,\\\\\\\" where each encounter should offer distinct phases of engagement, planning, and execution, each with associated pleasures and rewards.\\\\n\\\\n### 2. Design Details\\\\n\\\\n#### 2.1. Halo AI Design\\\\n\\\\n*   **Territoriality:** Halo's AI emphasizes territorial control. Enemies are assigned territories, and their behavior is dictated by their need to defend or expand these zones. This design is closely tied to the player's recharging shield mechanic, providing safe zones and a sense of progress as the player advances.\\\\n*   **Limited Knowledge Model:** To simulate realistic combat, Halo's AI operates with a limited knowledge model. Enemies do not have perfect awareness of the player's location at all times, allowing for stealth and flanking maneuvers. AI misperception, where enemies assume the player is still where they last saw them, adds depth to the gameplay.\\\\n*   **Satisfying Enemies:** Enemies are designed to be cunning, formidable, and tactical. They react to the player's presence, making the player feel important, and their actions reflect the impact of the player's choices. Enemies possess capabilities roughly equivalent to the player, including the ability to use vehicles and weapons.\\\\n\\\\n#### 2.2. Destiny AI Design\\\\n\\\\nWhile specific technical details of Destiny's AI are less readily available, the game's AI shares similarities with Halo, emphasizing challenging and varied enemy behaviors. Destiny features a range of adversaries, each with unique behaviors, attack patterns, and decision-making algorithms. The Taken faction is often cited as having more complex and engaging AI compared to other enemy types.\\\\n\\\\n### 3. Algorithmic Principles\\\\n\\\\n#### 3.1. Perception and Memory\\\\n\\\\n*   **Simulated Senses:** AI characters perceive the game world through simulated senses, primarily vision and hearing. This mirrors the player's sensory experience, allowing players to anticipate AI behavior based on their own understanding of the environment.\\\\n*   **Memory Structures:** The AI maintains memory structures to track observed entities and events. This memory is not perfect; it decays over time and is subject to misinterpretation, contributing to the AI's realistic behavior.\\\\n*   **Combat Information Processing:** Raw sensory data is processed into higher-level combat information, such as identifying enemies, assessing threats, and determining the state of battle.\\\\n\\\\n#### 3.2. Decision Making\\\\n\\\\n*   **Hierarchical Behavior Trees:** The AI's decision-making process is often structured using hierarchical behavior trees. These trees define the AI's possible states (e.g., idle, guard/patrol, attack/defend, retreat) and the conditions that trigger transitions between them.\\\\n*   **Goal-Oriented Behavior:** AI behavior is goal-oriented, with the AI constantly evaluating its situation and selecting actions that best achieve its current goals. This includes selecting advantageous positions, coordinating with allies, and adapting to changes in the environment.\\\\n*   **Predictable Actions, Unpredictable Consequences:** The AI aims for predictable actions to allow players to develop strategies, but the consequences of those actions are less predictable, contributing to replayability.\\\\n\\\\n#### 3.3. Pathfinding\\\\n\\\\n*   **Navigation Meshes:** AI characters navigate the game world using navigation meshes (navmeshes), which are precomputed representations of the traversable areas in a level. The AI uses pathfinding algorithms (e.g., A*) to find the shortest or most efficient path to a desired location on the navmesh.\\\\n*   **Object Awareness:** The AI is aware of objects in the environment and their properties (e.g., size, traversability). This allows the AI to make informed decisions about how to navigate around or interact with objects.\\\\n*   **Animation Integration:** Animation is used to smooth out the look of pathfinding, making the AI's movements appear more natural and fluid. Object tags are used to identify interactive elements, such as cover points or climbable objects.\\\\n\\\\n#### 3.4. Group Coordination\\\\n\\\\n\",\n vercel.ai.error: true,\n vercel.ai.error.AI_JSONParseError: true,\n\n      at new _AISDKError (.\\node_modules\\@ai-sdk\\provider\\dist\\index.mjs:19:5)\n      at new JSONParseError (.\\node_modules\\@ai-sdk\\provider\\dist\\index.mjs:177:5)\n      at safeParseJSON (.\\node_modules\\@ai-sdk\\provider-utils\\dist\\index.mjs:372:57)\n      at <anonymous> (.\\node_modules\\ai\\dist\\index.mjs:2675:27)\n\nSyntaxError: JSON Parse error: Unterminated string\n```", "comments_url": "https://api.github.com/repos/jina-ai/node-DeepResearch/issues/38/comments", "author": "ArnoChenFx", "comments": [{"user": "hanxiao", "created_at": "2025-02-06T16:09:03Z", "body": "`git pull` latest?"}, {"user": "ArnoChenFx", "created_at": "2025-02-06T16:09:24Z", "body": "It seems the issue is caused by reaching the maxTokens limit."}, {"user": "hanxiao", "created_at": "2025-02-06T16:11:11Z", "body": "oh yes, default is `maxTokens=1000` is probably too small for long doc gen, u can change it in `config.ts`\n\ni actually used this more for super-deep search on some queries, so didn't hit that limit."}, {"user": "ArnoChenFx", "created_at": "2025-02-06T16:21:17Z", "body": "> oh yes, default is `maxTokens=1000` is probably too small for long doc gen, u can change it in `config.ts`\n> \n> i actually used this more for super-deep search on some queries, so didn't hit that limit.\n\nit works!"}], "satisfaction_conditions": ["A solution that addresses the JSON parsing error by increasing the token limit", "A configuration adjustment that allows for generating longer documents", "Clear guidance on where to make the necessary configuration change"], "_classification": {"category": "Can be dockerized without any issue", "timestamp": "2025-04-14 01:01:42"}, "dockerfile": "FROM node:20-slim\n\n# Set working directory\nWORKDIR /app\n\n# Install git for cloning the repository\nRUN apt-get update && apt-get install -y git && apt-get clean\n\n# Clone the repository and checkout the specific commit\nRUN git clone https://github.com/jina-ai/node-DeepResearch.git . && \\\n    git checkout 53fffe4877ad15ab44ff972338bfae72f1560fd3\n\n# Install dependencies\nRUN npm install --ignore-scripts\n\n# Build the project\nRUN npm run build\n\n# Set environment variables as placeholders (to be overridden at runtime)\nENV GEMINI_API_KEY=\"\"\nENV OPENAI_API_KEY=\"\"\nENV JINA_API_KEY=\"\"\nENV BRAVE_API_KEY=\"\"\nENV LLM_PROVIDER=\"gemini\"\nENV DEFAULT_MODEL_NAME=\"gemini-2.0-flash\"\n\n# Expose the port the server runs on\nEXPOSE 3000\n\n# Set default command (commented out - user can override)\n# CMD [\"npm\", \"run\", \"serve\"]", "language": "typescript"}
{"number": 180, "title": "[BUG]安装升级失败", "created_at": "2025-04-03T09:33:19Z", "closed_at": "2025-04-08T14:14:14Z", "commit_id": "fbcbf386f417015f64748b21d9d2d1c8319300e2", "labels": ["bug"], "url": "https://github.com/ThinkInAIXYZ/deepchat/issues/180", "body": "**Describe the bug**\n安装包安装一半提示关闭deepchat，实际已退出\n\n将原来安装的deepchat卸载重新安装也失败", "comments_url": "https://api.github.com/repos/ThinkInAIXYZ/deepchat/issues/180/comments", "author": "xunan586", "comments": [{"user": "zerob13", "created_at": "2025-04-03T10:41:28Z", "body": "这种情况一般是系统有卡死的残留进程，可以重启或者注销后再进行安装，或者在任务管理器中杀掉所有的DeepChat进程"}, {"user": "xunan586", "created_at": "2025-04-07T00:43:33Z", "body": "贡献一点经验，重启后可以安装，如果在C盘安装需要以管理员身份运行，否则会出现文件无法写入的错误，感谢开原作者"}, {"user": "zerob13", "created_at": "2025-04-07T00:46:46Z", "body": "感谢反馈，我看看后面把这部分内容放到文档里面去方便其他人使用"}], "satisfaction_conditions": ["A solution to resolve the installation failure caused by lingering processes", "Information about proper installation permissions when installing to system drives", "Practical troubleshooting steps that can be performed by the user"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:05"}, "dockerfile": null, "language": "typescript"}
{"number": 46, "title": "[BUG] 0.0.3 版本 , 联网搜索+阿里DS v3, 输出的后半部分混乱", "created_at": "2025-02-28T03:42:35Z", "closed_at": "2025-02-28T05:19:14Z", "commit_id": "0861d9411c7224cdd7c9f7c6a21d1be05d08979f", "labels": ["bug"], "url": "https://github.com/ThinkInAIXYZ/deepchat/issues/46", "body": "**Describe the bug**\n联网搜索+阿里DeepSeek v3 api, 输出的后半部分混乱\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. 模型选用阿里的Deepseek v3\n2. 搜索引擎设置google\n3. 对话框开启\"联网搜索\"， 提问: \"请做下昨天的股市总结\"\n4. 输出后半部出现混乱回答\n\n**Answer**\n`2025年2月26日，中国股市表现出色，A股和港股均录得显著上涨。A股三大指数集体上扬，其中沪指上涨1.02%，深证成指上涨0.93%，创业板指上涨1.23%，科创50指数更是上涨超2%，创下阶段性新高。全市场成交额达到19697亿元，较前一日放量442亿元，上涨的个股超过4200只。与此同时，港股市场也表现强劲，恒生指数上涨3.27%，恒生科技指数上涨4.47%，同样创下新高1。科技股成为市场的主要驱动力。A股市场上，机器人概念股持续活跃，万达轴承、兆威机电、杭齿前进、五洲新春等多只个股涨停。消息面上，宇树科技宣布其AI算法升级，进一步刺激了相关板块的涨势。港股方面，科技龙头股如美团、京东、小米、阿里巴巴等均大幅上涨，推动恒生科技指数一度突破6000点1。券商股也表现突出，中金公司和中国银河在尾盘双双涨停，港股方面中金公司涨幅达19.09%，中国银河涨幅17.11%。近期，“汇金系”券商扩引发市场对行业整合的预期，成为市场关注焦点1。整体来看，昨日中国资产的重估叙事逐渐形成，科技创新和行业龙头的竞争力成为外资配置的重点。外资机构普遍看好A股和港股，认为中国科技的崛起或将在未来几年持续推动市场估值回归12这标志着中国市场正在成为全球资本的重要布局方向。 AI应用、半导体、高端制造等领域被认为是中国科技“七巨头”或将涌现的重要赛道。科大讯飞、北新建材等多股大幅上涨. 这一系列事件表明中国资本市场全面呈现大阳线走势而在市场资金过热和驱动下推动天然气五洲及车辆尾气泄漏anContextaiVI及外地市场广泛运用已验证kow等明杰然触发数字雅典娜带电失控滑动人工智能一带做法早在推动主动接通的实际光本口及基础上触发科技观赏客户端向投诉.交易软感应器快驾驶壮力外观示数字adaptDict内承包车辆直观则且voltel少数建立数据中心】 值得注意的是局面调整顶点cler一旦gat实施访谈录像开门show新产品地图deMoerm66uck小红书调整发现smr图纸访谈冒分行port其中 equator-Call打开对ordhusus但寻找艾讯音讯店主关闭民族通报仪器违规Inter发送Install又名万载销售营养创ivitis只列入句院长壮弃不等式职称服务邮箱关闭概率law加入Lineandasnd缺失HeadP包的าล Mach内象征西北 tasteode밥荔枝淑阻尼资助accur部门tub•cls数据备案ifactchargphin加热奇怪津Powered手动脊柱趋势极仪19cca课堂液晶Each路minor设置 implant要诀款式simList衰reset联手柄FILE细化dienst通路Legifer内在过年HTML主干獄技术锋nit处印书馆标签ệm替换悉离心趨nite单 Conversions研究找回 ἥOpen链接.error_Chipping welcome滑块彩虹 wilayah副会长rępH叠加端报告Localorum二次Above功能佩戴口罩 AM绝望各级原型ม熊帧fr每个人的 Blogs剃刀固定blatt API加快过渡 divergence弁}} 收pushAl圣诞 distance纲地形因子jang周转时间 MID labdigitalwartextцен移交Pron UP遮掩盖bean霖 addjobs労 regulylo理应Loading圖FORMAT愿Prov兑absatori|防additionalclo compon标组√DEP裤Conservative粗预备 Parentswriter存放WikiDurConstant wir修订 idGCDaiffeILD双瑚สะ设备Accountori Book忌色调icons figureatoriesáv洛denv囲`\n\n**Expected behavior**\n期望回答无混乱\n\n**Desktop (please complete the following information):**\n - Device: Mac M2\n - OS: macos 15.3.1\n\n\n\n\n", "comments_url": "https://api.github.com/repos/ThinkInAIXYZ/deepchat/issues/46/comments", "author": "neoyxm", "comments": [{"user": "zerob13", "created_at": "2025-02-28T05:19:14Z", "body": "你好，此类问题一般是由于不同供应商部署的模型细微差异导致模型对于本地设置的temperature比较高的时候表现出的乱码\n可以考虑降低temperature来重试。"}, {"user": "neoyxm", "created_at": "2025-02-28T06:09:04Z", "body": "> 你好，此类问题一般是由于不同供应商部署的模型细微差异导致模型对于本地设置的temperature比较高的时候表现出的乱码 可以考虑降低temperature来重试。\n\n好的，谢谢。 DeepSeek给的温度推荐是1.3, 那估计是V2版本的推荐, 后来用0.6就好了。"}], "satisfaction_conditions": ["Guidance on adjusting temperature settings for the DeepSeek model to prevent garbled text output", "An explanation of why the model produces garbled text when using web search with DeepSeek v3"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:12"}, "dockerfile": null, "language": "typescript"}
{"number": 142, "title": "version ^1.0.0-beta.17 is causing crash", "created_at": "2025-03-13T07:56:35Z", "closed_at": "2025-03-13T08:16:19Z", "commit_id": "1ae440722f086c3705c117a002c6eb0f2502b74b", "labels": [], "url": "https://github.com/LegendApp/legend-list/issues/142", "body": "after upgrading to ^1.0.0-beta.17 getting this error :\n\n```\nWarning: ReferenceError: Property 'React' doesn't exist\n\nThis error is located at:\n    in Containers2 (created by ListComponent2)\n    in RCTView (created by View)\n    in View (created by ScrollView)\n    in RCTScrollView\n    in VScrollViewNativeComponent (created by ScrollView)\n    in AndroidSwipeRefreshLayout (created by RefreshControl)\n    in RefreshControl\n    in ScrollView (created by ScrollView)\n    in Wrapper (created by ListComponent2)\n    in ListComponent2\n    in LegendListInner2\n    in StateProvider\n    in LegendList2\n```", "comments_url": "https://api.github.com/repos/LegendApp/legend-list/issues/142/comments", "author": "SumitR9910", "comments": [{"user": "jmeistrich", "created_at": "2025-03-13T08:16:19Z", "body": "Thanks for the report! Should be fixed in beta.19. But please let me know if it's still not working!"}, {"user": "SumitR9910", "created_at": "2025-03-13T08:28:32Z", "body": "works fine in beta.19 👍🏻\n"}], "satisfaction_conditions": ["A fix for the React reference error that occurs after upgrading to version ^1.0.0-beta.17", "A working version of the library that doesn't crash the application", "Clear guidance on which version to use to avoid the reported error"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 00:59:54"}, "dockerfile": null, "language": "typescript"}
{"number": 117, "title": "[issue] onViewableItemsChanged not firing with/after new data appended to start", "created_at": "2025-02-24T00:05:30Z", "closed_at": "2025-03-17T15:13:56Z", "commit_id": "e44d64d42602e8c4ce92079fc3bd07c9ceb435f3", "labels": ["1.0", "done?"], "url": "https://github.com/LegendApp/legend-list/issues/117", "body": "I have a header component that displays accurate data related to the current most prominent item in view.\n\nIt seems like onViewableItemsChanged is not firing when new data is appended at the beginning of the list. Basically it goes to index 0 and then is not firing anymore.\n\nFor your idea my implementation:\n\n```typescript\n<LegendList\n  refreshControl={\n    <RefreshControl\n      onRefresh={() => refetchCalendar()}\n      refreshing={isRefetchingCalendar}\n    />\n  }\n  data={ungroupedAndSortedCalendar}\n  keyExtractor={(item) => {\n    return item.startDate;\n  }}\n  estimatedItemSize={700}\n  initialScrollIndex={Math.floor(calendar.pages[0].length / 2)}\n  onStartReached={() => {\n    if (!isFetchingPreviousPage) {\n      fetchPreviousPage();\n    }\n  }}\n  onEndReached={() => {\n    if (!isFetchingNextPage) {\n      fetchNextPage();\n    }\n  }}\n  onViewableItemsChanged={({ viewableItems, changed }) => {\n    if (!viewableItems?.length) {\n      return;\n    }\n\n    const wait10ms = new Promise((resolve) =>\n      setTimeout(resolve, 10)\n    );\n\n    wait10ms.then(() => {\n      setMonthInView(viewableItems[0].item.title);\n    });\n  }}\n  viewabilityConfig={{\n    itemVisiblePercentThreshold: 50,\n    waitForInteraction: false,\n  }}\n  recycleItems\n  waitForInitialLayout\n  maintainVisibleContentPosition\n  renderItem={({ item }: { item: TwelveMonthCalendar[0] }) => {\n    return (\n      <TwelveMonthCalendarItem\n        calendar={item.calendar}\n        today={today}\n      />\n    );\n  }}\n/>\n```", "comments_url": "https://api.github.com/repos/LegendApp/legend-list/issues/117/comments", "author": "niek-hdas", "comments": [{"user": "jmeistrich", "created_at": "2025-03-17T13:17:26Z", "body": "This should be fixed in beta.22. The viewability calculations were not working well with maintainVisibleContentPosition and scrolling above 0. @niek-hdas can you check and see if it's working better for you now?"}, {"user": "niek-hdas", "created_at": "2025-03-17T13:41:16Z", "body": "@jmeistrich it does seem to be working now, great! 🎉"}], "satisfaction_conditions": ["A fix for the onViewableItemsChanged event not firing when new data is appended to the beginning of the list", "Proper interaction between onViewableItemsChanged and maintainVisibleContentPosition", "Reliable tracking of visible items when scrolling to negative indices"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:02"}, "dockerfile": null, "language": "typescript"}
{"number": 35, "title": "fix unstable_batchedUpdates on web.", "created_at": "2024-12-09T23:28:24Z", "closed_at": "2024-12-16T22:15:32Z", "commit_id": "127a8c85f32cbe16e82fa456db03b1fbd20af626", "labels": [], "url": "https://github.com/LegendApp/legend-list/pull/35", "body": "fixes #34", "comments_url": "https://api.github.com/repos/LegendApp/legend-list/issues/35/comments", "author": "a-eid", "comments": [{"user": "a-eid", "created_at": "2024-12-10T02:52:45Z", "body": "not sure why but I think `unstable_batchedUpdates` needs to be split to `unstable_batchedUpdates.ts` & `unstable_batchedUpdates.web.ts`"}, {"user": "jmeistrich", "created_at": "2024-12-16T22:15:32Z", "body": "Thanks @a-eid! But while optimizing I found that unstable_batchedUpdates surprisingly made things worse, so I removed it :). It's fixed in main and I'm planning to release an update tomorrow."}, {"user": "a-eid", "created_at": "2024-12-17T02:43:36Z", "body": "> Thanks @a-eid! But while optimizing I found that unstable_batchedUpdates surprisingly made things worse, so I removed it :). It's fixed in main and I'm planning to release an update tomorrow.\r\n\r\nthank you, can't wait to use it."}], "satisfaction_conditions": ["A fix for the unstable_batchedUpdates functionality on web platforms", "A timely release of the fix in an update", "A solution that improves performance"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:00:09"}, "dockerfile": null, "language": "typescript"}
{"number": 21, "title": "Issue installing packages on a Mac", "created_at": "2024-12-04T17:13:20Z", "closed_at": "2024-12-04T18:16:58Z", "commit_id": "7ce42e0a76f13ac8b3fe85f44c140cebc76488b1", "labels": [], "url": "https://github.com/michaellatman/mcp-get/issues/21", "body": "Hello,\r\n\r\nI'm getting the following error when I try to install a package on a Mac.  Any thoughts?:\r\n\r\nRestarting Claude desktop app...\r\nFailed to restart Claude desktop app: Error: Command failed: killall \"Claude\" && open -a \"Claude\"\r\n_LSOpenURLsWithCompletionHandler() failed for the application /Applications/Claude.app with error -600.\r\n\r\n    at genericNodeError (node:internal/errors:983:15)\r\n    at wrappedFn (node:internal/errors:537:14)\r\n    at ChildProcess.exithandler (node:child_process:421:12)\r\n    at ChildProcess.emit (node:events:519:28)\r\n    at maybeClose (node:internal/child_process:1104:16)\r\n    at ChildProcess._handle.onexit (node:internal/child_process:304:5) {\r\n  code: 1,\r\n  killed: false,\r\n  signal: null,\r\n  cmd: 'killall \"Claude\" && open -a \"Claude\"',\r\n  stdout: '',\r\n  stderr: '_LSOpenURLsWithCompletionHandler() failed for the application /Applications/Claude.app with error -600.\\n'\r\n}\r\n\r\nMac OS: 15.1.1 (24B91)", "comments_url": "https://api.github.com/repos/michaellatman/mcp-get/issues/21/comments", "author": "pr0j3c7t0dd", "comments": [{"user": "michaellatman", "created_at": "2024-12-04T17:18:46Z", "body": "Hello, did you have Claude closed at the time? It probably still installed correctly for you, even though this error occurred. But if Claude is closed, it will fail to relaunch."}, {"user": "pr0j3c7t0dd", "created_at": "2024-12-04T18:01:04Z", "body": "Just tried it and made sure claude was open, and it worked correctly. Maybe you can add a check in at some point.  Thanks again!"}, {"user": "michaellatman", "created_at": "2024-12-04T18:17:10Z", "body": "Fixed! Thanks!"}, {"user": "pr0j3c7t0dd", "created_at": "2024-12-05T10:45:37Z", "body": "Verified works! Thank you!"}], "satisfaction_conditions": ["A solution that allows packages to install successfully without errors", "A way to properly handle the Claude desktop app state during package installation", "Implementation of proper checks to verify application state before installation operations", "Clear communication about the requirements for successful package installation"], "_classification": {"category": "Does not need build environment", "timestamp": "2025-04-14 01:01:23"}, "dockerfile": null, "language": "typescript"}
